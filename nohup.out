/home/harvey/anaconda3/envs/285final/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/285final/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/285final/lib/python3.7/site-packages/gym/core.py:257: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.[0m
  "Function `env.seed(seed)` is marked as deprecated and will be removed in the future. "



LOGGING TO:  /home/harvey/Documents/cs285/CS285-Project/cs285/scripts/../../data/sac_peer2_agents_rpb20000_temp0.1_HalfCheetah-v4_09-12-2022_02-15-22 



########################
logging outputs to  /home/harvey/Documents/cs285/CS285-Project/cs285/scripts/../../data/sac_peer2_agents_rpb20000_temp0.1_HalfCheetah-v4_09-12-2022_02-15-22
########################
Using GPU id 0
Traceback (most recent call last):
  File "cs285/scripts/run_sac_peer_experiment.py", line 128, in <module>
    main()
  File "cs285/scripts/run_sac_peer_experiment.py", line 124, in main
    trainer.run_training_loop()
  File "cs285/scripts/run_sac_peer_experiment.py", line 56, in run_training_loop
    eval_policies = [agent.actor for agent in self.rl_trainer.agents],
  File "/home/harvey/Documents/cs285/CS285-Project/cs285/infrastructure/rl_trainer.py", line 146, in run_training_loop
    print_period = 1 if not isinstance(self.agent, PeerSACAgent) else 1000
AttributeError: 'RL_Trainer' object has no attribute 'agent'
./peersac.sh: 7: --seed: not found
/home/harvey/anaconda3/envs/285final/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/285final/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/285final/lib/python3.7/site-packages/gym/core.py:257: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.[0m
  "Function `env.seed(seed)` is marked as deprecated and will be removed in the future. "



LOGGING TO:  /home/harvey/Documents/cs285/CS285-Project/cs285/scripts/../../data/sac_peer2_agents_rpb20000_temp0.5_HalfCheetah-v4_09-12-2022_02-15-47 



########################
logging outputs to  /home/harvey/Documents/cs285/CS285-Project/cs285/scripts/../../data/sac_peer2_agents_rpb20000_temp0.5_HalfCheetah-v4_09-12-2022_02-15-47
########################
Using GPU id 0
Traceback (most recent call last):
  File "cs285/scripts/run_sac_peer_experiment.py", line 128, in <module>
    main()
  File "cs285/scripts/run_sac_peer_experiment.py", line 124, in main
    trainer.run_training_loop()
  File "cs285/scripts/run_sac_peer_experiment.py", line 56, in run_training_loop
    eval_policies = [agent.actor for agent in self.rl_trainer.agents],
  File "/home/harvey/Documents/cs285/CS285-Project/cs285/infrastructure/rl_trainer.py", line 146, in run_training_loop
    print_period = 1 if not isinstance(self.agent, PeerSACAgent) else 1000
AttributeError: 'RL_Trainer' object has no attribute 'agent'
./peersac.sh: 7: --seed: not found



LOGGING TO:  /home/harvey/Documents/cs285/CS285-Project/cs285/scripts/../../data/sac_peer2_agents_rpb20000_temp0.5_HalfCheetah-v4_09-12-2022_02-17-38 



########################
logging outputs to  /home/harvey/Documents/cs285/CS285-Project/cs285/scripts/../../data/sac_peer2_agents_rpb20000_temp0.5_HalfCheetah-v4_09-12-2022_02-17-38
########################
Using GPU id 0


********** Iteration 0 ************

Collecting data to be used for training...

Training agent...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.553035736083984
Agent0_Eval_StdReturn : 35.8353271484375
Agent0_Eval_MaxReturn : 11.839849472045898
Agent0_Eval_MinReturn : -106.5333251953125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -68.51490020751953
Agent0_Train_StdReturn : 35.8519172668457
Agent0_Train_MaxReturn : 10.17027473449707
Agent0_Train_MinReturn : -109.59283447265625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 1500
Agent0_TimeSinceStart : 4.426525831222534
Agent0_Critic_Loss : 5.271413803100586
Agent0_Actor_Loss : -2.274087905883789
Agent0_Alpha_Loss : 4.931520462036133
Agent0_Temperature : 0.49985002249805427
Agent0_Initial_DataCollection_AverageReturn : -68.51490020751953
Done logging...



Collecting data to be used for training...

Training agent...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -65.2655258178711
Agent1_Eval_StdReturn : 28.615264892578125
Agent1_Eval_MaxReturn : -13.171289443969727
Agent1_Eval_MinReturn : -112.93124389648438
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -41.37040710449219
Agent1_Train_StdReturn : 23.476428985595703
Agent1_Train_MaxReturn : -3.7883758544921875
Agent1_Train_MinReturn : -74.03202819824219
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 1500
Agent1_TimeSinceStart : 6.114991188049316
Agent1_Critic_Loss : 5.973700523376465
Agent1_Actor_Loss : -2.3039894104003906
Agent1_Alpha_Loss : 4.900323867797852
Agent1_Temperature : 0.4998500224980562
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -57.247718811035156
Agent0_Eval_StdReturn : 36.752994537353516
Agent0_Eval_MaxReturn : -5.503623962402344
Agent0_Eval_MinReturn : -140.23109436035156
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -53.583595275878906
Agent0_Train_StdReturn : 26.782987594604492
Agent0_Train_MaxReturn : -4.920452117919922
Agent0_Train_MinReturn : -95.44530487060547
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 3000
Agent0_TimeSinceStart : 7.800966024398804
Agent0_Critic_Loss : 4.120080947875977
Agent0_Actor_Loss : -2.5006396770477295
Agent0_Alpha_Loss : 4.940102577209473
Agent0_Temperature : 0.49970008324530474
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -52.62616729736328
Agent1_Eval_StdReturn : 32.91780471801758
Agent1_Eval_MaxReturn : 4.73239803314209
Agent1_Eval_MinReturn : -93.18003845214844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -55.14052200317383
Agent1_Train_StdReturn : 29.613248825073242
Agent1_Train_MaxReturn : 3.436203956604004
Agent1_Train_MinReturn : -92.37983703613281
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 3000
Agent1_TimeSinceStart : 9.49343752861023
Agent1_Critic_Loss : 4.489084720611572
Agent1_Actor_Loss : -2.620345115661621
Agent1_Alpha_Loss : 4.976039886474609
Agent1_Temperature : 0.4997000344772256
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -61.034881591796875
Agent0_Eval_StdReturn : 40.130821228027344
Agent0_Eval_MaxReturn : 17.870899200439453
Agent0_Eval_MinReturn : -141.66311645507812
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -51.725852966308594
Agent0_Train_StdReturn : 50.30283737182617
Agent0_Train_MaxReturn : 50.390098571777344
Agent0_Train_MinReturn : -129.45620727539062
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 4500
Agent0_TimeSinceStart : 11.200036525726318
Agent0_Critic_Loss : 3.6276228427886963
Agent0_Actor_Loss : -2.759157180786133
Agent0_Alpha_Loss : 4.959624290466309
Agent0_Temperature : 0.49955016634403443
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.928951263427734
Agent1_Eval_StdReturn : 32.30506896972656
Agent1_Eval_MaxReturn : 3.0390167236328125
Agent1_Eval_MinReturn : -91.75643920898438
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -39.769195556640625
Agent1_Train_StdReturn : 29.052013397216797
Agent1_Train_MaxReturn : 9.266439437866211
Agent1_Train_MinReturn : -79.24403381347656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 4500
Agent1_TimeSinceStart : 12.893198490142822
Agent1_Critic_Loss : 3.285701274871826
Agent1_Actor_Loss : -2.8370413780212402
Agent1_Alpha_Loss : 4.946570873260498
Agent1_Temperature : 0.49955010318840426
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -53.16351318359375
Agent0_Eval_StdReturn : 30.225069046020508
Agent0_Eval_MaxReturn : -3.908966064453125
Agent0_Eval_MinReturn : -109.67790222167969
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -55.9347038269043
Agent0_Train_StdReturn : 25.882003784179688
Agent0_Train_MaxReturn : -24.27569580078125
Agent0_Train_MinReturn : -102.03494262695312
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 6000
Agent0_TimeSinceStart : 14.584867238998413
Agent0_Critic_Loss : 2.625873565673828
Agent0_Actor_Loss : -3.045459032058716
Agent0_Alpha_Loss : 4.973779678344727
Agent0_Temperature : 0.49940026681415883
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -40.730953216552734
Agent1_Eval_StdReturn : 14.884360313415527
Agent1_Eval_MaxReturn : -12.189730644226074
Agent1_Eval_MinReturn : -60.9809455871582
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -35.890785217285156
Agent1_Train_StdReturn : 18.920143127441406
Agent1_Train_MaxReturn : 1.1882171630859375
Agent1_Train_MinReturn : -66.54618835449219
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 6000
Agent1_TimeSinceStart : 16.302336931228638
Agent1_Critic_Loss : 2.434112548828125
Agent1_Actor_Loss : -3.1293091773986816
Agent1_Alpha_Loss : 4.971828460693359
Agent1_Temperature : 0.4994001925982663
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -47.488128662109375
Agent0_Eval_StdReturn : 24.983102798461914
Agent0_Eval_MaxReturn : 0.5308036804199219
Agent0_Eval_MinReturn : -91.7286376953125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -61.50727462768555
Agent0_Train_StdReturn : 28.32615089416504
Agent0_Train_MaxReturn : -7.171201705932617
Agent0_Train_MinReturn : -103.07376861572266
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 7500
Agent0_TimeSinceStart : 18.140000104904175
Agent0_Critic_Loss : 2.0792312622070312
Agent0_Actor_Loss : -3.3195114135742188
Agent0_Alpha_Loss : 4.982168197631836
Agent0_Temperature : 0.4992503858497694
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -33.70835876464844
Agent1_Eval_StdReturn : 37.299530029296875
Agent1_Eval_MaxReturn : 36.350135803222656
Agent1_Eval_MinReturn : -95.64886474609375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -54.55439376831055
Agent1_Train_StdReturn : 25.96477508544922
Agent1_Train_MaxReturn : -16.506763458251953
Agent1_Train_MinReturn : -91.806884765625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 7500
Agent1_TimeSinceStart : 20.30997323989868
Agent1_Critic_Loss : 2.195338726043701
Agent1_Actor_Loss : -3.275552272796631
Agent1_Alpha_Loss : 4.963000297546387
Agent1_Temperature : 0.49925032492936267
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -40.56001281738281
Agent0_Eval_StdReturn : 31.11109733581543
Agent0_Eval_MaxReturn : 32.402767181396484
Agent0_Eval_MinReturn : -82.9545669555664
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.148101806640625
Agent0_Train_StdReturn : 32.43116760253906
Agent0_Train_MaxReturn : 3.367720603942871
Agent0_Train_MinReturn : -98.82298278808594
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 9000
Agent0_TimeSinceStart : 22.48936915397644
Agent0_Critic_Loss : 1.9227309226989746
Agent0_Actor_Loss : -3.4745006561279297
Agent0_Alpha_Loss : 4.962724685668945
Agent0_Temperature : 0.49910056036963224
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -30.033618927001953
Agent1_Eval_StdReturn : 31.53630828857422
Agent1_Eval_MaxReturn : 22.63113021850586
Agent1_Eval_MinReturn : -92.22898864746094
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -29.5263614654541
Agent1_Train_StdReturn : 25.832590103149414
Agent1_Train_MaxReturn : 14.521238327026367
Agent1_Train_MinReturn : -75.81573486328125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 9000
Agent1_TimeSinceStart : 24.669304132461548
Agent1_Critic_Loss : 1.9799832105636597
Agent1_Actor_Loss : -3.516110420227051
Agent1_Alpha_Loss : 4.97100830078125
Agent1_Temperature : 0.49910049028100273
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.636001586914062
Agent0_Eval_StdReturn : 28.118101119995117
Agent0_Eval_MaxReturn : 13.140093803405762
Agent0_Eval_MinReturn : -72.26290130615234
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -45.64712905883789
Agent0_Train_StdReturn : 20.86956787109375
Agent0_Train_MaxReturn : -11.88830280303955
Agent0_Train_MinReturn : -87.54824829101562
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 10500
Agent0_TimeSinceStart : 26.862282752990723
Agent0_Critic_Loss : 2.407615900039673
Agent0_Actor_Loss : -3.620788097381592
Agent0_Alpha_Loss : 4.972073554992676
Agent0_Temperature : 0.4989507738978775
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -51.34430694580078
Agent1_Eval_StdReturn : 33.24541091918945
Agent1_Eval_MaxReturn : -11.256982803344727
Agent1_Eval_MinReturn : -135.25222778320312
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -47.674034118652344
Agent1_Train_StdReturn : 36.415184020996094
Agent1_Train_MaxReturn : 2.0266284942626953
Agent1_Train_MinReturn : -119.54708862304688
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 10500
Agent1_TimeSinceStart : 29.054791688919067
Agent1_Critic_Loss : 1.7472151517868042
Agent1_Actor_Loss : -3.6867384910583496
Agent1_Alpha_Loss : 4.98563289642334
Agent1_Temperature : 0.4989506718275251
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -36.58491897583008
Agent0_Eval_StdReturn : 15.311973571777344
Agent0_Eval_MaxReturn : -4.047370910644531
Agent0_Eval_MinReturn : -63.698402404785156
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.258689880371094
Agent0_Train_StdReturn : 25.352930068969727
Agent0_Train_MaxReturn : -16.544532775878906
Agent0_Train_MinReturn : -96.87101745605469
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 12000
Agent0_TimeSinceStart : 31.263115644454956
Agent0_Critic_Loss : 1.9667459726333618
Agent0_Actor_Loss : -3.7279272079467773
Agent0_Alpha_Loss : 4.949680328369141
Agent0_Temperature : 0.4988010623559782
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -44.87451934814453
Agent1_Eval_StdReturn : 24.048837661743164
Agent1_Eval_MaxReturn : -14.754556655883789
Agent1_Eval_MinReturn : -102.10160827636719
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -41.78391647338867
Agent1_Train_StdReturn : 19.198673248291016
Agent1_Train_MaxReturn : -6.092344284057617
Agent1_Train_MinReturn : -69.25688934326172
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 12000
Agent1_TimeSinceStart : 33.503509759902954
Agent1_Critic_Loss : 1.9523261785507202
Agent1_Actor_Loss : -3.8190250396728516
Agent1_Alpha_Loss : 4.971359729766846
Agent1_Temperature : 0.498800899636685
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -43.251625061035156
Agent0_Eval_StdReturn : 29.22828483581543
Agent0_Eval_MaxReturn : 11.073091506958008
Agent0_Eval_MinReturn : -82.90940856933594
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.7837028503418
Agent0_Train_StdReturn : 18.36772918701172
Agent0_Train_MaxReturn : -12.121785163879395
Agent0_Train_MinReturn : -76.35292053222656
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 13500
Agent0_TimeSinceStart : 35.759931564331055
Agent0_Critic_Loss : 1.9913897514343262
Agent0_Actor_Loss : -3.786006450653076
Agent0_Alpha_Loss : 5.01402473449707
Agent0_Temperature : 0.498651318972599
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -52.379859924316406
Agent1_Eval_StdReturn : 31.1191349029541
Agent1_Eval_MaxReturn : 20.256195068359375
Agent1_Eval_MinReturn : -86.3876953125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -17.203083038330078
Agent1_Train_StdReturn : 28.219057083129883
Agent1_Train_MaxReturn : 20.692066192626953
Agent1_Train_MinReturn : -77.69319152832031
Agent1_Train_AverageEpLen : 150.0


LOGGING TO:  /home/harvey/Documents/cs285/CS285-Project/cs285/scripts/../../data/sac_peer2_agents_rpb20000_temp0.1_HalfCheetah-v4_09-12-2022_02-17-56 



########################
logging outputs to  /home/harvey/Documents/cs285/CS285-Project/cs285/scripts/../../data/sac_peer2_agents_rpb20000_temp0.1_HalfCheetah-v4_09-12-2022_02-17-56
########################
Using GPU id 0


********** Iteration 0 ************

Collecting data to be used for training...

Training agent...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.402889251708984
Agent0_Eval_StdReturn : 29.959867477416992
Agent0_Eval_MaxReturn : 2.8824901580810547
Agent0_Eval_MinReturn : -88.16786193847656
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -68.51490020751953
Agent0_Train_StdReturn : 35.8519172668457
Agent0_Train_MaxReturn : 10.17027473449707
Agent0_Train_MinReturn : -109.59283447265625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 1500
Agent0_TimeSinceStart : 2.3464815616607666
Agent0_Critic_Loss : 1.715803623199463
Agent0_Actor_Loss : -0.3432028293609619
Agent0_Alpha_Loss : 0.9863041639328003
Agent0_Temperature : 0.09997000449985415
Agent0_Initial_DataCollection_AverageReturn : -68.51490020751953
Done logging...



Collecting data to be used for training...

Training agent...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -54.462310791015625
Agent1_Eval_StdReturn : 37.87112045288086
Agent1_Eval_MaxReturn : 15.620088577270508
Agent1_Eval_MinReturn : -98.72659301757812
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -41.37040710449219
Agent1_Train_StdReturn : 23.476428985595703
Agent1_Train_MaxReturn : -3.7883758544921875
Agent1_Train_MinReturn : -74.03202819824219
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 1500
Agent1_TimeSinceStart : 4.526468753814697
Agent1_Critic_Loss : 1.1657971143722534
Agent1_Actor_Loss : -0.489025354385376
Agent1_Alpha_Loss : 0.980064868927002
Agent1_Temperature : 0.09997000449985606
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.50178146362305
Agent0_Eval_StdReturn : 28.27206039428711
Agent0_Eval_MaxReturn : -15.729722023010254
Agent0_Eval_MinReturn : -118.12063598632812
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -57.548858642578125
Agent0_Train_StdReturn : 36.27107238769531
Agent0_Train_MaxReturn : 21.940706253051758
Agent0_Train_MinReturn : -109.1875991821289
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 3000
Agent0_TimeSinceStart : 6.709948778152466
Agent0_Critic_Loss : 1.377016544342041
Agent0_Actor_Loss : -0.3033008575439453
Agent0_Alpha_Loss : 0.988323450088501
Agent0_Temperature : 0.09994001641427745
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -50.8686637878418
Agent1_Eval_StdReturn : 34.60274887084961
Agent1_Eval_MaxReturn : -4.3455047607421875
Agent1_Eval_MinReturn : -124.91232299804688
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -69.23501586914062
Agent1_Train_StdReturn : 29.758995056152344
Agent1_Train_MaxReturn : -34.14296340942383
Agent1_Train_MinReturn : -133.24810791015625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 3000
Agent1_TimeSinceStart : 8.890664339065552
Agent1_Critic_Loss : 0.9427248239517212
Agent1_Actor_Loss : -0.4349209666252136
Agent1_Alpha_Loss : 0.9940054416656494
Agent1_Temperature : 0.09994000770713929
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -51.574249267578125
Agent0_Eval_StdReturn : 26.604427337646484
Agent0_Eval_MaxReturn : 5.149239540100098
Agent0_Eval_MinReturn : -88.93110656738281
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -38.46282196044922
Agent0_Train_StdReturn : 33.80742645263672
Agent0_Train_MaxReturn : 32.84455871582031
Agent0_Train_MinReturn : -87.79841613769531
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 4500
Agent0_TimeSinceStart : 11.08657455444336
Agent0_Critic_Loss : 1.102364182472229
Agent0_Actor_Loss : -0.3328079581260681
Agent0_Alpha_Loss : 0.9912914633750916
Agent0_Temperature : 0.09991003370309641
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -30.823165893554688
Agent1_Eval_StdReturn : 29.782846450805664
Agent1_Eval_MaxReturn : 17.222949981689453
Agent1_Eval_MinReturn : -76.0450210571289
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -45.503578186035156
Agent1_Train_StdReturn : 28.42767906188965
Agent1_Train_MaxReturn : -11.233759880065918
Agent1_Train_MinReturn : -92.6666259765625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 4500
Agent1_TimeSinceStart : 13.291029453277588
Agent1_Critic_Loss : 0.8594810366630554
Agent1_Actor_Loss : -0.3383888602256775
Agent1_Alpha_Loss : 0.9892613291740417
Agent1_Temperature : 0.09991002137555709
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -50.588409423828125
Agent0_Eval_StdReturn : 26.644908905029297
Agent0_Eval_MaxReturn : 0.9879150390625
Agent0_Eval_MinReturn : -100.88360595703125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.40509796142578
Agent0_Train_StdReturn : 34.74382781982422
Agent0_Train_MaxReturn : 17.335905075073242
Agent0_Train_MinReturn : -81.64083099365234
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 6000
Agent0_TimeSinceStart : 15.537362813949585
Agent0_Critic_Loss : 1.1021710634231567
Agent0_Actor_Loss : -0.3556218445301056
Agent0_Alpha_Loss : 0.9930912852287292
Agent0_Temperature : 0.09988005611951914
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -24.487438201904297
Agent1_Eval_StdReturn : 17.571754455566406
Agent1_Eval_MaxReturn : 10.2545804977417
Agent1_Eval_MinReturn : -48.557613372802734
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -33.95558547973633
Agent1_Train_StdReturn : 34.20610809326172
Agent1_Train_MaxReturn : 23.963790893554688
Agent1_Train_MinReturn : -96.34185028076172
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 6000
Agent1_TimeSinceStart : 17.791240215301514
Agent1_Critic_Loss : 0.9638923406600952
Agent1_Actor_Loss : -0.3634716868400574
Agent1_Alpha_Loss : 0.9950607419013977
Agent1_Temperature : 0.0998800378833569
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -33.32909393310547
Agent0_Eval_StdReturn : 42.44218826293945
Agent0_Eval_MaxReturn : 26.87948226928711
Agent0_Eval_MinReturn : -105.79582214355469
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -66.22557067871094
Agent0_Train_StdReturn : 39.42101287841797
Agent0_Train_MaxReturn : 21.908092498779297
Agent1_Train_EnvstepsSoFar : 13500
Agent1_TimeSinceStart : 38.014347076416016
Agent1_Critic_Loss : 2.212939977645874
Agent1_Actor_Loss : -3.9558796882629395
Agent1_Alpha_Loss : 4.9647016525268555
Agent1_Temperature : 0.49865118422926064
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -43.55967712402344
Agent0_Eval_StdReturn : 22.83901023864746
Agent0_Eval_MaxReturn : -2.845803737640381
Agent0_Eval_MinReturn : -66.39705657958984
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.1032829284668
Agent0_Train_StdReturn : 20.960372924804688
Agent0_Train_MaxReturn : -6.281379699707031
Agent0_Train_MinReturn : -70.61060333251953
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 15000
Agent0_TimeSinceStart : 40.33706521987915
Agent0_Critic_Loss : 2.2507519721984863
Agent0_Actor_Loss : -3.8117289543151855
Agent0_Alpha_Loss : 4.948017120361328
Agent0_Temperature : 0.49850166729126083
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -41.37895584106445
Agent1_Eval_StdReturn : 33.066383361816406
Agent1_Eval_MaxReturn : 25.166414260864258
Agent1_Eval_MinReturn : -79.58399200439453
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.88693618774414
Agent1_Train_StdReturn : 34.691444396972656
Agent1_Train_MaxReturn : 9.763589859008789
Agent1_Train_MinReturn : -111.24258422851562
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 15000
Agent1_TimeSinceStart : 42.68465876579285
Agent1_Critic_Loss : 2.2654690742492676
Agent1_Actor_Loss : -4.0520524978637695
Agent1_Alpha_Loss : 4.963444709777832
Agent1_Temperature : 0.49850152523779157
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.58674621582031
Agent0_Eval_StdReturn : 37.692649841308594
Agent0_Eval_MaxReturn : 17.127513885498047
Agent0_Eval_MinReturn : -100.77119445800781
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -51.95970916748047
Agent0_Train_StdReturn : 32.93670654296875
Agent0_Train_MaxReturn : -10.346031188964844
Agent0_Train_MinReturn : -109.96633911132812
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 16500
Agent0_TimeSinceStart : 45.04816937446594
Agent0_Critic_Loss : 2.3136606216430664
Agent0_Actor_Loss : -3.8558783531188965
Agent0_Alpha_Loss : 4.978328704833984
Agent0_Temperature : 0.4983520477106461
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -19.871440887451172
Agent1_Eval_StdReturn : 35.12031936645508
Agent1_Eval_MaxReturn : 49.430057525634766
Agent1_Eval_MinReturn : -63.9360237121582
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.03594207763672
Agent1_Train_StdReturn : 34.043941497802734
Agent1_Train_MaxReturn : 12.311777114868164
Agent1_Train_MinReturn : -88.30783081054688
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 16500
Agent1_TimeSinceStart : 47.431241273880005
Agent1_Critic_Loss : 1.7238051891326904
Agent1_Actor_Loss : -3.9339680671691895
Agent1_Alpha_Loss : 4.952925682067871
Agent1_Temperature : 0.49835193771906583
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -52.65763473510742
Agent0_Eval_StdReturn : 27.519956588745117
Agent0_Eval_MaxReturn : -1.658787727355957
Agent0_Eval_MinReturn : -110.40586853027344
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -50.14384460449219
Agent0_Train_StdReturn : 27.0695743560791
Agent0_Train_MaxReturn : -20.872777938842773
Agent0_Train_MinReturn : -119.70115661621094
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 18000
Agent0_TimeSinceStart : 49.88647389411926
Agent0_Critic_Loss : 1.8242566585540771
Agent0_Actor_Loss : -3.7449917793273926
Agent0_Alpha_Loss : 4.933387756347656
Agent0_Temperature : 0.4982025385704739
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -36.63887405395508
Agent1_Eval_StdReturn : 27.272472381591797
Agent1_Eval_MaxReturn : -0.6957448720932007
Agent1_Eval_MinReturn : -84.41105651855469
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -44.680633544921875
Agent1_Train_StdReturn : 39.239383697509766
Agent1_Train_MaxReturn : 2.683595657348633
Agent1_Train_MinReturn : -111.92955780029297
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 18000
Agent1_TimeSinceStart : 52.306814193725586
Agent1_Critic_Loss : 1.9515055418014526
Agent1_Actor_Loss : -4.000459671020508
Agent1_Alpha_Loss : 4.969908237457275
Agent1_Temperature : 0.49820238798150673
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -34.96162033081055
Agent0_Eval_StdReturn : 45.530941009521484
Agent0_Eval_MaxReturn : 29.70899772644043
Agent0_Eval_MinReturn : -113.39888763427734
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -28.961910247802734
Agent0_Train_StdReturn : 12.103324890136719
Agent0_Train_MaxReturn : -1.156097412109375
Agent0_Train_MinReturn : -49.5271110534668
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 19500
Agent0_TimeSinceStart : 54.75184726715088
Agent0_Critic_Loss : 2.083848237991333
Agent0_Actor_Loss : -3.730099678039551
Agent0_Alpha_Loss : 4.961269378662109
Agent0_Temperature : 0.49805307933076637
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -36.46963882446289
Agent1_Eval_StdReturn : 36.309200286865234
Agent1_Eval_MaxReturn : 19.405899047851562
Agent1_Eval_MinReturn : -110.51025390625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -33.7338752746582
Agent1_Train_StdReturn : 21.27507781982422
Agent1_Train_MaxReturn : 0.028430581092834473
Agent1_Train_MinReturn : -69.98695373535156
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 19500
Agent1_TimeSinceStart : 57.19960331916809
Agent1_Critic_Loss : 2.1979904174804688
Agent1_Actor_Loss : -3.922222852706909
Agent1_Alpha_Loss : 4.951564788818359
Agent1_Temperature : 0.4980529089691386
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -47.392433166503906
Agent0_Eval_StdReturn : 32.10380935668945
Agent0_Eval_MaxReturn : -1.3534088134765625
Agent0_Eval_MinReturn : -95.33128356933594
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -40.000938415527344
Agent0_Train_StdReturn : 29.16868019104004
Agent0_Train_MaxReturn : 23.4322452545166
Agent0_Train_MinReturn : -89.55661010742188
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 21000
Agent0_TimeSinceStart : 59.6655797958374
Agent0_Train_MinReturn : -110.35493469238281
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 7500
Agent0_TimeSinceStart : 20.06345534324646
Agent0_Critic_Loss : 1.0822765827178955
Agent0_Actor_Loss : -0.39376240968704224
Agent0_Alpha_Loss : 0.9935915470123291
Agent0_Temperature : 0.09985008470998938
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -40.25818634033203
Agent1_Eval_StdReturn : 26.791078567504883
Agent1_Eval_MaxReturn : 3.1373186111450195
Agent1_Eval_MinReturn : -83.35585021972656
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -50.955833435058594
Agent1_Train_StdReturn : 41.289024353027344
Agent1_Train_MaxReturn : 18.022506713867188
Agent1_Train_MinReturn : -147.89962768554688
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 7500
Agent1_TimeSinceStart : 22.396388292312622
Agent1_Critic_Loss : 0.8992829918861389
Agent1_Actor_Loss : -0.44546881318092346
Agent1_Alpha_Loss : 0.9916924238204956
Agent1_Temperature : 0.09985006431694493
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -47.35380172729492
Agent0_Eval_StdReturn : 25.08116340637207
Agent0_Eval_MaxReturn : 9.244424819946289
Agent0_Eval_MinReturn : -85.44532775878906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -50.45553207397461
Agent0_Train_StdReturn : 33.46944808959961
Agent0_Train_MaxReturn : 11.840235710144043
Agent0_Train_MinReturn : -101.03453063964844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 9000
Agent0_TimeSinceStart : 24.751799821853638
Agent0_Critic_Loss : 1.1015925407409668
Agent0_Actor_Loss : -0.36635810136795044
Agent0_Alpha_Loss : 0.9906597137451172
Agent0_Temperature : 0.0998201246463585
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.232398986816406
Agent1_Eval_StdReturn : 14.581247329711914
Agent1_Eval_MaxReturn : -20.574586868286133
Agent1_Eval_MinReturn : -67.82565307617188
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -26.3101863861084
Agent1_Train_StdReturn : 26.204486846923828
Agent1_Train_MaxReturn : 24.030872344970703
Agent1_Train_MinReturn : -67.97994232177734
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 9000
Agent1_TimeSinceStart : 27.128743171691895
Agent1_Critic_Loss : 0.9869846105575562
Agent1_Actor_Loss : -0.5079582333564758
Agent1_Alpha_Loss : 0.9954215288162231
Agent1_Temperature : 0.09982009527359001
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -40.630680084228516
Agent0_Eval_StdReturn : 22.315242767333984
Agent0_Eval_MaxReturn : -9.70914077758789
Agent0_Eval_MinReturn : -76.70048522949219
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.466102600097656
Agent0_Train_StdReturn : 39.910888671875
Agent0_Train_MaxReturn : 8.614330291748047
Agent0_Train_MinReturn : -121.73418426513672
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 10500
Agent0_TimeSinceStart : 29.517807722091675
Agent0_Critic_Loss : 1.0934898853302002
Agent0_Actor_Loss : -0.42531245946884155
Agent0_Alpha_Loss : 0.9934787154197693
Agent0_Temperature : 0.09979017113648583
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.02058410644531
Agent1_Eval_StdReturn : 26.271907806396484
Agent1_Eval_MaxReturn : -5.555606842041016
Agent1_Eval_MinReturn : -104.22146606445312
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -45.053218841552734
Agent1_Train_StdReturn : 41.627750396728516
Agent1_Train_MaxReturn : 19.20272445678711
Agent1_Train_MinReturn : -128.7860565185547
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 10500
Agent1_TimeSinceStart : 31.967753171920776
Agent1_Critic_Loss : 1.0880341529846191
Agent1_Actor_Loss : -0.5598553419113159
Agent1_Alpha_Loss : 0.9990009069442749
Agent1_Temperature : 0.0997901271381291
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.87876892089844
Agent0_Eval_StdReturn : 23.855377197265625
Agent0_Eval_MaxReturn : -2.854642868041992
Agent0_Eval_MinReturn : -84.77202606201172
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -63.14087677001953
Agent0_Train_StdReturn : 25.70836067199707
Agent0_Train_MaxReturn : -31.009246826171875
Agent0_Train_MinReturn : -108.109130859375
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 12000
Agent0_TimeSinceStart : 34.40171027183533
Agent0_Critic_Loss : 1.1183881759643555
Agent0_Actor_Loss : -0.41472378373146057
Agent0_Alpha_Loss : 0.9888871908187866
Agent0_Temperature : 0.0997602318752047
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.02485656738281
Agent1_Eval_StdReturn : 31.597360610961914
Agent1_Eval_MaxReturn : 11.284029960632324
Agent1_Eval_MinReturn : -111.11717224121094
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -39.857757568359375
Agent1_Train_StdReturn : 27.220069885253906
Agent1_Train_MaxReturn : -3.808542251586914
Agent1_Train_MinReturn : -96.26510620117188
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 12000
Agent1_TimeSinceStart : 36.853174924850464
Agent1_Critic_Loss : 1.0833048820495605
Agent1_Actor_Loss : -0.5539859533309937
Agent1_Alpha_Loss : 0.9923879504203796
Agent1_Temperature : 0.09976017222999907
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.561187744140625
Agent0_Eval_StdReturn : 25.478744506835938
Agent0_Eval_MaxReturn : 2.7093191146850586
Agent0_Eval_MinReturn : -97.03373718261719
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.364341735839844
Agent0_Train_StdReturn : 27.04146957397461
Agent0_Train_MaxReturn : 18.82952117919922
Agent0_Train_MinReturn : -91.18022155761719
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 13500
Agent0_TimeSinceStart : 39.31307506561279
Agent0_Critic_Loss : 0.9186875820159912
Agent0_Actor_Loss : -0.414511501789093
Agent0_Alpha_Loss : 0.9984827041625977
Agent0_Temperature : 0.09973029066263002
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -33.81681442260742
Agent1_Eval_StdReturn : 25.52098274230957
Agent1_Eval_MaxReturn : 11.80908489227295
Agent1_Eval_MinReturn : -83.94857788085938
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -32.996498107910156
Agent1_Train_StdReturn : 23.799388885498047
Agent1_Train_MaxReturn : 9.359731674194336
Agent1_Train_MinReturn : -65.46714782714844
Agent0_Critic_Loss : 1.905556082725525
Agent0_Actor_Loss : -3.638218641281128
Agent0_Alpha_Loss : 4.960024833679199
Agent0_Temperature : 0.4979036712750414
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.01837921142578
Agent1_Eval_StdReturn : 38.04998016357422
Agent1_Eval_MaxReturn : 11.0285062789917
Agent1_Eval_MinReturn : -119.98128509521484
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -22.881996154785156
Agent1_Train_StdReturn : 29.9969425201416
Agent1_Train_MaxReturn : 11.160066604614258
Agent1_Train_MinReturn : -85.28504943847656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 21000
Agent1_TimeSinceStart : 62.15090370178223
Agent1_Critic_Loss : 1.8710483312606812
Agent1_Actor_Loss : -3.9023280143737793
Agent1_Alpha_Loss : 4.937586784362793
Agent1_Temperature : 0.49790352040079044
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -47.3806266784668
Agent0_Eval_StdReturn : 31.78101348876953
Agent0_Eval_MaxReturn : -16.46893882751465
Agent0_Eval_MinReturn : -127.37288665771484
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -41.30103302001953
Agent0_Train_StdReturn : 34.453773498535156
Agent0_Train_MaxReturn : 7.077362060546875
Agent0_Train_MinReturn : -91.34927368164062
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 22500
Agent0_TimeSinceStart : 64.65090656280518
Agent0_Critic_Loss : 1.5266923904418945
Agent0_Actor_Loss : -3.54866886138916
Agent0_Alpha_Loss : 4.943473815917969
Agent0_Temperature : 0.49775434266045004
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.37323760986328
Agent1_Eval_StdReturn : 25.3554630279541
Agent1_Eval_MaxReturn : 9.037692070007324
Agent1_Eval_MinReturn : -80.67838287353516
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -48.8111457824707
Agent1_Train_StdReturn : 32.026241302490234
Agent1_Train_MaxReturn : 12.622520446777344
Agent1_Train_MinReturn : -97.61729431152344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 22500
Agent1_TimeSinceStart : 67.13770914077759
Agent1_Critic_Loss : 1.8910832405090332
Agent1_Actor_Loss : -3.8905694484710693
Agent1_Alpha_Loss : 4.953516960144043
Agent1_Temperature : 0.4977541858152084
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.573917388916016
Agent0_Eval_StdReturn : 31.889850616455078
Agent0_Eval_MaxReturn : -0.16768741607666016
Agent0_Eval_MinReturn : -94.74449157714844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -47.561622619628906
Agent0_Train_StdReturn : 36.81809997558594
Agent0_Train_MaxReturn : 9.194551467895508
Agent0_Train_MinReturn : -88.50279235839844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 24000
Agent0_TimeSinceStart : 69.63439440727234
Agent0_Critic_Loss : 1.6659984588623047
Agent0_Actor_Loss : -3.447774648666382
Agent0_Alpha_Loss : 4.951720714569092
Agent0_Temperature : 0.4976050725858039
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -24.065919876098633
Agent1_Eval_StdReturn : 30.489397048950195
Agent1_Eval_MaxReturn : 34.22461700439453
Agent1_Eval_MinReturn : -72.13082122802734
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -47.002288818359375
Agent1_Train_StdReturn : 34.50031280517578
Agent1_Train_MaxReturn : -2.0938127040863037
Agent1_Train_MinReturn : -133.66278076171875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 24000
Agent1_TimeSinceStart : 72.14046502113342
Agent1_Critic_Loss : 1.8279187679290771
Agent1_Actor_Loss : -3.8236327171325684
Agent1_Alpha_Loss : 4.980724334716797
Agent1_Temperature : 0.4976048547049438
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.79832458496094
Agent0_Eval_StdReturn : 25.739990234375
Agent0_Eval_MaxReturn : -0.5996317863464355
Agent0_Eval_MinReturn : -78.02816009521484
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.31233215332031
Agent0_Train_StdReturn : 27.75354766845703
Agent0_Train_MaxReturn : -4.263675689697266
Agent0_Train_MinReturn : -94.62916564941406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 25500
Agent0_TimeSinceStart : 74.66650009155273
Agent0_Critic_Loss : 1.621220588684082
Agent0_Actor_Loss : -3.440688133239746
Agent0_Alpha_Loss : 4.933727264404297
Agent0_Temperature : 0.4974558916493702
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -47.361366271972656
Agent1_Eval_StdReturn : 34.057613372802734
Agent1_Eval_MaxReturn : -5.989035606384277
Agent1_Eval_MinReturn : -95.07817077636719
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -48.18928527832031
Agent1_Train_StdReturn : 39.512210845947266
Agent1_Train_MaxReturn : 19.000518798828125
Agent1_Train_MinReturn : -117.16372680664062
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 25500
Agent1_TimeSinceStart : 77.19388055801392
Agent1_Critic_Loss : 1.7714354991912842
Agent1_Actor_Loss : -3.7467751502990723
Agent1_Alpha_Loss : 4.964450836181641
Agent1_Temperature : 0.49745556365538623
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.55059051513672
Agent0_Eval_StdReturn : 21.51689338684082
Agent0_Eval_MaxReturn : 9.46875
Agent0_Eval_MinReturn : -72.0660629272461
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -47.41785430908203
Agent0_Train_StdReturn : 17.52119255065918
Agent0_Train_MaxReturn : -10.594505310058594
Agent0_Train_MinReturn : -68.02523803710938
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 27000
Agent0_TimeSinceStart : 79.7078161239624
Agent0_Critic_Loss : 1.8167040348052979
Agent0_Actor_Loss : -3.3845252990722656
Agent0_Alpha_Loss : 4.952118873596191
Agent0_Temperature : 0.49730675826468507
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -43.947513580322266
Agent1_Eval_StdReturn : 28.71098518371582
Agent1_Eval_MaxReturn : 21.66482162475586
Agent1_Eval_MinReturn : -85.99301147460938
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -62.7984504699707
Agent1_Train_StdReturn : 30.17755126953125
Agent1_Train_MaxReturn : -10.602132797241211
Agent1_Train_MinReturn : -96.33815002441406
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 27000
Agent1_TimeSinceStart : 82.21109795570374
Agent1_Critic_Loss : 1.8641713857650757
Agent1_Actor_Loss : -3.7583673000335693
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 13500
Agent1_TimeSinceStart : 41.778106927871704
Agent1_Critic_Loss : 0.8290349245071411
Agent1_Actor_Loss : -0.6366333961486816
Agent1_Alpha_Loss : 0.9948270320892334
Agent1_Temperature : 0.09973022578291707
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -66.75680541992188
Agent0_Eval_StdReturn : 29.9494686126709
Agent0_Eval_MaxReturn : -16.047746658325195
Agent0_Eval_MinReturn : -115.24150848388672
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.0987548828125
Agent0_Train_StdReturn : 34.17978286743164
Agent0_Train_MaxReturn : 27.16629981994629
Agent0_Train_MinReturn : -93.21746063232422
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 15000
Agent0_TimeSinceStart : 44.308748722076416
Agent0_Critic_Loss : 0.9619836211204529
Agent0_Actor_Loss : -0.3851199150085449
Agent0_Alpha_Loss : 0.9894185066223145
Agent0_Temperature : 0.09970036460678382
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -30.888113021850586
Agent1_Eval_StdReturn : 21.561796188354492
Agent1_Eval_MaxReturn : 4.353200912475586
Agent1_Eval_MinReturn : -63.088462829589844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -40.79240417480469
Agent1_Train_StdReturn : 37.412681579589844
Agent1_Train_MaxReturn : 2.8071231842041016
Agent1_Train_MinReturn : -110.79158020019531
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 15000
Agent1_TimeSinceStart : 46.819817543029785
Agent1_Critic_Loss : 0.8274048566818237
Agent1_Actor_Loss : -0.5814658403396606
Agent1_Alpha_Loss : 0.9918347001075745
Agent1_Temperature : 0.09970029279696951
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -39.81513214111328
Agent0_Eval_StdReturn : 30.084787368774414
Agent0_Eval_MaxReturn : 14.41203498840332
Agent0_Eval_MinReturn : -90.2256851196289
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -67.01056671142578
Agent0_Train_StdReturn : 23.53095245361328
Agent0_Train_MaxReturn : -29.46395492553711
Agent0_Train_MinReturn : -97.24429321289062
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 16500
Agent0_TimeSinceStart : 49.307605266571045
Agent0_Critic_Loss : 1.0583548545837402
Agent0_Actor_Loss : -0.4019642174243927
Agent0_Alpha_Loss : 0.9910473823547363
Agent0_Temperature : 0.09967044971670572
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.590721130371094
Agent1_Eval_StdReturn : 30.895355224609375
Agent1_Eval_MaxReturn : 16.759552001953125
Agent1_Eval_MinReturn : -88.8189697265625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -35.78852081298828
Agent1_Train_StdReturn : 17.599164962768555
Agent1_Train_MaxReturn : -9.211185455322266
Agent1_Train_MinReturn : -69.4311752319336
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 16500
Agent1_TimeSinceStart : 51.812432289123535
Agent1_Critic_Loss : 0.8449898958206177
Agent1_Actor_Loss : -0.5698773860931396
Agent1_Alpha_Loss : 0.9871135950088501
Agent1_Temperature : 0.0996703801262041
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -45.7461051940918
Agent0_Eval_StdReturn : 29.040796279907227
Agent0_Eval_MaxReturn : -4.222467422485352
Agent0_Eval_MinReturn : -105.06190490722656
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.0339469909668
Agent0_Train_StdReturn : 30.65576934814453
Agent0_Train_MaxReturn : -3.5025510787963867
Agent0_Train_MinReturn : -106.82188415527344
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 18000
Agent0_TimeSinceStart : 54.340171813964844
Agent0_Critic_Loss : 0.9826845526695251
Agent0_Actor_Loss : -0.42697760462760925
Agent0_Alpha_Loss : 0.9846998453140259
Agent0_Temperature : 0.09964055626210715
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -42.12908172607422
Agent1_Eval_StdReturn : 16.902084350585938
Agent1_Eval_MaxReturn : -10.631082534790039
Agent1_Eval_MinReturn : -66.50272369384766
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -34.21439743041992
Agent1_Train_StdReturn : 25.58402442932129
Agent1_Train_MaxReturn : 9.26856803894043
Agent1_Train_MinReturn : -90.19670104980469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 18000
Agent1_TimeSinceStart : 56.874497175216675
Agent1_Critic_Loss : 0.8330356478691101
Agent1_Actor_Loss : -0.5763218402862549
Agent1_Alpha_Loss : 0.9922900199890137
Agent1_Temperature : 0.0996404767537474
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -31.534072875976562
Agent0_Eval_StdReturn : 33.47090530395508
Agent0_Eval_MaxReturn : 21.781944274902344
Agent0_Eval_MinReturn : -81.9556655883789
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.152973175048828
Agent0_Train_StdReturn : 18.83197784423828
Agent0_Train_MaxReturn : -0.6530666351318359
Agent0_Train_MinReturn : -61.5647087097168
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 19500
Agent0_TimeSinceStart : 59.40335130691528
Agent0_Critic_Loss : 0.9682410359382629
Agent0_Actor_Loss : -0.43950146436691284
Agent0_Alpha_Loss : 0.9894790649414062
Agent0_Temperature : 0.09961067357134258
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -31.655248641967773
Agent1_Eval_StdReturn : 32.008968353271484
Agent1_Eval_MaxReturn : 29.773178100585938
Agent1_Eval_MinReturn : -76.6165771484375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -27.0517520904541
Agent1_Train_StdReturn : 27.527090072631836
Agent1_Train_MaxReturn : 21.604961395263672
Agent1_Train_MinReturn : -66.89344024658203
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 19500
Agent1_TimeSinceStart : 61.94471073150635
Agent1_Critic_Loss : 0.8952416777610779
Agent1_Actor_Loss : -0.5653643012046814
Agent1_Alpha_Loss : 0.9848805665969849
Agent1_Temperature : 0.09961059535164335
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -33.72126770019531
Agent0_Eval_StdReturn : 49.38864517211914
Agent0_Eval_MaxReturn : 66.48100280761719
Agent0_Eval_MinReturn : -102.85629272460938
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -37.219261169433594
Agent0_Train_StdReturn : 19.334796905517578
Agent0_Train_MaxReturn : -1.608297348022461
Agent0_Train_MinReturn : -65.60542297363281
Agent0_Train_AverageEpLen : 150.0
Agent1_Alpha_Loss : 4.969808578491211
Agent1_Temperature : 0.49730630359624534
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -40.65614318847656
Agent0_Eval_StdReturn : 21.97581672668457
Agent0_Eval_MaxReturn : 16.64892578125
Agent0_Eval_MinReturn : -74.43067932128906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.10875701904297
Agent0_Train_StdReturn : 23.92666244506836
Agent0_Train_MaxReturn : -7.7208709716796875
Agent0_Train_MinReturn : -89.71711730957031
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 28500
Agent0_TimeSinceStart : 84.73090076446533
Agent0_Critic_Loss : 2.0711777210235596
Agent0_Actor_Loss : -3.4546637535095215
Agent0_Alpha_Loss : 4.995589733123779
Agent0_Temperature : 0.49715759019621786
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.84488296508789
Agent1_Eval_StdReturn : 32.81758117675781
Agent1_Eval_MaxReturn : 13.265523910522461
Agent1_Eval_MinReturn : -82.87053680419922
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -57.3309440612793
Agent1_Train_StdReturn : 36.84581756591797
Agent1_Train_MaxReturn : -13.069101333618164
Agent1_Train_MinReturn : -111.18499755859375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 28500
Agent1_TimeSinceStart : 87.24506187438965
Agent1_Critic_Loss : 1.928434133529663
Agent1_Actor_Loss : -3.759875535964966
Agent1_Alpha_Loss : 4.983705520629883
Agent1_Temperature : 0.49715705077038297
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -33.007930755615234
Agent0_Eval_StdReturn : 26.878427505493164
Agent0_Eval_MaxReturn : 29.06951904296875
Agent0_Eval_MinReturn : -62.9310302734375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -31.38849449157715
Agent0_Train_StdReturn : 25.225379943847656
Agent0_Train_MaxReturn : 4.781787872314453
Agent0_Train_MinReturn : -82.95339965820312
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 30000
Agent0_TimeSinceStart : 89.76794195175171
Agent0_Critic_Loss : 1.6670711040496826
Agent0_Actor_Loss : -3.3963704109191895
Agent0_Alpha_Loss : 4.961721420288086
Agent0_Temperature : 0.4970084638426988
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -57.55999755859375
Agent1_Eval_StdReturn : 32.62877655029297
Agent1_Eval_MaxReturn : 20.58182144165039
Agent1_Eval_MinReturn : -92.14759826660156
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -43.779415130615234
Agent1_Train_StdReturn : 31.010793685913086
Agent1_Train_MaxReturn : 14.349384307861328
Agent1_Train_MinReturn : -88.81163787841797
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 30000
Agent1_TimeSinceStart : 92.27936744689941
Agent1_Critic_Loss : 2.0095267295837402
Agent1_Actor_Loss : -3.6920127868652344
Agent1_Alpha_Loss : 5.006295680999756
Agent1_Temperature : 0.49700776837476457
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -34.854408264160156
Agent0_Eval_StdReturn : 25.6860408782959
Agent0_Eval_MaxReturn : 9.712474822998047
Agent0_Eval_MinReturn : -84.63978576660156
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -21.97072982788086
Agent0_Train_StdReturn : 32.974510192871094
Agent0_Train_MaxReturn : 20.424118041992188
Agent0_Train_MinReturn : -82.01615905761719
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 31500
Agent0_TimeSinceStart : 94.80643796920776
Agent0_Critic_Loss : 1.6653802394866943
Agent0_Actor_Loss : -3.3606033325195312
Agent0_Alpha_Loss : 4.939201354980469
Agent0_Temperature : 0.4968594231506775
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -49.016231536865234
Agent1_Eval_StdReturn : 26.3259334564209
Agent1_Eval_MaxReturn : -21.494171142578125
Agent1_Eval_MinReturn : -112.16551208496094
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -39.86219024658203
Agent1_Train_StdReturn : 31.762866973876953
Agent1_Train_MaxReturn : 9.942300796508789
Agent1_Train_MinReturn : -78.59821319580078
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 31500
Agent1_TimeSinceStart : 97.33653426170349
Agent1_Critic_Loss : 1.9765053987503052
Agent1_Actor_Loss : -3.5079798698425293
Agent1_Alpha_Loss : 4.915196418762207
Agent1_Temperature : 0.49685864348049197
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.46253204345703
Agent0_Eval_StdReturn : 38.453060150146484
Agent0_Eval_MaxReturn : 5.226772308349609
Agent0_Eval_MinReturn : -131.73501586914062
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -38.08604431152344
Agent0_Train_StdReturn : 42.39235305786133
Agent0_Train_MaxReturn : 20.05987548828125
Agent0_Train_MinReturn : -121.66719818115234
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 33000
Agent0_TimeSinceStart : 99.86680698394775
Agent0_Critic_Loss : 1.9088612794876099
Agent0_Actor_Loss : -3.3324084281921387
Agent0_Alpha_Loss : 4.928953170776367
Agent0_Temperature : 0.496710482158537
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -62.23529052734375
Agent1_Eval_StdReturn : 35.44353485107422
Agent1_Eval_MaxReturn : 2.025941848754883
Agent1_Eval_MinReturn : -114.83418273925781
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -57.638511657714844
Agent1_Train_StdReturn : 37.516014099121094
Agent1_Train_MaxReturn : 34.24092102050781
Agent1_Train_MinReturn : -91.4760513305664
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 33000
Agent1_TimeSinceStart : 102.39018106460571
Agent1_Critic_Loss : 1.8252685070037842
Agent1_Actor_Loss : -3.535412311553955
Agent1_Alpha_Loss : 4.929129123687744
Agent1_Temperature : 0.49670963254404404
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -32.70166778564453
Agent0_Eval_StdReturn : 29.62781524658203
Agent0_Eval_MaxReturn : 18.037853240966797
Agent0_Eval_MinReturn : -97.42040252685547
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.026058197021484
Agent0_Train_StdReturn : 31.545129776000977
Agent0_Train_MaxReturn : 17.574777603149414
Agent0_Train_MinReturn : -91.99200439453125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 34500
Agent0_TimeSinceStart : 104.91129922866821
Agent0_Critic_Loss : 1.908015251159668
Agent0_Actor_Loss : -3.3195786476135254
Agent0_Alpha_Loss : 4.919437408447266
Agent0_Temperature : 0.4965616516950256
Agent0_Train_EnvstepsSoFar : 21000
Agent0_TimeSinceStart : 64.47099733352661
Agent0_Critic_Loss : 0.7813681364059448
Agent0_Actor_Loss : -0.39860445261001587
Agent0_Alpha_Loss : 0.987784206867218
Agent0_Temperature : 0.09958080424891966
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -19.710739135742188
Agent1_Eval_StdReturn : 24.433765411376953
Agent1_Eval_MaxReturn : 26.642452239990234
Agent1_Eval_MinReturn : -59.368492126464844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -38.117759704589844
Agent1_Train_StdReturn : 28.894330978393555
Agent1_Train_MaxReturn : -1.1950052976608276
Agent1_Train_MinReturn : -90.51670837402344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 21000
Agent1_TimeSinceStart : 67.02495837211609
Agent1_Critic_Loss : 0.8246204853057861
Agent1_Actor_Loss : -0.5802809000015259
Agent1_Alpha_Loss : 0.9864833950996399
Agent1_Temperature : 0.09958073066069689
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -32.777488708496094
Agent0_Eval_StdReturn : 19.897865295410156
Agent0_Eval_MaxReturn : -1.5067481994628906
Agent0_Eval_MinReturn : -61.37498092651367
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -48.36101531982422
Agent0_Train_StdReturn : 34.45298767089844
Agent0_Train_MaxReturn : 8.035160064697266
Agent0_Train_MinReturn : -92.18748474121094
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 22500
Agent0_TimeSinceStart : 69.56659579277039
Agent0_Critic_Loss : 0.8562659621238708
Agent0_Actor_Loss : -0.4295939803123474
Agent0_Alpha_Loss : 0.9844211339950562
Agent0_Temperature : 0.09955095346803891
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -48.22343826293945
Agent1_Eval_StdReturn : 33.65849685668945
Agent1_Eval_MaxReturn : -9.01915168762207
Agent1_Eval_MinReturn : -128.49716186523438
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -45.236228942871094
Agent1_Train_StdReturn : 11.513681411743164
Agent1_Train_MaxReturn : -29.36355972290039
Agent1_Train_MinReturn : -62.39009094238281
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 22500
Agent1_TimeSinceStart : 72.11577200889587
Agent1_Critic_Loss : 0.764254093170166
Agent1_Actor_Loss : -0.6205582618713379
Agent1_Alpha_Loss : 0.9897716641426086
Agent1_Temperature : 0.09955087539775707
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -45.38914108276367
Agent0_Eval_StdReturn : 38.42457962036133
Agent0_Eval_MaxReturn : 26.373926162719727
Agent0_Eval_MinReturn : -118.87615203857422
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.20920944213867
Agent0_Train_StdReturn : 35.794376373291016
Agent0_Train_MaxReturn : 46.520469665527344
Agent0_Train_MinReturn : -96.70504760742188
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 24000
Agent0_TimeSinceStart : 74.65501260757446
Agent0_Critic_Loss : 0.8482153415679932
Agent0_Actor_Loss : -0.4209645986557007
Agent0_Alpha_Loss : 0.9901190400123596
Agent0_Temperature : 0.09952110921103322
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.687374114990234
Agent1_Eval_StdReturn : 26.507522583007812
Agent1_Eval_MaxReturn : 15.291105270385742
Agent1_Eval_MinReturn : -61.0766716003418
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -28.539648056030273
Agent1_Train_StdReturn : 30.10036849975586
Agent1_Train_MaxReturn : 41.793949127197266
Agent1_Train_MinReturn : -73.32518005371094
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 24000
Agent1_TimeSinceStart : 77.19770407676697
Agent1_Critic_Loss : 0.7683751583099365
Agent1_Actor_Loss : -0.5987086296081543
Agent1_Alpha_Loss : 0.9936906695365906
Agent1_Temperature : 0.09952102237600079
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.97985076904297
Agent0_Eval_StdReturn : 28.544506072998047
Agent0_Eval_MaxReturn : 4.405790328979492
Agent0_Eval_MinReturn : -87.29347229003906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -46.6693229675293
Agent0_Train_StdReturn : 37.92164611816406
Agent0_Train_MaxReturn : -4.001792907714844
Agent0_Train_MinReturn : -139.56893920898438
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 25500
Agent0_TimeSinceStart : 79.74314188957214
Agent0_Critic_Loss : 0.8094382286071777
Agent0_Actor_Loss : -0.4556485414505005
Agent0_Alpha_Loss : 0.9860867261886597
Agent0_Temperature : 0.09949127922453525
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.21317672729492
Agent1_Eval_StdReturn : 37.34050369262695
Agent1_Eval_MaxReturn : 33.15092468261719
Agent1_Eval_MinReturn : -88.76345825195312
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -49.1035041809082
Agent1_Train_StdReturn : 13.904664039611816
Agent1_Train_MaxReturn : -26.96491241455078
Agent1_Train_MinReturn : -73.73807525634766
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 25500
Agent1_TimeSinceStart : 82.29475736618042
Agent1_Critic_Loss : 0.694168210029602
Agent1_Actor_Loss : -0.61127769947052
Agent1_Alpha_Loss : 0.9912681579589844
Agent1_Temperature : 0.09949117710648751
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -43.713680267333984
Agent0_Eval_StdReturn : 27.147499084472656
Agent0_Eval_MaxReturn : 11.052845001220703
Agent0_Eval_MinReturn : -84.48927307128906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -77.83245849609375
Agent0_Train_StdReturn : 30.874374389648438
Agent0_Train_MaxReturn : -24.425270080566406
Agent0_Train_MinReturn : -126.28707885742188
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 27000
Agent0_TimeSinceStart : 84.8412971496582
Agent0_Critic_Loss : 0.8118811249732971
Agent0_Actor_Loss : -0.44492557644844055
Agent0_Alpha_Loss : 0.9913936853408813
Agent0_Temperature : 0.09946145270935423
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -51.94208908081055
Agent1_Eval_StdReturn : 25.624059677124023
Agent1_Eval_MaxReturn : -19.244129180908203
Agent1_Eval_MinReturn : -96.98492431640625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -44.09136199951172
Agent1_Train_StdReturn : 23.416706085205078
Agent1_Train_MaxReturn : -1.6872682571411133
Agent1_Train_MinReturn : -82.81083679199219
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 27000
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -55.320579528808594
Agent1_Eval_StdReturn : 30.74994468688965
Agent1_Eval_MaxReturn : 8.601322174072266
Agent1_Eval_MinReturn : -91.41368103027344
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -28.477489471435547
Agent1_Train_StdReturn : 32.293827056884766
Agent1_Train_MaxReturn : 16.717510223388672
Agent1_Train_MinReturn : -88.08846282958984
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 34500
Agent1_TimeSinceStart : 107.42620730400085
Agent1_Critic_Loss : 1.7052412033081055
Agent1_Actor_Loss : -3.539618968963623
Agent1_Alpha_Loss : 4.935779571533203
Agent1_Temperature : 0.49656071246741135
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -51.27207565307617
Agent0_Eval_StdReturn : 25.159696578979492
Agent0_Eval_MaxReturn : -2.4896297454833984
Agent0_Eval_MinReturn : -88.86474609375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -40.14439392089844
Agent0_Train_StdReturn : 36.88702392578125
Agent0_Train_MaxReturn : 27.46839714050293
Agent0_Train_MinReturn : -92.7503890991211
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 36000
Agent0_TimeSinceStart : 109.95088338851929
Agent0_Critic_Loss : 2.090303421020508
Agent0_Actor_Loss : -3.4015440940856934
Agent0_Alpha_Loss : 4.968493461608887
Agent0_Temperature : 0.49641282386981667
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -55.545372009277344
Agent1_Eval_StdReturn : 25.096454620361328
Agent1_Eval_MaxReturn : -1.0930304527282715
Agent1_Eval_MinReturn : -83.38623046875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -31.806732177734375
Agent1_Train_StdReturn : 24.96828842163086
Agent1_Train_MaxReturn : 1.8823437690734863
Agent1_Train_MinReturn : -76.95455932617188
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 36000
Agent1_TimeSinceStart : 112.46917676925659
Agent1_Critic_Loss : 1.688866376876831
Agent1_Actor_Loss : -3.551520824432373
Agent1_Alpha_Loss : 4.959586143493652
Agent1_Temperature : 0.4964118289657211
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -40.53547668457031
Agent0_Eval_StdReturn : 37.31855010986328
Agent0_Eval_MaxReturn : 17.77577781677246
Agent0_Eval_MinReturn : -96.59581756591797
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -63.69207000732422
Agent0_Train_StdReturn : 34.264129638671875
Agent0_Train_MaxReturn : -19.731733322143555
Agent0_Train_MinReturn : -126.09278869628906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 37500
Agent0_TimeSinceStart : 114.99830770492554
Agent0_Critic_Loss : 1.9943084716796875
Agent0_Actor_Loss : -3.490729808807373
Agent0_Alpha_Loss : 4.961879730224609
Agent0_Temperature : 0.49626401750878246
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -44.03763961791992
Agent1_Eval_StdReturn : 25.244476318359375
Agent1_Eval_MaxReturn : 8.081317901611328
Agent1_Eval_MinReturn : -75.85384368896484
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -58.03276443481445
Agent1_Train_StdReturn : 25.487430572509766
Agent1_Train_MaxReturn : -22.951801300048828
Agent1_Train_MinReturn : -103.32923126220703
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 37500
Agent1_TimeSinceStart : 117.52849793434143
Agent1_Critic_Loss : 1.765782117843628
Agent1_Actor_Loss : -3.567430019378662
Agent1_Alpha_Loss : 4.974241256713867
Agent1_Temperature : 0.49626295341846177
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.95726776123047
Agent0_Eval_StdReturn : 35.95022964477539
Agent0_Eval_MaxReturn : 14.885432243347168
Agent0_Eval_MinReturn : -109.93602752685547
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -37.9614143371582
Agent0_Train_StdReturn : 32.27155303955078
Agent0_Train_MaxReturn : 24.43669891357422
Agent0_Train_MinReturn : -76.0283203125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 39000
Agent0_TimeSinceStart : 120.0627748966217
Agent0_Critic_Loss : 1.5446304082870483
Agent0_Actor_Loss : -3.592106819152832
Agent0_Alpha_Loss : 5.011488914489746
Agent0_Temperature : 0.4961151346212634
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -46.65683364868164
Agent1_Eval_StdReturn : 32.727752685546875
Agent1_Eval_MaxReturn : -7.15355110168457
Agent1_Eval_MinReturn : -109.01959991455078
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -59.1713981628418
Agent1_Train_StdReturn : 41.80258560180664
Agent1_Train_MaxReturn : 15.17540168762207
Agent1_Train_MinReturn : -129.44415283203125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 39000
Agent1_TimeSinceStart : 122.59055972099304
Agent1_Critic_Loss : 1.8157938718795776
Agent1_Actor_Loss : -3.5558574199676514
Agent1_Alpha_Loss : 4.964650630950928
Agent1_Temperature : 0.4961141102700448
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -63.89448928833008
Agent0_Eval_StdReturn : 39.19863510131836
Agent0_Eval_MaxReturn : -19.64415168762207
Agent0_Eval_MinReturn : -140.14515686035156
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -31.62398338317871
Agent0_Train_StdReturn : 26.720985412597656
Agent0_Train_MaxReturn : 8.335166931152344
Agent0_Train_MinReturn : -72.52067565917969
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 40500
Agent0_TimeSinceStart : 125.11737275123596
Agent0_Critic_Loss : 1.7078560590744019
Agent0_Actor_Loss : -3.5779314041137695
Agent0_Alpha_Loss : 4.938227653503418
Agent0_Temperature : 0.49596634215651536
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -45.624961853027344
Agent1_Eval_StdReturn : 39.489559173583984
Agent1_Eval_MaxReturn : 21.034717559814453
Agent1_Eval_MinReturn : -97.53782653808594
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -39.541648864746094
Agent1_Train_StdReturn : 17.816844940185547
Agent1_Train_MaxReturn : -16.936288833618164
Agent1_Train_MinReturn : -85.52580261230469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 40500
Agent1_TimeSinceStart : 127.64594292640686
Agent1_Critic_Loss : 1.8426727056503296
Agent1_Actor_Loss : -3.5460832118988037
Agent1_Alpha_Loss : 4.932355880737305
Agent1_Temperature : 0.4959653677190375
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Agent1_TimeSinceStart : 87.37843489646912
Agent1_Critic_Loss : 0.7918368577957153
Agent1_Actor_Loss : -0.6142369508743286
Agent1_Alpha_Loss : 0.9908934831619263
Agent1_Temperature : 0.09946134045455464
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -41.96603775024414
Agent0_Eval_StdReturn : 30.31974220275879
Agent0_Eval_MaxReturn : -0.578526496887207
Agent0_Eval_MinReturn : -89.70709228515625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -41.716209411621094
Agent0_Train_StdReturn : 24.91172218322754
Agent0_Train_MaxReturn : 13.704111099243164
Agent0_Train_MinReturn : -70.28943634033203
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 28500
Agent0_TimeSinceStart : 89.90887093544006
Agent0_Critic_Loss : 0.9057186245918274
Agent0_Actor_Loss : -0.47181734442710876
Agent0_Alpha_Loss : 1.001798152923584
Agent0_Temperature : 0.09943161098193352
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -47.00211715698242
Agent1_Eval_StdReturn : 35.879737854003906
Agent1_Eval_MaxReturn : 13.579434394836426
Agent1_Eval_MinReturn : -127.37272644042969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.46116638183594
Agent1_Train_StdReturn : 41.32269287109375
Agent1_Train_MaxReturn : 28.300247192382812
Agent1_Train_MinReturn : -104.24873352050781
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 28500
Agent1_TimeSinceStart : 92.46035933494568
Agent1_Critic_Loss : 0.7985086441040039
Agent1_Actor_Loss : -0.6422288417816162
Agent1_Alpha_Loss : 0.9975098371505737
Agent1_Temperature : 0.09943150003633597
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -47.14923858642578
Agent0_Eval_StdReturn : 29.793254852294922
Agent0_Eval_MaxReturn : 6.996241569519043
Agent0_Eval_MinReturn : -114.3123550415039
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -39.399986267089844
Agent0_Train_StdReturn : 44.434913635253906
Agent0_Train_MaxReturn : 14.468165397644043
Agent0_Train_MinReturn : -152.09918212890625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 30000
Agent0_TimeSinceStart : 95.002450466156
Agent0_Critic_Loss : 0.9049587249755859
Agent0_Actor_Loss : -0.4502902626991272
Agent0_Alpha_Loss : 0.9886776208877563
Agent0_Temperature : 0.09940178267659511
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -50.307655334472656
Agent1_Eval_StdReturn : 30.60295295715332
Agent1_Eval_MaxReturn : -3.8379287719726562
Agent1_Eval_MinReturn : -109.28166198730469
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -34.5245361328125
Agent1_Train_StdReturn : 30.109025955200195
Agent1_Train_MaxReturn : -2.5688419342041016
Agent1_Train_MinReturn : -91.34690856933594
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 30000
Agent1_TimeSinceStart : 97.55111622810364
Agent1_Critic_Loss : 1.0946305990219116
Agent1_Actor_Loss : -0.6108120679855347
Agent1_Alpha_Loss : 1.0005260705947876
Agent1_Temperature : 0.09940165209657005
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -40.9626350402832
Agent0_Eval_StdReturn : 30.15603256225586
Agent0_Eval_MaxReturn : -1.8011322021484375
Agent0_Eval_MinReturn : -90.40487670898438
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.28255844116211
Agent0_Train_StdReturn : 22.887502670288086
Agent0_Train_MaxReturn : 8.663355827331543
Agent0_Train_MinReturn : -68.86810302734375
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 31500
Agent0_TimeSinceStart : 100.09944200515747
Agent0_Critic_Loss : 0.7764205932617188
Agent0_Actor_Loss : -0.4837247133255005
Agent0_Alpha_Loss : 0.9885690808296204
Agent0_Temperature : 0.09937196734694355
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -52.567955017089844
Agent1_Eval_StdReturn : 27.426769256591797
Agent1_Eval_MaxReturn : -16.37167739868164
Agent1_Eval_MinReturn : -97.44410705566406
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -38.7281379699707
Agent1_Train_StdReturn : 26.70332908630371
Agent1_Train_MaxReturn : -1.2043911218643188
Agent1_Train_MinReturn : -105.21249389648438
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 31500
Agent1_TimeSinceStart : 102.6504111289978
Agent1_Critic_Loss : 0.8566279411315918
Agent1_Actor_Loss : -0.5599788427352905
Agent1_Alpha_Loss : 0.9852504730224609
Agent1_Temperature : 0.09937182858382619
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -58.35145950317383
Agent0_Eval_StdReturn : 14.147913932800293
Agent0_Eval_MaxReturn : -31.18358039855957
Agent0_Eval_MinReturn : -75.34425354003906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -37.84282684326172
Agent0_Train_StdReturn : 28.938785552978516
Agent0_Train_MaxReturn : 9.015148162841797
Agent0_Train_MinReturn : -86.74305725097656
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 33000
Agent0_TimeSinceStart : 105.20685815811157
Agent0_Critic_Loss : 0.8823339939117432
Agent0_Actor_Loss : -0.4278985857963562
Agent0_Alpha_Loss : 0.9820587635040283
Agent0_Temperature : 0.09934217718603226
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -59.14531326293945
Agent1_Eval_StdReturn : 30.169788360595703
Agent1_Eval_MaxReturn : -12.087516784667969
Agent1_Eval_MinReturn : -108.09294128417969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.33036422729492
Agent1_Train_StdReturn : 39.38785934448242
Agent1_Train_MaxReturn : -1.0789082050323486
Agent1_Train_MinReturn : -130.0101776123047
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 33000
Agent1_TimeSinceStart : 107.75787615776062
Agent1_Critic_Loss : 0.8258866667747498
Agent1_Actor_Loss : -0.569866418838501
Agent1_Alpha_Loss : 0.9881391525268555
Agent1_Temperature : 0.09934202161241136
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -50.90111541748047
Agent0_Eval_StdReturn : 55.05590057373047
Agent0_Eval_MaxReturn : 9.423296928405762
Agent0_Eval_MinReturn : -180.6466064453125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -33.33357238769531
Agent0_Train_StdReturn : 20.52121925354004
Agent0_Train_MaxReturn : 2.219759941101074
Agent0_Train_MinReturn : -66.50537872314453
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 34500
Agent0_TimeSinceStart : 110.29826831817627
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -41.02063751220703
Agent0_Eval_StdReturn : 25.86836814880371
Agent0_Eval_MaxReturn : -0.08159255981445312
Agent0_Eval_MinReturn : -83.08551788330078
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -31.899627685546875
Agent0_Train_StdReturn : 24.757017135620117
Agent0_Train_MaxReturn : 11.962577819824219
Agent0_Train_MinReturn : -72.43437957763672
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 42000
Agent0_TimeSinceStart : 130.18044781684875
Agent0_Critic_Loss : 1.5877667665481567
Agent0_Actor_Loss : -3.605268955230713
Agent0_Alpha_Loss : 4.938969135284424
Agent0_Temperature : 0.4958176324735456
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -44.361183166503906
Agent1_Eval_StdReturn : 18.20415687561035
Agent1_Eval_MaxReturn : -16.601272583007812
Agent1_Eval_MinReturn : -75.77086639404297
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -24.141521453857422
Agent1_Train_StdReturn : 29.396316528320312
Agent1_Train_MaxReturn : 26.426578521728516
Agent1_Train_MinReturn : -75.75286102294922
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 42000
Agent1_TimeSinceStart : 132.71429586410522
Agent1_Critic_Loss : 1.8130872249603271
Agent1_Actor_Loss : -3.583111524581909
Agent1_Alpha_Loss : 4.946470260620117
Agent1_Temperature : 0.4958166887941381
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -36.52915573120117
Agent0_Eval_StdReturn : 37.11091613769531
Agent0_Eval_MaxReturn : 15.500289916992188
Agent0_Eval_MinReturn : -96.7843017578125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.32754898071289
Agent0_Train_StdReturn : 42.199684143066406
Agent0_Train_MaxReturn : 21.108612060546875
Agent0_Train_MinReturn : -92.85610961914062
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 43500
Agent0_TimeSinceStart : 135.25552439689636
Agent0_Critic_Loss : 1.7577731609344482
Agent0_Actor_Loss : -3.6894140243530273
Agent0_Alpha_Loss : 4.9517903327941895
Agent0_Temperature : 0.4956689734997861
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -41.85343933105469
Agent1_Eval_StdReturn : 39.22167205810547
Agent1_Eval_MaxReturn : 10.276314735412598
Agent1_Eval_MinReturn : -121.52285766601562
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -55.40351104736328
Agent1_Train_StdReturn : 32.679039001464844
Agent1_Train_MaxReturn : -9.523871421813965
Agent1_Train_MinReturn : -127.95182800292969
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 43500
Agent1_TimeSinceStart : 137.7835714817047
Agent1_Critic_Loss : 2.1767756938934326
Agent1_Actor_Loss : -3.6545586585998535
Agent1_Alpha_Loss : 4.946593284606934
Agent1_Temperature : 0.4956680705364627
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -48.27450942993164
Agent0_Eval_StdReturn : 23.630950927734375
Agent0_Eval_MaxReturn : -10.18851089477539
Agent0_Eval_MinReturn : -85.60423278808594
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -45.957149505615234
Agent0_Train_StdReturn : 36.150081634521484
Agent0_Train_MaxReturn : 25.985132217407227
Agent0_Train_MinReturn : -93.38690185546875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 45000
Agent0_TimeSinceStart : 140.3123960494995
Agent0_Critic_Loss : 1.8786702156066895
Agent0_Actor_Loss : -3.6876707077026367
Agent0_Alpha_Loss : 4.962245464324951
Agent0_Temperature : 0.49552034214475144
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -40.11199951171875
Agent1_Eval_StdReturn : 18.35184097290039
Agent1_Eval_MaxReturn : -19.524250030517578
Agent1_Eval_MinReturn : -72.2166976928711
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -38.25145721435547
Agent1_Train_StdReturn : 22.76781463623047
Agent1_Train_MaxReturn : -4.95070743560791
Agent1_Train_MinReturn : -81.33355712890625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 45000
Agent1_TimeSinceStart : 142.84201097488403
Agent1_Critic_Loss : 1.5487732887268066
Agent1_Actor_Loss : -3.70198392868042
Agent1_Alpha_Loss : 4.940088272094727
Agent1_Temperature : 0.495519524418755
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -62.658119201660156
Agent0_Eval_StdReturn : 42.224571228027344
Agent0_Eval_MaxReturn : 7.993841171264648
Agent0_Eval_MinReturn : -140.384033203125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -39.61425018310547
Agent0_Train_StdReturn : 35.95425796508789
Agent0_Train_MaxReturn : 4.683099746704102
Agent0_Train_MinReturn : -85.62588500976562
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 46500
Agent0_TimeSinceStart : 145.38474941253662
Agent0_Critic_Loss : 2.027672290802002
Agent0_Actor_Loss : -3.711392879486084
Agent0_Alpha_Loss : 4.956854343414307
Agent0_Temperature : 0.4953717519733161
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -43.773677825927734
Agent1_Eval_StdReturn : 28.601564407348633
Agent1_Eval_MaxReturn : -0.8632755279541016
Agent1_Eval_MinReturn : -104.30401611328125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -38.598514556884766
Agent1_Train_StdReturn : 18.832670211791992
Agent1_Train_MaxReturn : -16.419071197509766
Agent1_Train_MinReturn : -81.7685546875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 46500
Agent1_TimeSinceStart : 147.9229667186737
Agent1_Critic_Loss : 1.6669756174087524
Agent1_Actor_Loss : -3.710008144378662
Agent1_Alpha_Loss : 4.938831329345703
Agent1_Temperature : 0.4953710493004955
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -43.6041374206543
Agent0_Eval_StdReturn : 25.53697395324707
Agent0_Eval_MaxReturn : -13.925565719604492
Agent0_Eval_MinReturn : -95.20899963378906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -47.98710250854492
Agent0_Train_StdReturn : 30.347665786743164
Agent0_Train_MaxReturn : -5.002738952636719
Agent0_Train_MinReturn : -91.59971618652344
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 48000
Agent0_TimeSinceStart : 150.4617965221405
Agent0_Critic_Loss : 2.0520036220550537
Agent0_Actor_Loss : -3.7660841941833496
Agent0_Alpha_Loss : 4.934596061706543
Agent0_Temperature : 0.495223251275243
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...
Agent0_Critic_Loss : 0.8448498249053955
Agent0_Actor_Loss : -0.43362003564834595
Agent0_Alpha_Loss : 0.9865957498550415
Agent0_Temperature : 0.09931240082377699
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -49.00706481933594
Agent1_Eval_StdReturn : 18.475875854492188
Agent1_Eval_MaxReturn : -14.659488677978516
Agent1_Eval_MinReturn : -75.85062408447266
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -48.71054458618164
Agent1_Train_StdReturn : 39.731388092041016
Agent1_Train_MaxReturn : 6.9532318115234375
Agent1_Train_MinReturn : -119.5660400390625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 34500
Agent1_TimeSinceStart : 112.86140847206116
Agent1_Critic_Loss : 0.8565757870674133
Agent1_Actor_Loss : -0.5681405067443848
Agent1_Alpha_Loss : 0.986566424369812
Agent1_Temperature : 0.09931223321680205
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -39.79488754272461
Agent0_Eval_StdReturn : 26.93739891052246
Agent0_Eval_MaxReturn : -1.2611160278320312
Agent0_Eval_MinReturn : -108.00636291503906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.08692169189453
Agent0_Train_StdReturn : 32.7374382019043
Agent0_Train_MaxReturn : 1.203641653060913
Agent0_Train_MinReturn : -99.79647827148438
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 36000
Agent0_TimeSinceStart : 115.41375207901001
Agent0_Critic_Loss : 0.8753998279571533
Agent0_Actor_Loss : -0.42814624309539795
Agent0_Alpha_Loss : 0.9894462823867798
Agent0_Temperature : 0.09928263180618704
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.84988021850586
Agent1_Eval_StdReturn : 45.72030258178711
Agent1_Eval_MaxReturn : 39.09492874145508
Agent1_Eval_MinReturn : -124.33256530761719
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -51.437721252441406
Agent1_Train_StdReturn : 26.324737548828125
Agent1_Train_MaxReturn : -11.97465705871582
Agent1_Train_MinReturn : -103.29388427734375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 36000
Agent1_TimeSinceStart : 117.97158718109131
Agent1_Critic_Loss : 0.7376892566680908
Agent1_Actor_Loss : -0.5654957294464111
Agent1_Alpha_Loss : 0.991793155670166
Agent1_Temperature : 0.09928245155989954
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -28.26904296875
Agent0_Eval_StdReturn : 33.93218231201172
Agent0_Eval_MaxReturn : 19.068267822265625
Agent0_Eval_MinReturn : -81.96875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -58.0765380859375
Agent0_Train_StdReturn : 44.55050277709961
Agent0_Train_MaxReturn : -9.83216667175293
Agent0_Train_MinReturn : -152.66677856445312
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 37500
Agent0_TimeSinceStart : 120.51484799385071
Agent0_Critic_Loss : 0.7485758066177368
Agent0_Actor_Loss : -0.3972724974155426
Agent0_Alpha_Loss : 0.9926971197128296
Agent0_Temperature : 0.09925286372379993
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.249717712402344
Agent1_Eval_StdReturn : 38.15171813964844
Agent1_Eval_MaxReturn : 56.68659591674805
Agent1_Eval_MinReturn : -88.11131286621094
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -45.96656036376953
Agent1_Train_StdReturn : 31.022363662719727
Agent1_Train_MaxReturn : -1.5536603927612305
Agent1_Train_MinReturn : -98.87954711914062
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 37500
Agent1_TimeSinceStart : 123.07286238670349
Agent1_Critic_Loss : 0.7663833498954773
Agent1_Actor_Loss : -0.5669386386871338
Agent1_Alpha_Loss : 0.9980077743530273
Agent1_Temperature : 0.09925266439669755
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -32.61152648925781
Agent0_Eval_StdReturn : 28.302087783813477
Agent0_Eval_MaxReturn : 9.51192855834961
Agent0_Eval_MinReturn : -97.78268432617188
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -41.6761474609375
Agent0_Train_StdReturn : 37.476402282714844
Agent0_Train_MaxReturn : 25.196866989135742
Agent0_Train_MinReturn : -102.59992218017578
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 39000
Agent0_TimeSinceStart : 125.62654113769531
Agent0_Critic_Loss : 0.8015234470367432
Agent0_Actor_Loss : -0.44107478857040405
Agent0_Alpha_Loss : 1.0000181198120117
Agent0_Temperature : 0.09922308272605697
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.622093200683594
Agent1_Eval_StdReturn : 30.99830436706543
Agent1_Eval_MaxReturn : -3.984701156616211
Agent1_Eval_MinReturn : -103.27308654785156
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -50.1801872253418
Agent1_Train_StdReturn : 31.15873146057129
Agent1_Train_MaxReturn : 9.895744323730469
Agent1_Train_MinReturn : -87.53252410888672
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 39000
Agent1_TimeSinceStart : 128.18866562843323
Agent1_Critic_Loss : 0.8519771099090576
Agent1_Actor_Loss : -0.6047926545143127
Agent1_Alpha_Loss : 0.9917216300964355
Agent1_Temperature : 0.09922288650312594
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -50.88775634765625
Agent0_Eval_StdReturn : 33.29325485229492
Agent0_Eval_MaxReturn : 6.320981979370117
Agent0_Eval_MinReturn : -86.04035949707031
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -30.342437744140625
Agent0_Train_StdReturn : 44.536529541015625
Agent0_Train_MaxReturn : 36.712982177734375
Agent0_Train_MinReturn : -94.08296203613281
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 40500
Agent0_TimeSinceStart : 130.73844695091248
Agent0_Critic_Loss : 0.8084367513656616
Agent0_Actor_Loss : -0.41941899061203003
Agent0_Alpha_Loss : 0.9880611896514893
Agent0_Temperature : 0.0991933163768057
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.26639938354492
Agent1_Eval_StdReturn : 38.109169006347656
Agent1_Eval_MaxReturn : 28.55547332763672
Agent1_Eval_MinReturn : -81.9331283569336
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -38.84937286376953
Agent1_Train_StdReturn : 33.022552490234375
Agent1_Train_MaxReturn : -7.676582336425781
Agent1_Train_MinReturn : -109.8722915649414
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 40500
Agent1_TimeSinceStart : 133.30394983291626
Agent1_Critic_Loss : 1.0040781497955322
Agent1_Actor_Loss : -0.5737190246582031

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -33.92432403564453
Agent1_Eval_StdReturn : 29.28649139404297
Agent1_Eval_MaxReturn : 20.156362533569336
Agent1_Eval_MinReturn : -78.1865005493164
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.836212158203125
Agent1_Train_StdReturn : 27.20333480834961
Agent1_Train_MaxReturn : 7.378322601318359
Agent1_Train_MinReturn : -84.83805847167969
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 48000
Agent1_TimeSinceStart : 153.00162887573242
Agent1_Critic_Loss : 1.6640357971191406
Agent1_Actor_Loss : -3.6806163787841797
Agent1_Alpha_Loss : 4.937534332275391
Agent1_Temperature : 0.49522264427202634
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -48.03557586669922
Agent0_Eval_StdReturn : 31.829381942749023
Agent0_Eval_MaxReturn : 14.145650863647461
Agent0_Eval_MinReturn : -93.38575744628906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -48.742881774902344
Agent0_Train_StdReturn : 41.05773162841797
Agent0_Train_MaxReturn : 14.33819580078125
Agent0_Train_MinReturn : -130.11065673828125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 49500
Agent0_TimeSinceStart : 155.5435347557068
Agent0_Critic_Loss : 1.7425429821014404
Agent0_Actor_Loss : -3.8764052391052246
Agent0_Alpha_Loss : 4.956688404083252
Agent0_Temperature : 0.4950747862237107
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.0926513671875
Agent1_Eval_StdReturn : 31.324655532836914
Agent1_Eval_MaxReturn : 1.9261980056762695
Agent1_Eval_MinReturn : -112.68663024902344
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -32.51620864868164
Agent1_Train_StdReturn : 29.158769607543945
Agent1_Train_MaxReturn : 24.964004516601562
Agent1_Train_MinReturn : -80.35346984863281
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 49500
Agent1_TimeSinceStart : 158.08896493911743
Agent1_Critic_Loss : 1.7832343578338623
Agent1_Actor_Loss : -3.751837730407715
Agent1_Alpha_Loss : 4.949411392211914
Agent1_Temperature : 0.49507427990605457
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -54.837493896484375
Agent0_Eval_StdReturn : 36.6554069519043
Agent0_Eval_MaxReturn : -2.088275909423828
Agent0_Eval_MinReturn : -118.91795349121094
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.0096549987793
Agent0_Train_StdReturn : 32.894893646240234
Agent0_Train_MaxReturn : -0.7078094482421875
Agent0_Train_MinReturn : -114.74441528320312
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 51000
Agent0_TimeSinceStart : 160.63273739814758
Agent0_Critic_Loss : 1.8336354494094849
Agent0_Actor_Loss : -3.8591973781585693
Agent0_Alpha_Loss : 4.914107799530029
Agent0_Temperature : 0.4949264510082303
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -38.54420852661133
Agent1_Eval_StdReturn : 43.052608489990234
Agent1_Eval_MaxReturn : 38.29109573364258
Agent1_Eval_MinReturn : -87.20782470703125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -59.989593505859375
Agent1_Train_StdReturn : 32.030609130859375
Agent1_Train_MaxReturn : 3.5966739654541016
Agent1_Train_MinReturn : -94.86580657958984
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 51000
Agent1_TimeSinceStart : 163.17510962486267
Agent1_Critic_Loss : 1.8510282039642334
Agent1_Actor_Loss : -3.7151949405670166
Agent1_Alpha_Loss : 4.904638290405273
Agent1_Temperature : 0.4949260544604604
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.282203674316406
Agent0_Eval_StdReturn : 22.53019142150879
Agent0_Eval_MaxReturn : 0.2521381378173828
Agent0_Eval_MinReturn : -77.50274658203125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.245155334472656
Agent0_Train_StdReturn : 31.46364974975586
Agent0_Train_MaxReturn : 24.726903915405273
Agent0_Train_MinReturn : -83.14138793945312
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 52500
Agent0_TimeSinceStart : 165.72784805297852
Agent0_Critic_Loss : 1.807163119316101
Agent0_Actor_Loss : -3.8288989067077637
Agent0_Alpha_Loss : 4.933527946472168
Agent0_Temperature : 0.49477819184336985
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -38.697052001953125
Agent1_Eval_StdReturn : 30.54305076599121
Agent1_Eval_MaxReturn : 26.86504554748535
Agent1_Eval_MinReturn : -98.99554443359375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -49.473026275634766
Agent1_Train_StdReturn : 31.43758773803711
Agent1_Train_MaxReturn : 8.30721664428711
Agent1_Train_MinReturn : -102.11219787597656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 52500
Agent1_TimeSinceStart : 168.28490257263184
Agent1_Critic_Loss : 1.6860638856887817
Agent1_Actor_Loss : -3.845151901245117
Agent1_Alpha_Loss : 4.972072124481201
Agent1_Temperature : 0.49477780689113754
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -49.241455078125
Agent0_Eval_StdReturn : 37.4254035949707
Agent0_Eval_MaxReturn : 22.923078536987305
Agent0_Eval_MinReturn : -115.88160705566406
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -56.29448318481445
Agent0_Train_StdReturn : 39.52996063232422
Agent0_Train_MaxReturn : -7.496082305908203
Agent0_Train_MinReturn : -134.86407470703125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 54000
Agent0_TimeSinceStart : 170.8369812965393
Agent0_Critic_Loss : 1.8060518503189087
Agent0_Actor_Loss : -3.8083651065826416
Agent0_Alpha_Loss : 4.95180606842041
Agent0_Temperature : 0.4946299638484361
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -20.69058609008789
Agent1_Eval_StdReturn : 31.600984573364258
Agent1_Eval_MaxReturn : 32.870391845703125
Agent1_Eval_MinReturn : -71.8866195678711
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.973487854003906
Agent1_Train_StdReturn : 23.155366897583008
Agent1_Train_MaxReturn : -11.861381530761719
Agent1_Train_MinReturn : -89.36972045898438
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 54000
Agent1_TimeSinceStart : 173.39183020591736
Agent1_Critic_Loss : 1.6600738763809204
Agent1_Actor_Loss : -3.878749370574951
Agent1_Alpha_Loss : 4.968748092651367
Agent1_Temperature : 0.4946295523376071
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...
Agent1_Alpha_Loss : 0.9830466508865356
Agent1_Temperature : 0.09919313576483997
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.25227737426758
Agent0_Eval_StdReturn : 32.12028503417969
Agent0_Eval_MaxReturn : -5.728395462036133
Agent0_Eval_MinReturn : -87.54178619384766
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -33.07017517089844
Agent0_Train_StdReturn : 24.965072631835938
Agent0_Train_MaxReturn : 13.350378036499023
Agent0_Train_MinReturn : -73.97539520263672
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 42000
Agent0_TimeSinceStart : 135.87602472305298
Agent0_Critic_Loss : 0.768791675567627
Agent0_Actor_Loss : -0.39676445722579956
Agent0_Alpha_Loss : 0.985124945640564
Agent0_Temperature : 0.09916357005595099
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -53.3942756652832
Agent1_Eval_StdReturn : 32.249515533447266
Agent1_Eval_MaxReturn : -9.427106857299805
Agent1_Eval_MinReturn : -126.55973815917969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -35.78147506713867
Agent1_Train_StdReturn : 20.052528381347656
Agent1_Train_MaxReturn : 4.546283721923828
Agent1_Train_MinReturn : -69.99932861328125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 42000
Agent1_TimeSinceStart : 138.4355571269989
Agent1_Critic_Loss : 0.8170763254165649
Agent1_Actor_Loss : -0.6001193523406982
Agent1_Alpha_Loss : 0.9887992143630981
Agent1_Temperature : 0.0991633977457559
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -51.35826873779297
Agent0_Eval_StdReturn : 34.40176773071289
Agent0_Eval_MaxReturn : -1.5894088745117188
Agent0_Eval_MinReturn : -111.24187469482422
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -40.834293365478516
Agent0_Train_StdReturn : 40.53874969482422
Agent0_Train_MaxReturn : 46.94917297363281
Agent0_Train_MinReturn : -83.31674194335938
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 43500
Agent0_TimeSinceStart : 141.0092911720276
Agent0_Critic_Loss : 1.0166336297988892
Agent0_Actor_Loss : -0.4351668059825897
Agent0_Alpha_Loss : 0.9880465269088745
Agent0_Temperature : 0.09913383614411772
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -40.01546096801758
Agent1_Eval_StdReturn : 30.61081886291504
Agent1_Eval_MaxReturn : 5.098873138427734
Agent1_Eval_MinReturn : -93.36260986328125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -50.38044357299805
Agent1_Train_StdReturn : 29.34974479675293
Agent1_Train_MaxReturn : -7.699463367462158
Agent1_Train_MinReturn : -96.51921081542969
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 43500
Agent1_TimeSinceStart : 143.56005692481995
Agent1_Critic_Loss : 0.753375232219696
Agent1_Actor_Loss : -0.5791974067687988
Agent1_Alpha_Loss : 0.9877139329910278
Agent1_Temperature : 0.09913367419521385
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -27.884130477905273
Agent0_Eval_StdReturn : 30.554161071777344
Agent0_Eval_MaxReturn : 23.10334014892578
Agent0_Eval_MinReturn : -80.12039184570312
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -39.28071212768555
Agent0_Train_StdReturn : 30.776363372802734
Agent0_Train_MaxReturn : 4.9557414054870605
Agent0_Train_MinReturn : -82.59444427490234
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 45000
Agent0_TimeSinceStart : 146.1395251750946
Agent0_Critic_Loss : 0.8818604946136475
Agent0_Actor_Loss : -0.43326669931411743
Agent0_Alpha_Loss : 0.9917951822280884
Agent0_Temperature : 0.09910410621358526
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -29.072803497314453
Agent1_Eval_StdReturn : 27.72119140625
Agent1_Eval_MaxReturn : 21.36469841003418
Agent1_Eval_MinReturn : -95.36259460449219
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.47939682006836
Agent1_Train_StdReturn : 34.31270217895508
Agent1_Train_MaxReturn : 27.23346519470215
Agent1_Train_MinReturn : -93.98992156982422
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 45000
Agent1_TimeSinceStart : 148.7082200050354
Agent1_Critic_Loss : 0.8228013515472412
Agent1_Actor_Loss : -0.5653153657913208
Agent1_Alpha_Loss : 0.9853910207748413
Agent1_Temperature : 0.0991039692805048
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -60.495628356933594
Agent0_Eval_StdReturn : 32.92407989501953
Agent0_Eval_MaxReturn : -6.622692108154297
Agent0_Eval_MinReturn : -108.82998657226562
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.019371032714844
Agent0_Train_StdReturn : 29.829227447509766
Agent0_Train_MaxReturn : 7.3693084716796875
Agent0_Train_MinReturn : -98.67569732666016
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 46500
Agent0_TimeSinceStart : 151.28179359436035
Agent0_Critic_Loss : 0.862257719039917
Agent0_Actor_Loss : -0.48679935932159424
Agent0_Alpha_Loss : 0.9870626330375671
Agent0_Temperature : 0.09907439100433786
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -42.596534729003906
Agent1_Eval_StdReturn : 27.827476501464844
Agent1_Eval_MaxReturn : 7.9470062255859375
Agent1_Eval_MinReturn : -84.55802154541016
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -45.00421142578125
Agent1_Train_StdReturn : 27.587345123291016
Agent1_Train_MaxReturn : -3.7378578186035156
Agent1_Train_MinReturn : -82.19117736816406
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 46500
Agent1_TimeSinceStart : 153.85096549987793
Agent1_Critic_Loss : 0.8155502080917358
Agent1_Actor_Loss : -0.5911684632301331
Agent1_Alpha_Loss : 0.9846792221069336
Agent1_Temperature : 0.09907428321578936
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -36.943702697753906
Agent0_Eval_StdReturn : 23.041719436645508
Agent0_Eval_MaxReturn : -1.8531246185302734
Agent0_Eval_MinReturn : -78.729248046875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.198944091796875
Agent0_Train_StdReturn : 29.461870193481445
Agent0_Train_MaxReturn : -1.3896656036376953
Agent0_Train_MinReturn : -103.57658386230469
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 48000
Agent0_TimeSinceStart : 156.43728923797607
Agent0_Critic_Loss : 0.9056991934776306
Agent0_Actor_Loss : -0.5163251161575317
Agent0_Alpha_Loss : 0.9832513928413391

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.071044921875
Agent0_Eval_StdReturn : 20.311540603637695
Agent0_Eval_MaxReturn : -4.248510360717773
Agent0_Eval_MinReturn : -76.11735534667969
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -45.647621154785156
Agent0_Train_StdReturn : 28.65673828125
Agent0_Train_MaxReturn : -8.050041198730469
Agent0_Train_MinReturn : -93.59137725830078
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 55500
Agent0_TimeSinceStart : 175.95765161514282
Agent0_Critic_Loss : 1.9165866374969482
Agent0_Actor_Loss : -3.8402960300445557
Agent0_Alpha_Loss : 4.940425872802734
Agent0_Temperature : 0.49448179370301315
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -45.792633056640625
Agent1_Eval_StdReturn : 30.212984085083008
Agent1_Eval_MaxReturn : -6.273834228515625
Agent1_Eval_MinReturn : -95.20599365234375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -62.20540237426758
Agent1_Train_StdReturn : 29.23523712158203
Agent1_Train_MaxReturn : -9.35457992553711
Agent1_Train_MinReturn : -100.54710388183594
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 55500
Agent1_TimeSinceStart : 178.5136067867279
Agent1_Critic_Loss : 1.8993158340454102
Agent1_Actor_Loss : -3.901531457901001
Agent1_Alpha_Loss : 4.944033622741699
Agent1_Temperature : 0.4944813519524985
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -35.131248474121094
Agent0_Eval_StdReturn : 25.728803634643555
Agent0_Eval_MaxReturn : 14.794334411621094
Agent0_Eval_MinReturn : -77.3138198852539
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.141204833984375
Agent0_Train_StdReturn : 40.68620681762695
Agent0_Train_MaxReturn : 10.843917846679688
Agent0_Train_MinReturn : -140.00076293945312
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 57000
Agent0_TimeSinceStart : 181.07041144371033
Agent0_Critic_Loss : 1.987477421760559
Agent0_Actor_Loss : -3.8442745208740234
Agent0_Alpha_Loss : 4.954906463623047
Agent0_Temperature : 0.494333646817366
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -31.837505340576172
Agent1_Eval_StdReturn : 28.436418533325195
Agent1_Eval_MaxReturn : 10.59328556060791
Agent1_Eval_MinReturn : -81.09159851074219
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -52.34785079956055
Agent1_Train_StdReturn : 25.275291442871094
Agent1_Train_MaxReturn : -12.753273963928223
Agent1_Train_MinReturn : -87.2076644897461
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 57000
Agent1_TimeSinceStart : 183.6244068145752
Agent1_Critic_Loss : 1.5505645275115967
Agent1_Actor_Loss : -3.8883275985717773
Agent1_Alpha_Loss : 4.933682441711426
Agent1_Temperature : 0.49433322750522196
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -51.89800262451172
Agent0_Eval_StdReturn : 30.825870513916016
Agent0_Eval_MaxReturn : -4.656069755554199
Agent0_Eval_MinReturn : -112.93138885498047
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.4887809753418
Agent0_Train_StdReturn : 29.30999755859375
Agent0_Train_MaxReturn : 15.650311470031738
Agent0_Train_MinReturn : -88.85353088378906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 58500
Agent0_TimeSinceStart : 186.18085074424744
Agent0_Critic_Loss : 1.7303582429885864
Agent0_Actor_Loss : -3.813486099243164
Agent0_Alpha_Loss : 4.956270694732666
Agent0_Temperature : 0.4941855223204647
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -58.2476692199707
Agent1_Eval_StdReturn : 22.751611709594727
Agent1_Eval_MaxReturn : -18.498096466064453
Agent1_Eval_MinReturn : -90.0875015258789
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -39.837074279785156
Agent1_Train_StdReturn : 34.201316833496094
Agent1_Train_MaxReturn : 12.988668441772461
Agent1_Train_MinReturn : -89.42166137695312
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 58500
Agent1_TimeSinceStart : 188.74625778198242
Agent1_Critic_Loss : 1.574430227279663
Agent1_Actor_Loss : -3.8615846633911133
Agent1_Alpha_Loss : 4.918296813964844
Agent1_Temperature : 0.4941852096639579
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.6826057434082
Agent0_Eval_StdReturn : 35.96561050415039
Agent0_Eval_MaxReturn : 5.789908409118652
Agent0_Eval_MinReturn : -95.10879516601562
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -39.44413375854492
Agent0_Train_StdReturn : 36.11466979980469
Agent0_Train_MaxReturn : 19.146175384521484
Agent0_Train_MinReturn : -99.31334686279297
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 60000
Agent0_TimeSinceStart : 191.31852316856384
Agent0_Critic_Loss : 1.5261964797973633
Agent0_Actor_Loss : -3.7866339683532715
Agent0_Alpha_Loss : 4.9382781982421875
Agent0_Temperature : 0.4940374635098521
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -34.76164627075195
Agent1_Eval_StdReturn : 29.818700790405273
Agent1_Eval_MaxReturn : 9.31450080871582
Agent1_Eval_MinReturn : -70.38677215576172
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -48.247886657714844
Agent1_Train_StdReturn : 28.19908332824707
Agent1_Train_MaxReturn : 4.366792678833008
Agent1_Train_MinReturn : -94.60246276855469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 60000
Agent1_TimeSinceStart : 193.89208459854126
Agent1_Critic_Loss : 1.4880083799362183
Agent1_Actor_Loss : -3.8410043716430664
Agent1_Alpha_Loss : 4.924526214599609
Agent1_Temperature : 0.49403727624387284
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -49.706703186035156
Agent0_Eval_StdReturn : 24.204530715942383
Agent0_Eval_MaxReturn : 4.115816116333008
Agent0_Eval_MinReturn : -89.56936645507812
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.77156448364258
Agent0_Train_StdReturn : 18.84708023071289
Agent0_Train_MaxReturn : -0.13339900970458984
Agent0_Train_MinReturn : -69.02837371826172
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 61500
Agent0_TimeSinceStart : 196.4617121219635
Agent0_Critic_Loss : 1.6969066858291626
Agent0_Actor_Loss : -3.795665740966797
Agent0_Alpha_Loss : 4.938727378845215
Agent0_Temperature : 0.4938894664690381
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Temperature : 0.09904469798965253
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -38.39387512207031
Agent1_Eval_StdReturn : 29.409666061401367
Agent1_Eval_MaxReturn : 10.291375160217285
Agent1_Eval_MinReturn : -100.35120391845703
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -49.850311279296875
Agent1_Train_StdReturn : 37.335426330566406
Agent1_Train_MaxReturn : 1.324252724647522
Agent1_Train_MinReturn : -109.65785217285156
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 48000
Agent1_TimeSinceStart : 159.0109076499939
Agent1_Critic_Loss : 0.5747864246368408
Agent1_Actor_Loss : -0.5714495182037354
Agent1_Alpha_Loss : 0.9859848618507385
Agent1_Temperature : 0.09904461183092304
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -61.21995162963867
Agent0_Eval_StdReturn : 28.53441619873047
Agent0_Eval_MaxReturn : -17.425609588623047
Agent0_Eval_MinReturn : -105.56881713867188
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.32866287231445
Agent0_Train_StdReturn : 12.933239936828613
Agent0_Train_MaxReturn : -21.4295654296875
Agent0_Train_MinReturn : -71.98097229003906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 49500
Agent0_TimeSinceStart : 161.59722137451172
Agent0_Critic_Loss : 0.6959612369537354
Agent0_Actor_Loss : -0.5846469402313232
Agent0_Alpha_Loss : 0.9910273551940918
Agent0_Temperature : 0.09901500852514933
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -41.74701690673828
Agent1_Eval_StdReturn : 32.90968704223633
Agent1_Eval_MaxReturn : 10.13399887084961
Agent1_Eval_MinReturn : -88.80872344970703
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.287639617919922
Agent1_Train_StdReturn : 34.70893859863281
Agent1_Train_MaxReturn : 11.41775894165039
Agent1_Train_MinReturn : -98.29093170166016
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 49500
Agent1_TimeSinceStart : 164.16557502746582
Agent1_Critic_Loss : 0.7646611928939819
Agent1_Actor_Loss : -0.5970878601074219
Agent1_Alpha_Loss : 0.9900572896003723
Agent1_Temperature : 0.09901494544063312
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -50.30745315551758
Agent0_Eval_StdReturn : 28.25421905517578
Agent0_Eval_MaxReturn : 17.800573348999023
Agent0_Eval_MinReturn : -76.84844207763672
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -46.90703201293945
Agent0_Train_StdReturn : 25.767892837524414
Agent0_Train_MaxReturn : 4.3412346839904785
Agent0_Train_MinReturn : -80.19935607910156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 51000
Agent0_TimeSinceStart : 166.75104117393494
Agent0_Critic_Loss : 0.710469126701355
Agent0_Actor_Loss : -0.5585867166519165
Agent0_Alpha_Loss : 0.9832022190093994
Agent0_Temperature : 0.09898534039061417
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -34.753780364990234
Agent1_Eval_StdReturn : 23.88458824157715
Agent1_Eval_MaxReturn : -4.137777328491211
Agent1_Eval_MinReturn : -77.24288940429688
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -41.13232421875
Agent1_Train_StdReturn : 27.11815643310547
Agent1_Train_MaxReturn : 14.562864303588867
Agent1_Train_MinReturn : -81.11965942382812
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 51000
Agent1_TimeSinceStart : 169.3282220363617
Agent1_Critic_Loss : 0.7120561003684998
Agent1_Actor_Loss : -0.5781822204589844
Agent1_Alpha_Loss : 0.978557288646698
Agent1_Temperature : 0.09898530968106926
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -48.29916763305664
Agent0_Eval_StdReturn : 34.12910842895508
Agent0_Eval_MaxReturn : 11.524356842041016
Agent0_Eval_MinReturn : -114.11810302734375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.32329177856445
Agent0_Train_StdReturn : 20.361812591552734
Agent0_Train_MaxReturn : -10.520708084106445
Agent0_Train_MinReturn : -66.66043090820312
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 52500
Agent0_TimeSinceStart : 171.9294147491455
Agent0_Critic_Loss : 0.8581188917160034
Agent0_Actor_Loss : -0.540892481803894
Agent0_Alpha_Loss : 0.9881113171577454
Agent0_Temperature : 0.09895568113316093
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -33.546688079833984
Agent1_Eval_StdReturn : 29.234249114990234
Agent1_Eval_MaxReturn : 17.718795776367188
Agent1_Eval_MinReturn : -74.06364440917969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.39534378051758
Agent1_Train_StdReturn : 26.517908096313477
Agent1_Train_MaxReturn : 1.8507049083709717
Agent1_Train_MinReturn : -72.66191864013672
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 52500
Agent1_TimeSinceStart : 174.52412939071655
Agent1_Critic_Loss : 0.7387776374816895
Agent1_Actor_Loss : -0.5764716863632202
Agent1_Alpha_Loss : 0.9943042993545532
Agent1_Temperature : 0.09895566694756901
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -55.78068923950195
Agent0_Eval_StdReturn : 23.3514461517334
Agent0_Eval_MaxReturn : -6.248990058898926
Agent0_Eval_MinReturn : -91.54911804199219
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -51.628089904785156
Agent0_Train_StdReturn : 27.29883575439453
Agent0_Train_MaxReturn : -17.060712814331055
Agent0_Train_MinReturn : -91.98275756835938
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 54000
Agent0_TimeSinceStart : 177.12793707847595
Agent0_Critic_Loss : 0.7652698755264282
Agent0_Actor_Loss : -0.5179668664932251
Agent0_Alpha_Loss : 0.9890227913856506
Agent0_Temperature : 0.09892602866242856
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -44.48286056518555
Agent1_Eval_StdReturn : 30.364294052124023
Agent1_Eval_MaxReturn : 19.7025089263916
Agent1_Eval_MinReturn : -84.53271484375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -24.043479919433594
Agent1_Train_StdReturn : 38.20001983642578
Agent1_Train_MaxReturn : 33.58323669433594
Agent1_Train_MinReturn : -77.76271057128906
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 54000
Agent1_TimeSinceStart : 179.7374291419983
Agent1_Critic_Loss : 0.6687330603599548
Agent1_Actor_Loss : -0.5707255005836487
Agent1_Alpha_Loss : 0.9909029006958008
Agent1_Temperature : 0.09892602663986476
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -52.43157196044922
Agent0_Eval_StdReturn : 38.21602249145508
Agent0_Eval_MaxReturn : -6.231910228729248
Agent0_Eval_MinReturn : -118.06686401367188
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -51.12773895263672
Agent0_Train_StdReturn : 33.869537353515625
Agent0_Train_MaxReturn : -5.746119976043701
Agent0_Train_MinReturn : -107.89594268798828
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 55500
Agent0_TimeSinceStart : 182.36914706230164
Agent0_Critic_Loss : 0.6431047320365906
Agent0_Actor_Loss : -0.5150781869888306
Agent0_Alpha_Loss : 0.9839533567428589
Agent0_Temperature : 0.09889639452465702
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -45.574581146240234
Agent1_Eval_StdReturn : 34.163509368896484
Agent1_Eval_MaxReturn : 30.004281997680664
Agent1_Eval_MinReturn : -78.90914154052734
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -32.471858978271484
Agent1_Train_StdReturn : 31.15865135192871
Agent1_Train_MaxReturn : 28.819488525390625
Agent1_Train_MinReturn : -70.1280517578125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 55500
Agent1_TimeSinceStart : 185.01014161109924
Agent1_Critic_Loss : 0.6688973903656006
Agent1_Actor_Loss : -0.5941727161407471
Agent1_Alpha_Loss : 0.9901119470596313
Agent1_Temperature : 0.09889639123060592
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -35.60116195678711
Agent0_Eval_StdReturn : 20.064401626586914
Agent0_Eval_MaxReturn : -8.53857421875
Agent0_Eval_MinReturn : -75.87258911132812
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.287330627441406
Agent0_Train_StdReturn : 32.421993255615234
Agent0_Train_MaxReturn : 7.321257591247559
Agent0_Train_MinReturn : -90.27223205566406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 57000
Agent0_TimeSinceStart : 187.63554859161377
Agent0_Critic_Loss : 0.8040398359298706
Agent0_Actor_Loss : -0.46906107664108276
Agent0_Alpha_Loss : 0.9892407655715942
Agent0_Temperature : 0.0988667655787105
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -40.44770050048828
Agent1_Eval_StdReturn : 29.027013778686523
Agent1_Eval_MaxReturn : -9.228703498840332
Agent1_Eval_MinReturn : -91.73954010009766
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -21.521106719970703
Agent1_Train_StdReturn : 15.209959030151367
Agent1_Train_MaxReturn : 0.4298972487449646
Agent1_Train_MinReturn : -53.90322494506836
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 57000
Agent1_TimeSinceStart : 190.25104904174805
Agent1_Critic_Loss : 0.7038291692733765
Agent1_Actor_Loss : -0.5807892084121704
Agent1_Alpha_Loss : 0.9858642816543579
Agent1_Temperature : 0.09886677068986889
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -48.10352325439453
Agent0_Eval_StdReturn : 29.973947525024414
Agent0_Eval_MaxReturn : -0.9067797660827637
Agent0_Eval_MinReturn : -116.68663787841797
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -48.232540130615234
Agent0_Train_StdReturn : 22.227169036865234
Agent0_Train_MaxReturn : -10.684713363647461
Agent0_Train_MinReturn : -91.12077331542969
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 58500
Agent0_TimeSinceStart : 192.90538048744202
Agent0_Critic_Loss : 0.7426462173461914
Agent0_Actor_Loss : -0.4451863765716553
Agent0_Alpha_Loss : 0.9912346005439758
Agent0_Temperature : 0.09883713768738209
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -49.42542266845703
Agent1_Eval_StdReturn : 29.451990127563477
Agent1_Eval_MaxReturn : -1.8009510040283203
Agent1_Eval_MinReturn : -89.44203186035156
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -39.22800064086914
Agent1_Train_StdReturn : 20.325088500976562
Agent1_Train_MaxReturn : -10.931228637695312
Agent1_Train_MinReturn : -78.74545288085938
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 58500
Agent1_TimeSinceStart : 195.5153992176056
Agent1_Critic_Loss : 0.6913563013076782
Agent1_Actor_Loss : -0.60968416929245
Agent1_Alpha_Loss : 0.9819724559783936
Agent1_Temperature : 0.09883717303669728
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -41.76116180419922
Agent0_Eval_StdReturn : 38.989559173583984
Agent0_Eval_MaxReturn : 11.110711097717285
Agent0_Eval_MinReturn : -132.9359130859375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.80467987060547
Agent0_Train_StdReturn : 41.306941986083984
Agent0_Train_MaxReturn : 21.756818771362305
Agent0_Train_MinReturn : -128.41152954101562
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 60000
Agent0_TimeSinceStart : 198.12805032730103
Agent0_Critic_Loss : 0.6859951615333557
Agent0_Actor_Loss : -0.4423810839653015
Agent0_Alpha_Loss : 0.986197292804718
Agent0_Temperature : 0.09880752321675536
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -30.277984619140625
Agent1_Eval_StdReturn : 31.33710289001465
Agent1_Eval_MaxReturn : 40.66303634643555
Agent1_Eval_MinReturn : -70.27769470214844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.258275985717773
Agent1_Train_StdReturn : 22.60586929321289
Agent1_Train_MaxReturn : 16.451263427734375
Agent1_Train_MinReturn : -74.19381713867188
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 60000
Agent1_TimeSinceStart : 200.737446308136
Agent1_Critic_Loss : 0.6994388699531555
Agent1_Actor_Loss : -0.6634247899055481
Agent1_Alpha_Loss : 0.9801174402236938
Agent1_Temperature : 0.09880760071633395
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -53.08845138549805
Agent0_Eval_StdReturn : 22.61684226989746
Agent0_Eval_MaxReturn : -12.472286224365234
Agent0_Eval_MinReturn : -84.20429992675781
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -48.86018753051758
Agent0_Train_StdReturn : 30.135223388671875
Agent0_Train_MaxReturn : 11.668729782104492
Agent0_Train_MinReturn : -84.85906982421875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 61500
Agent0_TimeSinceStart : 203.3561990261078
Agent0_Critic_Loss : 0.8388164043426514
Agent0_Actor_Loss : -0.4349696636199951
Agent0_Alpha_Loss : 0.9896912574768066
Agent0_Temperature : 0.09877791355156479
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Agent1_Eval_AverageReturn : -38.12643814086914
Agent1_Eval_StdReturn : 29.53792953491211
Agent1_Eval_MaxReturn : 7.590252876281738
Agent1_Eval_MinReturn : -85.21327209472656
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -54.7995491027832
Agent1_Train_StdReturn : 33.79011154174805
Agent1_Train_MaxReturn : 19.60177230834961
Agent1_Train_MinReturn : -100.99966430664062
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 61500
Agent1_TimeSinceStart : 199.04899263381958
Agent1_Critic_Loss : 1.9495302438735962
Agent1_Actor_Loss : -3.8898425102233887
Agent1_Alpha_Loss : 4.958615303039551
Agent1_Temperature : 0.4938893437935201
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.58343505859375
Agent0_Eval_StdReturn : 20.443866729736328
Agent0_Eval_MaxReturn : 8.104902267456055
Agent0_Eval_MinReturn : -57.693687438964844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -53.802978515625
Agent0_Train_StdReturn : 31.385831832885742
Agent0_Train_MaxReturn : -5.219663143157959
Agent0_Train_MinReturn : -92.27386474609375
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 63000
Agent0_TimeSinceStart : 201.6605179309845
Agent0_Critic_Loss : 1.6204581260681152
Agent0_Actor_Loss : -3.7203352451324463
Agent0_Alpha_Loss : 4.948305130004883
Agent0_Temperature : 0.49374150671162464
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -51.15243911743164
Agent1_Eval_StdReturn : 28.14630889892578
Agent1_Eval_MaxReturn : -16.985088348388672
Agent1_Eval_MinReturn : -99.55315399169922
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -67.000244140625
Agent1_Train_StdReturn : 20.33897590637207
Agent1_Train_MaxReturn : -36.75010299682617
Agent1_Train_MinReturn : -92.03346252441406
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 63000
Agent1_TimeSinceStart : 204.26069021224976
Agent1_Critic_Loss : 1.9223604202270508
Agent1_Actor_Loss : -3.9148383140563965
Agent1_Alpha_Loss : 4.945062637329102
Agent1_Temperature : 0.4937414482011917
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -52.40458297729492
Agent0_Eval_StdReturn : 37.13365173339844
Agent0_Eval_MaxReturn : -2.3895864486694336
Agent0_Eval_MinReturn : -121.74528503417969
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -46.66603469848633
Agent0_Train_StdReturn : 26.473697662353516
Agent0_Train_MaxReturn : 9.178787231445312
Agent0_Train_MinReturn : -92.05039978027344
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 64500
Agent0_TimeSinceStart : 206.85532522201538
Agent0_Critic_Loss : 1.6365320682525635
Agent0_Actor_Loss : -3.759519338607788
Agent0_Alpha_Loss : 4.9887003898620605
Agent0_Temperature : 0.49359349140605113
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.843055725097656
Agent1_Eval_StdReturn : 32.62118148803711
Agent1_Eval_MaxReturn : 0.9483890533447266
Agent1_Eval_MinReturn : -107.69793701171875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -43.67143249511719
Agent1_Train_StdReturn : 34.763916015625
Agent1_Train_MaxReturn : 13.096034049987793
Agent1_Train_MinReturn : -93.55020141601562
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 64500
Agent1_TimeSinceStart : 209.4467921257019
Agent1_Critic_Loss : 1.7670109272003174
Agent1_Actor_Loss : -3.878037214279175
Agent1_Alpha_Loss : 4.937594413757324
Agent1_Temperature : 0.49359360718506295
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -41.024742126464844
Agent0_Eval_StdReturn : 30.753782272338867
Agent0_Eval_MaxReturn : 31.11935043334961
Agent0_Eval_MinReturn : -82.42699432373047
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.237895965576172
Agent0_Train_StdReturn : 34.446929931640625
Agent0_Train_MaxReturn : 17.43093490600586
Agent0_Train_MinReturn : -95.39517211914062
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 66000
Agent0_TimeSinceStart : 212.0815863609314
Agent0_Critic_Loss : 1.5660908222198486
Agent0_Actor_Loss : -3.7514705657958984
Agent0_Alpha_Loss : 4.942337989807129
Agent0_Temperature : 0.49344553991449785
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.84611511230469
Agent1_Eval_StdReturn : 28.629432678222656
Agent1_Eval_MaxReturn : 1.3306794166564941
Agent1_Eval_MinReturn : -92.47767639160156
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -59.091827392578125
Agent1_Train_StdReturn : 21.592273712158203
Agent1_Train_MaxReturn : -9.918960571289062
Agent1_Train_MinReturn : -84.41056060791016
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 66000
Agent1_TimeSinceStart : 214.66249251365662
Agent1_Critic_Loss : 1.7615559101104736
Agent1_Actor_Loss : -3.794795036315918
Agent1_Alpha_Loss : 4.952298164367676
Agent1_Temperature : 0.49344578485179624
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -41.93612289428711
Agent0_Eval_StdReturn : 29.191247940063477
Agent0_Eval_MaxReturn : 11.429388046264648
Agent0_Eval_MinReturn : -93.13506317138672
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -35.85474395751953
Agent0_Train_StdReturn : 23.679935455322266
Agent0_Train_MaxReturn : 1.1720590591430664
Agent0_Train_MinReturn : -73.43751525878906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 67500
Agent0_TimeSinceStart : 217.2539575099945
Agent0_Critic_Loss : 1.280444622039795
Agent0_Actor_Loss : -3.7360897064208984
Agent0_Alpha_Loss : 4.9155659675598145
Agent0_Temperature : 0.49329771237597325
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -61.96668243408203
Agent1_Eval_StdReturn : 45.431209564208984
Agent1_Eval_MaxReturn : -2.6612155437469482
Agent1_Eval_MinReturn : -128.30560302734375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -45.50614929199219
Agent1_Train_StdReturn : 28.39311408996582
Agent1_Train_MaxReturn : -4.970752716064453
Agent1_Train_MinReturn : -80.29825592041016
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 67500
Agent1_TimeSinceStart : 219.84181308746338
Agent1_Critic_Loss : 1.7219774723052979
Agent1_Actor_Loss : -3.879179000854492
Agent1_Alpha_Loss : 4.895681381225586
Agent1_Temperature : 0.4932981163514219
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -43.38237762451172
Agent0_Eval_StdReturn : 21.808530807495117
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -25.471790313720703
Agent1_Eval_StdReturn : 23.085474014282227
Agent1_Eval_MaxReturn : 7.504731178283691
Agent1_Eval_MinReturn : -57.629661560058594
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.67963409423828
Agent1_Train_StdReturn : 42.47309875488281
Agent1_Train_MaxReturn : 43.489341735839844
Agent1_Train_MinReturn : -107.12396240234375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 61500
Agent1_TimeSinceStart : 205.96371412277222
Agent1_Critic_Loss : 0.7795572280883789
Agent1_Actor_Loss : -0.6857151985168457
Agent1_Alpha_Loss : 0.988686740398407
Agent1_Temperature : 0.09877803194876415
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.19078826904297
Agent0_Eval_StdReturn : 30.394805908203125
Agent0_Eval_MaxReturn : 7.257411956787109
Agent0_Eval_MinReturn : -102.32559204101562
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -63.55945587158203
Agent0_Train_StdReturn : 36.213077545166016
Agent0_Train_MaxReturn : -1.7061786651611328
Agent0_Train_MinReturn : -127.50889587402344
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 63000
Agent0_TimeSinceStart : 208.5841257572174
Agent0_Critic_Loss : 0.7858294248580933
Agent0_Actor_Loss : -0.43463245034217834
Agent0_Alpha_Loss : 0.9870685935020447
Agent0_Temperature : 0.09874831516781518
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -27.799184799194336
Agent1_Eval_StdReturn : 31.898805618286133
Agent1_Eval_MaxReturn : 24.09625244140625
Agent1_Eval_MinReturn : -103.16249084472656
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -47.506126403808594
Agent1_Train_StdReturn : 21.08617401123047
Agent1_Train_MaxReturn : -15.192752838134766
Agent1_Train_MinReturn : -76.6612777709961
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 63000
Agent1_TimeSinceStart : 211.1839792728424
Agent1_Critic_Loss : 0.7641353607177734
Agent1_Actor_Loss : -0.6774346828460693
Agent1_Alpha_Loss : 0.9865731000900269
Agent1_Temperature : 0.09874847211193802
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.43918991088867
Agent0_Eval_StdReturn : 35.96240997314453
Agent0_Eval_MaxReturn : 23.60881805419922
Agent0_Eval_MinReturn : -81.23487854003906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -39.987327575683594
Agent0_Train_StdReturn : 44.87491989135742
Agent0_Train_MaxReturn : 2.895264148712158
Agent0_Train_MinReturn : -130.87010192871094
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 64500
Agent0_TimeSinceStart : 213.80387926101685
Agent0_Critic_Loss : 0.8052726984024048
Agent0_Actor_Loss : -0.4629135727882385
Agent0_Alpha_Loss : 0.9952311515808105
Agent0_Temperature : 0.09871870883035447
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -51.66417694091797
Agent1_Eval_StdReturn : 40.47594451904297
Agent1_Eval_MaxReturn : 17.91388511657715
Agent1_Eval_MinReturn : -124.74303436279297
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -40.414817810058594
Agent1_Train_StdReturn : 33.335182189941406
Agent1_Train_MaxReturn : 0.8383722305297852
Agent1_Train_MinReturn : -92.646484375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 64500
Agent1_TimeSinceStart : 216.4241464138031
Agent1_Critic_Loss : 0.7622727155685425
Agent1_Actor_Loss : -0.6423074007034302
Agent1_Alpha_Loss : 0.9840545654296875
Agent1_Temperature : 0.09871892691254984
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -38.6304931640625
Agent0_Eval_StdReturn : 36.98081588745117
Agent0_Eval_MaxReturn : 21.21718406677246
Agent0_Eval_MinReturn : -98.53414916992188
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -38.872520446777344
Agent0_Train_StdReturn : 31.455909729003906
Agent0_Train_MaxReturn : 5.893037796020508
Agent0_Train_MinReturn : -86.7760009765625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 66000
Agent0_TimeSinceStart : 219.04170513153076
Agent0_Critic_Loss : 0.5695723295211792
Agent0_Actor_Loss : -0.5214040279388428
Agent0_Alpha_Loss : 0.9852012395858765
Agent0_Temperature : 0.09868911984565922
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -45.45022964477539
Agent1_Eval_StdReturn : 37.39659118652344
Agent1_Eval_MaxReturn : 6.222562789916992
Agent1_Eval_MinReturn : -119.60018920898438
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -53.32037353515625
Agent1_Train_StdReturn : 32.440887451171875
Agent1_Train_MaxReturn : -8.223932266235352
Agent1_Train_MinReturn : -114.50747680664062
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 66000
Agent1_TimeSinceStart : 221.67024779319763
Agent1_Critic_Loss : 0.7051658034324646
Agent1_Actor_Loss : -0.6002453565597534
Agent1_Alpha_Loss : 0.9865850210189819
Agent1_Temperature : 0.09868938963186186
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -40.88225555419922
Agent0_Eval_StdReturn : 37.35234069824219
Agent0_Eval_MaxReturn : 28.736249923706055
Agent0_Eval_MinReturn : -113.81314086914062
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -37.88752365112305
Agent0_Train_StdReturn : 36.9916877746582
Agent0_Train_MaxReturn : 20.263477325439453
Agent0_Train_MinReturn : -100.32177734375
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 67500
Agent0_TimeSinceStart : 224.2939748764038
Agent0_Critic_Loss : 0.6068935990333557
Agent0_Actor_Loss : -0.5051787495613098
Agent0_Alpha_Loss : 0.9787667393684387
Agent0_Temperature : 0.09865956225948064
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -53.717987060546875
Agent1_Eval_StdReturn : 41.121337890625
Agent1_Eval_MaxReturn : 21.406185150146484
Agent1_Eval_MinReturn : -112.5570068359375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -18.8482608795166
Agent1_Train_StdReturn : 34.386558532714844
Agent1_Train_MaxReturn : 30.07822608947754
Agent1_Train_MinReturn : -75.94621276855469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 67500
Agent1_TimeSinceStart : 226.94109749794006
Agent1_Critic_Loss : 0.6781609654426575
Agent1_Actor_Loss : -0.5942045450210571
Agent1_Alpha_Loss : 0.9746663570404053
Agent1_Temperature : 0.09865988821827408
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...
Agent0_Eval_MaxReturn : -1.5416607856750488
Agent0_Eval_MinReturn : -67.04168701171875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -28.062374114990234
Agent0_Train_StdReturn : 22.445737838745117
Agent0_Train_MaxReturn : 2.126105785369873
Agent0_Train_MinReturn : -67.28471374511719
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 69000
Agent0_TimeSinceStart : 222.43051981925964
Agent0_Critic_Loss : 1.8247319459915161
Agent0_Actor_Loss : -3.6610426902770996
Agent0_Alpha_Loss : 4.929472923278809
Agent0_Temperature : 0.49314996641517295
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -41.37342071533203
Agent1_Eval_StdReturn : 21.746734619140625
Agent1_Eval_MaxReturn : 1.2481985092163086
Agent1_Eval_MinReturn : -72.64986419677734
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -48.246131896972656
Agent1_Train_StdReturn : 31.839237213134766
Agent1_Train_MaxReturn : -17.49102020263672
Agent1_Train_MinReturn : -113.43203735351562
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 69000
Agent1_TimeSinceStart : 225.02512669563293
Agent1_Critic_Loss : 1.9210047721862793
Agent1_Actor_Loss : -3.764266014099121
Agent1_Alpha_Loss : 4.902301788330078
Agent1_Temperature : 0.49315057264711726
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.014671325683594
Agent0_Eval_StdReturn : 31.928436279296875
Agent0_Eval_MaxReturn : 4.129268646240234
Agent0_Eval_MinReturn : -97.08164978027344
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -47.43907928466797
Agent0_Train_StdReturn : 29.52364158630371
Agent0_Train_MaxReturn : -5.005746364593506
Agent0_Train_MinReturn : -101.51258850097656
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 70500
Agent0_TimeSinceStart : 227.62510108947754
Agent0_Critic_Loss : 1.7897708415985107
Agent0_Actor_Loss : -3.7212789058685303
Agent0_Alpha_Loss : 4.940056800842285
Agent0_Temperature : 0.4930022722904781
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -31.797887802124023
Agent1_Eval_StdReturn : 11.8078031539917
Agent1_Eval_MaxReturn : -7.016168594360352
Agent1_Eval_MinReturn : -48.47431182861328
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.86894989013672
Agent1_Train_StdReturn : 39.04388427734375
Agent1_Train_MaxReturn : 15.32862377166748
Agent1_Train_MinReturn : -116.50778198242188
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 70500
Agent1_TimeSinceStart : 230.21780920028687
Agent1_Critic_Loss : 1.7432348728179932
Agent1_Actor_Loss : -3.802791118621826
Agent1_Alpha_Loss : 4.922794342041016
Agent1_Temperature : 0.49300309511332313
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -38.464881896972656
Agent0_Eval_StdReturn : 29.9790096282959
Agent0_Eval_MaxReturn : 19.48465347290039
Agent0_Eval_MinReturn : -74.74234008789062
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -24.366313934326172
Agent0_Train_StdReturn : 39.27000427246094
Agent0_Train_MaxReturn : 30.93075180053711
Agent0_Train_MinReturn : -100.71353149414062
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 72000
Agent0_TimeSinceStart : 232.81589365005493
Agent0_Critic_Loss : 2.2522404193878174
Agent0_Actor_Loss : -3.77675199508667
Agent0_Alpha_Loss : 4.939406871795654
Agent0_Temperature : 0.4928546302654093
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.65158462524414
Agent1_Eval_StdReturn : 22.134132385253906
Agent1_Eval_MaxReturn : -8.497740745544434
Agent1_Eval_MinReturn : -79.09637451171875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -45.607521057128906
Agent1_Train_StdReturn : 28.294761657714844
Agent1_Train_MaxReturn : 8.988882064819336
Agent1_Train_MinReturn : -75.28109741210938
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 72000
Agent1_TimeSinceStart : 235.42255353927612
Agent1_Critic_Loss : 1.8200373649597168
Agent1_Actor_Loss : -3.849208116531372
Agent1_Alpha_Loss : 4.937560081481934
Agent1_Temperature : 0.49285564537772536
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -64.23722076416016
Agent0_Eval_StdReturn : 18.932157516479492
Agent0_Eval_MaxReturn : -32.05133056640625
Agent0_Eval_MinReturn : -109.75360870361328
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -26.045787811279297
Agent0_Train_StdReturn : 22.332426071166992
Agent0_Train_MaxReturn : 18.866809844970703
Agent0_Train_MinReturn : -64.58123779296875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 73500
Agent0_TimeSinceStart : 238.0166609287262
Agent0_Critic_Loss : 1.7419202327728271
Agent0_Actor_Loss : -3.6782894134521484
Agent0_Alpha_Loss : 4.9341654777526855
Agent0_Temperature : 0.49270705150631133
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -47.41645050048828
Agent1_Eval_StdReturn : 43.527950286865234
Agent1_Eval_MaxReturn : 25.36953353881836
Agent1_Eval_MinReturn : -122.27521514892578
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -54.5841064453125
Agent1_Train_StdReturn : 50.68195724487305
Agent1_Train_MaxReturn : 21.703140258789062
Agent1_Train_MinReturn : -142.00079345703125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 73500
Agent1_TimeSinceStart : 240.61482310295105
Agent1_Critic_Loss : 1.708867073059082
Agent1_Actor_Loss : -3.7643930912017822
Agent1_Alpha_Loss : 4.884640693664551
Agent1_Temperature : 0.4927083509245765
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -38.00288772583008
Agent0_Eval_StdReturn : 17.53450584411621
Agent0_Eval_MaxReturn : -14.85899543762207
Agent0_Eval_MinReturn : -82.48365783691406
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -58.27642059326172
Agent0_Train_StdReturn : 23.741329193115234
Agent0_Train_MaxReturn : -17.145923614501953
Agent0_Train_MinReturn : -96.1558609008789
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 75000
Agent0_TimeSinceStart : 243.21140456199646
Agent0_Critic_Loss : 1.71933913230896
Agent0_Actor_Loss : -3.758608818054199
Agent0_Alpha_Loss : 4.926050662994385
Agent0_Temperature : 0.4925595527850704
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -42.212806701660156
Agent1_Eval_StdReturn : 39.4849739074707
Agent1_Eval_MaxReturn : 28.33927345275879
Agent1_Eval_MinReturn : -105.1104736328125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.3100700378418
Agent1_Train_StdReturn : 21.31736946105957
Agent1_Train_MaxReturn : -3.436028003692627
Agent1_Train_MinReturn : -72.58489990234375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 75000
Agent1_TimeSinceStart : 245.83278012275696
Agent1_Critic_Loss : 1.975348949432373
Agent1_Actor_Loss : -3.887624502182007
Agent1_Alpha_Loss : 4.91633415222168
Agent1_Temperature : 0.4925611219861794
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -35.09862518310547
Agent0_Eval_StdReturn : 43.762939453125
Agent0_Eval_MaxReturn : 20.80255126953125
Agent0_Eval_MinReturn : -130.941650390625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -55.08192825317383
Agent0_Train_StdReturn : 26.9521541595459
Agent0_Train_MaxReturn : -16.349742889404297
Agent0_Train_MinReturn : -100.61638641357422
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 76500
Agent0_TimeSinceStart : 248.44603729248047
Agent0_Critic_Loss : 1.6161441802978516
Agent0_Actor_Loss : -3.833272933959961
Agent0_Alpha_Loss : 4.919554233551025
Agent0_Temperature : 0.492412145084576
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -41.61649703979492
Agent1_Eval_StdReturn : 33.45518493652344
Agent1_Eval_MaxReturn : 15.83828353881836
Agent1_Eval_MinReturn : -103.16816711425781
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -28.695165634155273
Agent1_Train_StdReturn : 29.96913719177246
Agent1_Train_MaxReturn : 20.814430236816406
Agent1_Train_MinReturn : -80.60088348388672
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 76500
Agent1_TimeSinceStart : 251.05664682388306
Agent1_Critic_Loss : 1.991004467010498
Agent1_Actor_Loss : -3.890509843826294
Agent1_Alpha_Loss : 4.948032855987549
Agent1_Temperature : 0.492413878979521
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -24.539907455444336
Agent0_Eval_StdReturn : 32.222328186035156
Agent0_Eval_MaxReturn : 14.293283462524414
Agent0_Eval_MinReturn : -98.36580657958984
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -40.16940689086914
Agent0_Train_StdReturn : 18.961139678955078
Agent0_Train_MaxReturn : -14.16908073425293
Agent0_Train_MinReturn : -84.68556213378906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 78000
Agent0_TimeSinceStart : 253.6740539073944
Agent0_Critic_Loss : 1.730368971824646
Agent0_Actor_Loss : -3.8467884063720703
Agent0_Alpha_Loss : 4.902261734008789
Agent0_Temperature : 0.49226486418956195
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -42.40730667114258
Agent1_Eval_StdReturn : 33.16054916381836
Agent1_Eval_MaxReturn : 4.966348648071289
Agent1_Eval_MinReturn : -97.71640014648438
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -61.84505081176758
Agent1_Train_StdReturn : 25.4014835357666
Agent1_Train_MaxReturn : -25.47702407836914
Agent1_Train_MinReturn : -102.30450439453125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 78000
Agent1_TimeSinceStart : 256.2921693325043
Agent1_Critic_Loss : 1.4908154010772705
Agent1_Actor_Loss : -3.899353265762329
Agent1_Alpha_Loss : 4.94985294342041
Agent1_Temperature : 0.49226662343829053
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -70.13013458251953
Agent0_Eval_StdReturn : 37.010704040527344
Agent0_Eval_MaxReturn : -16.898544311523438
Agent0_Eval_MinReturn : -122.33576965332031
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -31.949752807617188
Agent0_Train_StdReturn : 27.483768463134766
Agent0_Train_MaxReturn : 15.490209579467773
Agent0_Train_MinReturn : -78.37274932861328
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 79500
Agent0_TimeSinceStart : 258.92409014701843
Agent0_Critic_Loss : 1.6055561304092407
Agent0_Actor_Loss : -3.931589126586914
Agent0_Alpha_Loss : 4.918378829956055
Agent0_Temperature : 0.49211766086766867
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -28.349239349365234
Agent1_Eval_StdReturn : 23.999006271362305
Agent1_Eval_MaxReturn : -2.968581199645996
Agent1_Eval_MinReturn : -68.2802734375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.17328643798828
Agent1_Train_StdReturn : 35.393741607666016
Agent1_Train_MaxReturn : 20.527618408203125
Agent1_Train_MinReturn : -110.19578552246094
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 79500
Agent1_TimeSinceStart : 261.55083179473877
Agent1_Critic_Loss : 1.839479684829712
Agent1_Actor_Loss : -3.8234896659851074
Agent1_Alpha_Loss : 4.928481578826904
Agent1_Temperature : 0.49211941291665207
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -59.69315719604492
Agent0_Eval_StdReturn : 23.08131980895996
Agent0_Eval_MaxReturn : -30.736204147338867
Agent0_Eval_MinReturn : -98.43721771240234
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -45.87992477416992
Agent0_Train_StdReturn : 24.695636749267578
Agent0_Train_MaxReturn : 0.751317024230957
Agent0_Train_MinReturn : -83.9870376586914
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 81000
Agent0_TimeSinceStart : 264.18040895462036
Agent0_Critic_Loss : 1.5566983222961426
Agent0_Actor_Loss : -4.003267765045166
Agent0_Alpha_Loss : 4.931643486022949
Agent0_Temperature : 0.49197049833168244
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -35.75632858276367
Agent1_Eval_StdReturn : 24.879148483276367
Agent1_Eval_MaxReturn : 3.8230762481689453
Agent1_Eval_MinReturn : -77.96305847167969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -59.99458694458008
Agent1_Train_StdReturn : 32.28258514404297
Agent1_Train_MaxReturn : -14.817541122436523
Agent1_Train_MinReturn : -123.14173889160156
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 81000
Agent1_TimeSinceStart : 266.8115782737732
Agent1_Critic_Loss : 2.176107406616211
Agent1_Actor_Loss : -3.8159122467041016
Agent1_Alpha_Loss : 4.902620315551758
Agent1_Temperature : 0.4919723095019868
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -52.3389892578125
Agent0_Eval_StdReturn : 39.25265121459961
Agent0_Eval_MaxReturn : 4.085447311401367
Agent0_Eval_MinReturn : -122.29325866699219
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.19316101074219

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.879215240478516
Agent0_Eval_StdReturn : 26.859207153320312
Agent0_Eval_MaxReturn : 20.24459457397461
Agent0_Eval_MinReturn : -78.57439422607422
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.619800567626953
Agent0_Train_StdReturn : 25.82798957824707
Agent0_Train_MaxReturn : 10.529390335083008
Agent0_Train_MinReturn : -68.90978240966797
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 69000
Agent0_TimeSinceStart : 229.59179401397705
Agent0_Critic_Loss : 0.7299948334693909
Agent0_Actor_Loss : -0.43494200706481934
Agent0_Alpha_Loss : 0.9816904067993164
Agent0_Temperature : 0.09863002645852804
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.450149536132812
Agent1_Eval_StdReturn : 24.69829559326172
Agent1_Eval_MaxReturn : 13.028192520141602
Agent1_Eval_MinReturn : -67.0530776977539
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -35.6467399597168
Agent1_Train_StdReturn : 41.18144989013672
Agent1_Train_MaxReturn : 12.105114936828613
Agent1_Train_MinReturn : -124.67857360839844
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 69000
Agent1_TimeSinceStart : 232.2387089729309
Agent1_Critic_Loss : 0.8073104619979858
Agent1_Actor_Loss : -0.5112317800521851
Agent1_Alpha_Loss : 0.9767638444900513
Agent1_Temperature : 0.0986304143796601
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -34.17782974243164
Agent0_Eval_StdReturn : 23.0425968170166
Agent0_Eval_MaxReturn : -1.397256851196289
Agent0_Eval_MinReturn : -61.23556900024414
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -35.79780197143555
Agent0_Train_StdReturn : 33.54450225830078
Agent0_Train_MaxReturn : 9.200859069824219
Agent0_Train_MinReturn : -92.31640625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 70500
Agent0_TimeSinceStart : 234.88343715667725
Agent0_Critic_Loss : 0.637910008430481
Agent0_Actor_Loss : -0.4944652020931244
Agent0_Alpha_Loss : 0.9816797971725464
Agent0_Temperature : 0.0986005108554917
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -40.344051361083984
Agent1_Eval_StdReturn : 26.047067642211914
Agent1_Eval_MaxReturn : 10.053140640258789
Agent1_Eval_MinReturn : -76.24705505371094
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -45.70708084106445
Agent1_Train_StdReturn : 32.32157897949219
Agent1_Train_MaxReturn : 10.774044036865234
Agent1_Train_MinReturn : -117.03169250488281
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 70500
Agent1_TimeSinceStart : 237.510324716568
Agent1_Critic_Loss : 0.7576922178268433
Agent1_Actor_Loss : -0.5626367926597595
Agent1_Alpha_Loss : 0.9840418100357056
Agent1_Temperature : 0.09860094847336393
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -35.78114318847656
Agent0_Eval_StdReturn : 29.71436882019043
Agent0_Eval_MaxReturn : 4.885702133178711
Agent0_Eval_MinReturn : -78.9186019897461
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -51.4144401550293
Agent0_Train_StdReturn : 22.9044189453125
Agent0_Train_MaxReturn : -11.82342529296875
Agent0_Train_MinReturn : -88.9567642211914
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 72000
Agent0_TimeSinceStart : 240.15414834022522
Agent0_Critic_Loss : 0.7883453369140625
Agent0_Actor_Loss : -0.4764888882637024
Agent0_Alpha_Loss : 0.984815239906311
Agent0_Temperature : 0.09857100655479324
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.727699279785156
Agent1_Eval_StdReturn : 27.736530303955078
Agent1_Eval_MaxReturn : 17.57894515991211
Agent1_Eval_MinReturn : -86.90775299072266
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -37.88695526123047
Agent1_Train_StdReturn : 23.33734893798828
Agent1_Train_MaxReturn : -1.6860675811767578
Agent1_Train_MinReturn : -86.57305145263672
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 72000
Agent1_TimeSinceStart : 242.77948451042175
Agent1_Critic_Loss : 0.7298257946968079
Agent1_Actor_Loss : -0.5640579462051392
Agent1_Alpha_Loss : 0.9804238080978394
Agent1_Temperature : 0.09857149901109885
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.58694839477539
Agent0_Eval_StdReturn : 20.75455093383789
Agent0_Eval_MaxReturn : -10.83821964263916
Agent0_Eval_MinReturn : -78.21135711669922
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -26.18564224243164
Agent0_Train_StdReturn : 20.639667510986328
Agent0_Train_MaxReturn : 13.556485176086426
Agent0_Train_MinReturn : -55.97374725341797
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 73500
Agent0_TimeSinceStart : 245.41372418403625
Agent0_Critic_Loss : 0.6899142861366272
Agent0_Actor_Loss : -0.44469085335731506
Agent0_Alpha_Loss : 0.9911396503448486
Agent0_Temperature : 0.09854149807182837
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -46.21072006225586
Agent1_Eval_StdReturn : 34.49800109863281
Agent1_Eval_MaxReturn : 30.22909927368164
Agent1_Eval_MinReturn : -96.69877624511719
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.07649612426758
Agent1_Train_StdReturn : 37.60407257080078
Agent1_Train_MaxReturn : -3.778256416320801
Agent1_Train_MinReturn : -139.62197875976562
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 73500
Agent1_TimeSinceStart : 248.05195331573486
Agent1_Critic_Loss : 0.7789053916931152
Agent1_Actor_Loss : -0.5975539684295654
Agent1_Alpha_Loss : 0.9684298038482666
Agent1_Temperature : 0.09854209362626051
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -52.11661911010742
Agent0_Eval_StdReturn : 26.663362503051758
Agent0_Eval_MaxReturn : -8.073087692260742
Agent0_Eval_MinReturn : -105.67350769042969
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -61.744537353515625
Agent0_Train_StdReturn : 27.148212432861328
Agent0_Train_MaxReturn : -12.619760513305664
Agent0_Train_MinReturn : -109.82768249511719
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 75000
Agent0_TimeSinceStart : 250.69067239761353
Agent0_Critic_Loss : 0.7907835245132446
Agent0_Actor_Loss : -0.441400945186615
Agent0_Alpha_Loss : 0.9792989492416382
Agent0_Temperature : 0.09851201518900093
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...
Agent0_Train_StdReturn : 26.877782821655273
Agent0_Train_MaxReturn : 6.413919448852539
Agent0_Train_MinReturn : -81.54287719726562
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 82500
Agent0_TimeSinceStart : 269.44436383247375
Agent0_Critic_Loss : 1.5655299425125122
Agent0_Actor_Loss : -3.935455322265625
Agent0_Alpha_Loss : 4.9213361740112305
Agent0_Temperature : 0.49182340139254865
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -46.8763313293457
Agent1_Eval_StdReturn : 29.797401428222656
Agent1_Eval_MaxReturn : -10.713335037231445
Agent1_Eval_MinReturn : -99.80489349365234
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -52.5914421081543
Agent1_Train_StdReturn : 26.153732299804688
Agent1_Train_MaxReturn : -15.301918983459473
Agent1_Train_MinReturn : -99.03672790527344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 82500
Agent1_TimeSinceStart : 272.07987689971924
Agent1_Critic_Loss : 1.7371304035186768
Agent1_Actor_Loss : -3.842700481414795
Agent1_Alpha_Loss : 4.915615081787109
Agent1_Temperature : 0.49182527345912996
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.29230499267578
Agent0_Eval_StdReturn : 25.959569931030273
Agent0_Eval_MaxReturn : 12.897964477539062
Agent0_Eval_MinReturn : -78.82441711425781
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -47.12273025512695
Agent0_Train_StdReturn : 24.818384170532227
Agent0_Train_MaxReturn : 9.86552619934082
Agent0_Train_MinReturn : -81.53300476074219
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 84000
Agent0_TimeSinceStart : 274.7193546295166
Agent0_Critic_Loss : 1.6005616188049316
Agent0_Actor_Loss : -4.033444404602051
Agent0_Alpha_Loss : 4.934596061706543
Agent0_Temperature : 0.4916763344304612
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -44.88588333129883
Agent1_Eval_StdReturn : 20.93071937561035
Agent1_Eval_MaxReturn : -18.75057601928711
Agent1_Eval_MinReturn : -75.03584289550781
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -45.22892379760742
Agent1_Train_StdReturn : 38.82371139526367
Agent1_Train_MaxReturn : -0.9141378402709961
Agent1_Train_MinReturn : -129.5189666748047
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 84000
Agent1_TimeSinceStart : 277.3514950275421
Agent1_Critic_Loss : 1.6693639755249023
Agent1_Actor_Loss : -3.856977701187134
Agent1_Alpha_Loss : 4.922025680541992
Agent1_Temperature : 0.491678285608713
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -51.50087356567383
Agent0_Eval_StdReturn : 46.05498504638672
Agent0_Eval_MaxReturn : 9.150127410888672
Agent0_Eval_MinReturn : -128.2193603515625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -59.302284240722656
Agent0_Train_StdReturn : 17.873294830322266
Agent0_Train_MaxReturn : -21.828399658203125
Agent0_Train_MinReturn : -86.72084045410156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 85500
Agent0_TimeSinceStart : 279.9892740249634
Agent0_Critic_Loss : 1.7282743453979492
Agent0_Actor_Loss : -4.002697944641113
Agent0_Alpha_Loss : 4.913012504577637
Agent0_Temperature : 0.49152935133985176
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.84025192260742
Agent1_Eval_StdReturn : 30.150957107543945
Agent1_Eval_MaxReturn : 3.4300942420959473
Agent1_Eval_MinReturn : -93.01313781738281
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -57.75602340698242
Agent1_Train_StdReturn : 37.56206130981445
Agent1_Train_MaxReturn : -13.413328170776367
Agent1_Train_MinReturn : -147.67498779296875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 85500
Agent1_TimeSinceStart : 282.6319398880005
Agent1_Critic_Loss : 1.7706880569458008
Agent1_Actor_Loss : -3.889122724533081
Agent1_Alpha_Loss : 4.956366062164307
Agent1_Temperature : 0.49153126040363526
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -24.439823150634766
Agent0_Eval_StdReturn : 26.944250106811523
Agent0_Eval_MaxReturn : 24.036951065063477
Agent0_Eval_MinReturn : -66.66277313232422
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -57.181854248046875
Agent0_Train_StdReturn : 27.547880172729492
Agent0_Train_MaxReturn : 2.5861644744873047
Agent0_Train_MinReturn : -96.03419494628906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 87000
Agent0_TimeSinceStart : 285.2645740509033
Agent0_Critic_Loss : 1.7004481554031372
Agent0_Actor_Loss : -3.9777519702911377
Agent0_Alpha_Loss : 4.919330596923828
Agent0_Temperature : 0.4913824312766126
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -42.52069854736328
Agent1_Eval_StdReturn : 40.720367431640625
Agent1_Eval_MaxReturn : 17.385120391845703
Agent1_Eval_MinReturn : -106.21040344238281
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.300804138183594
Agent1_Train_StdReturn : 20.402040481567383
Agent1_Train_MaxReturn : 4.598602771759033
Agent1_Train_MinReturn : -75.64689636230469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 87000
Agent1_TimeSinceStart : 287.89813470840454
Agent1_Critic_Loss : 1.7389625310897827
Agent1_Actor_Loss : -3.9575815200805664
Agent1_Alpha_Loss : 4.927280426025391
Agent1_Temperature : 0.49138427803767654
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.34315872192383
Agent0_Eval_StdReturn : 42.09407424926758
Agent0_Eval_MaxReturn : 23.94333839416504
Agent0_Eval_MinReturn : -132.00653076171875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.154354095458984
Agent0_Train_StdReturn : 34.77784729003906
Agent0_Train_MaxReturn : 31.652976989746094
Agent0_Train_MinReturn : -87.87525939941406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 88500
Agent0_TimeSinceStart : 290.52136301994324
Agent0_Critic_Loss : 1.8697588443756104
Agent0_Actor_Loss : -3.9924001693725586
Agent0_Alpha_Loss : 4.936643123626709
Agent0_Temperature : 0.49123552856951397
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -31.448938369750977
Agent1_Eval_StdReturn : 36.139808654785156
Agent1_Eval_MaxReturn : 29.268024444580078
Agent1_Eval_MinReturn : -86.99560546875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -43.47336196899414
Agent1_Train_StdReturn : 23.299419403076172
Agent1_Train_MaxReturn : 2.883051872253418

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -34.04757308959961
Agent1_Eval_StdReturn : 23.890926361083984
Agent1_Eval_MaxReturn : -4.472447395324707
Agent1_Eval_MinReturn : -82.98735809326172
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -37.568355560302734
Agent1_Train_StdReturn : 25.550609588623047
Agent1_Train_MaxReturn : -5.192645072937012
Agent1_Train_MinReturn : -85.16412353515625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 75000
Agent1_TimeSinceStart : 253.33027911186218
Agent1_Critic_Loss : 0.682093620300293
Agent1_Actor_Loss : -0.5768343210220337
Agent1_Alpha_Loss : 0.975928783416748
Agent1_Temperature : 0.09851270992988205
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -20.13193130493164
Agent0_Eval_StdReturn : 31.46375274658203
Agent0_Eval_MaxReturn : 8.475042343139648
Agent0_Eval_MinReturn : -73.9013442993164
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.984092712402344
Agent0_Train_StdReturn : 27.201889038085938
Agent0_Train_MaxReturn : -1.8974113464355469
Agent0_Train_MinReturn : -73.46279907226562
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 76500
Agent0_TimeSinceStart : 255.97047996520996
Agent0_Critic_Loss : 0.740324079990387
Agent0_Actor_Loss : -0.4126487970352173
Agent0_Alpha_Loss : 0.9755273461341858
Agent0_Temperature : 0.0984825649389329
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.73204803466797
Agent1_Eval_StdReturn : 36.87681579589844
Agent1_Eval_MaxReturn : 5.105381965637207
Agent1_Eval_MinReturn : -100.7763671875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -25.306615829467773
Agent1_Train_StdReturn : 35.52524948120117
Agent1_Train_MaxReturn : 24.539077758789062
Agent1_Train_MinReturn : -104.38292694091797
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 76500
Agent1_TimeSinceStart : 258.61599469184875
Agent1_Critic_Loss : 0.7832931280136108
Agent1_Actor_Loss : -0.6251115798950195
Agent1_Alpha_Loss : 0.9854437708854675
Agent1_Temperature : 0.09848332320593302
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -38.728111267089844
Agent0_Eval_StdReturn : 27.025070190429688
Agent0_Eval_MaxReturn : -11.30111312866211
Agent0_Eval_MinReturn : -87.64701843261719
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.521827697753906
Agent0_Train_StdReturn : 19.402503967285156
Agent0_Train_MaxReturn : -25.09458351135254
Agent0_Train_MinReturn : -77.50454711914062
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 78000
Agent0_TimeSinceStart : 261.2616512775421
Agent0_Critic_Loss : 0.7034173607826233
Agent0_Actor_Loss : -0.443493127822876
Agent0_Alpha_Loss : 0.9781957864761353
Agent0_Temperature : 0.09845313796083818
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -40.42023468017578
Agent1_Eval_StdReturn : 24.63620376586914
Agent1_Eval_MaxReturn : 7.671339988708496
Agent1_Eval_MinReturn : -76.61384582519531
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.44300842285156
Agent1_Train_StdReturn : 21.544918060302734
Agent1_Train_MaxReturn : -0.6492977142333984
Agent1_Train_MinReturn : -76.80012512207031
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 78000
Agent1_TimeSinceStart : 263.89998412132263
Agent1_Critic_Loss : 0.638287365436554
Agent1_Actor_Loss : -0.6257447004318237
Agent1_Alpha_Loss : 0.9810934066772461
Agent1_Temperature : 0.09845394508405259
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.50432586669922
Agent0_Eval_StdReturn : 38.427093505859375
Agent0_Eval_MaxReturn : 5.980242729187012
Agent0_Eval_MinReturn : -131.58364868164062
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.27899932861328
Agent0_Train_StdReturn : 23.717056274414062
Agent0_Train_MaxReturn : -4.219089508056641
Agent0_Train_MinReturn : -78.45625305175781
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 79500
Agent0_TimeSinceStart : 266.54088044166565
Agent0_Critic_Loss : 0.6148272752761841
Agent0_Actor_Loss : -0.5200257897377014
Agent0_Alpha_Loss : 0.9816038012504578
Agent0_Temperature : 0.0984237241372749
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -43.75585174560547
Agent1_Eval_StdReturn : 14.301628112792969
Agent1_Eval_MaxReturn : -16.600357055664062
Agent1_Eval_MinReturn : -62.0599365234375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -44.34486770629883
Agent1_Train_StdReturn : 43.17292022705078
Agent1_Train_MaxReturn : 22.251802444458008
Agent1_Train_MinReturn : -106.73728942871094
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 79500
Agent1_TimeSinceStart : 269.1780366897583
Agent1_Critic_Loss : 0.761432945728302
Agent1_Actor_Loss : -0.6163406372070312
Agent1_Alpha_Loss : 0.9835087060928345
Agent1_Temperature : 0.09842456946606395
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -59.37261199951172
Agent0_Eval_StdReturn : 33.37680435180664
Agent0_Eval_MaxReturn : 8.216144561767578
Agent0_Eval_MinReturn : -103.44961547851562
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -40.45314025878906
Agent0_Train_StdReturn : 25.247661590576172
Agent0_Train_MaxReturn : -9.081579208374023
Agent0_Train_MinReturn : -77.96025085449219
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 81000
Agent0_TimeSinceStart : 271.8170976638794
Agent0_Critic_Loss : 0.7925280332565308
Agent0_Actor_Loss : -0.5390182733535767
Agent0_Alpha_Loss : 0.9798160791397095
Agent0_Temperature : 0.09839432714185732
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -33.870872497558594
Agent1_Eval_StdReturn : 24.357894897460938
Agent1_Eval_MaxReturn : -0.2892017364501953
Agent1_Eval_MinReturn : -77.00688171386719
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -51.53571319580078
Agent1_Train_StdReturn : 29.126184463500977
Agent1_Train_MaxReturn : -6.104682445526123
Agent1_Train_MinReturn : -86.88946533203125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 81000
Agent1_TimeSinceStart : 274.45789194107056
Agent1_Critic_Loss : 0.803856611251831
Agent1_Actor_Loss : -0.6095976233482361
Agent1_Alpha_Loss : 0.9781920909881592
Agent1_Temperature : 0.09839520979416122
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Train_MinReturn : -76.24880981445312
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 88500
Agent1_TimeSinceStart : 293.1645941734314
Agent1_Critic_Loss : 1.5528208017349243
Agent1_Actor_Loss : -3.9958839416503906
Agent1_Alpha_Loss : 4.932797908782959
Agent1_Temperature : 0.4912373243519758
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -50.836421966552734
Agent0_Eval_StdReturn : 39.229736328125
Agent0_Eval_MaxReturn : 1.6568679809570312
Agent0_Eval_MinReturn : -116.74463653564453
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -42.17338943481445
Agent0_Train_StdReturn : 20.91474723815918
Agent0_Train_MaxReturn : -6.702391624450684
Agent0_Train_MinReturn : -69.49148559570312
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 90000
Agent0_TimeSinceStart : 295.8525173664093
Agent0_Critic_Loss : 1.4262351989746094
Agent0_Actor_Loss : -4.020144462585449
Agent0_Alpha_Loss : 4.9347944259643555
Agent0_Temperature : 0.49108865011590225
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -61.29517364501953
Agent1_Eval_StdReturn : 30.20671272277832
Agent1_Eval_MaxReturn : -3.1435089111328125
Agent1_Eval_MinReturn : -99.36769104003906
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.21852493286133
Agent1_Train_StdReturn : 23.10080909729004
Agent1_Train_MaxReturn : 7.421572685241699
Agent1_Train_MinReturn : -69.00386810302734
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 90000
Agent1_TimeSinceStart : 298.5011205673218
Agent1_Critic_Loss : 1.680739164352417
Agent1_Actor_Loss : -4.00164794921875
Agent1_Alpha_Loss : 4.903947830200195
Agent1_Temperature : 0.4910904719398518
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -56.187232971191406
Agent0_Eval_StdReturn : 27.545150756835938
Agent0_Eval_MaxReturn : -9.785687446594238
Agent0_Eval_MinReturn : -109.1949462890625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -60.2503547668457
Agent0_Train_StdReturn : 44.51836013793945
Agent0_Train_MaxReturn : -0.9359226226806641
Agent0_Train_MinReturn : -152.2251739501953
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 91500
Agent0_TimeSinceStart : 301.14771342277527
Agent0_Critic_Loss : 1.7674933671951294
Agent0_Actor_Loss : -3.873002529144287
Agent0_Alpha_Loss : 4.903096675872803
Agent0_Temperature : 0.4909418762231694
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -44.92639923095703
Agent1_Eval_StdReturn : 29.693565368652344
Agent1_Eval_MaxReturn : -4.150221824645996
Agent1_Eval_MinReturn : -91.45220184326172
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -40.630306243896484
Agent1_Train_StdReturn : 27.333675384521484
Agent1_Train_MaxReturn : -3.7404909133911133
Agent1_Train_MinReturn : -103.94630432128906
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 91500
Agent1_TimeSinceStart : 303.787216424942
Agent1_Critic_Loss : 1.6448832750320435
Agent1_Actor_Loss : -3.9842052459716797
Agent1_Alpha_Loss : 4.875380992889404
Agent1_Temperature : 0.49094378468397976
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -47.234619140625
Agent0_Eval_StdReturn : 27.926944732666016
Agent0_Eval_MaxReturn : 16.066810607910156
Agent0_Eval_MinReturn : -86.41230010986328
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -57.70961380004883
Agent0_Train_StdReturn : 30.949975967407227
Agent0_Train_MaxReturn : 0.7458095550537109
Agent0_Train_MinReturn : -113.26020050048828
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 93000
Agent0_TimeSinceStart : 306.4261283874512
Agent0_Critic_Loss : 2.0061144828796387
Agent0_Actor_Loss : -3.919509172439575
Agent0_Alpha_Loss : 4.927651882171631
Agent0_Temperature : 0.4907951381003886
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -38.001304626464844
Agent1_Eval_StdReturn : 29.951339721679688
Agent1_Eval_MaxReturn : 15.217466354370117
Agent1_Eval_MinReturn : -74.52490234375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.297821044921875
Agent1_Train_StdReturn : 35.08108901977539
Agent1_Train_MaxReturn : 13.551031112670898
Agent1_Train_MinReturn : -97.22877502441406
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 93000
Agent1_TimeSinceStart : 309.06239652633667
Agent1_Critic_Loss : 1.4021427631378174
Agent1_Actor_Loss : -4.042445182800293
Agent1_Alpha_Loss : 4.940547943115234
Agent1_Temperature : 0.4907970855088143
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -39.99186325073242
Agent0_Eval_StdReturn : 31.27952003479004
Agent0_Eval_MaxReturn : 4.567392349243164
Agent0_Eval_MinReturn : -101.14305877685547
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.0749626159668
Agent0_Train_StdReturn : 28.12351417541504
Agent0_Train_MaxReturn : -8.271218299865723
Agent0_Train_MinReturn : -106.60462951660156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 94500
Agent0_TimeSinceStart : 311.70740509033203
Agent0_Critic_Loss : 1.4644849300384521
Agent0_Actor_Loss : -3.7954015731811523
Agent0_Alpha_Loss : 4.871641159057617
Agent0_Temperature : 0.4906485759574189
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.956912994384766
Agent1_Eval_StdReturn : 34.68489074707031
Agent1_Eval_MaxReturn : 20.734325408935547
Agent1_Eval_MinReturn : -85.85841369628906
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -37.079673767089844
Agent1_Train_StdReturn : 31.47248077392578
Agent1_Train_MaxReturn : 10.943615913391113
Agent1_Train_MinReturn : -84.91522216796875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 94500
Agent1_TimeSinceStart : 314.35679388046265
Agent1_Critic_Loss : 1.69256591796875
Agent1_Actor_Loss : -4.040233612060547
Agent1_Alpha_Loss : 4.890851020812988
Agent1_Temperature : 0.49065050414828065
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.52999496459961
Agent0_Eval_StdReturn : 43.17069625854492
Agent0_Eval_MaxReturn : 7.5909528732299805
Agent0_Eval_MinReturn : -126.26535034179688
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -59.04351043701172
Agent0_Train_StdReturn : 26.964515686035156
Agent0_Train_MaxReturn : -25.692893981933594
Agent0_Train_MinReturn : -107.11470031738281
Agent0_Train_AverageEpLen : 150.0
Agent0_Eval_AverageReturn : -51.45915985107422
Agent0_Eval_StdReturn : 39.1889533996582
Agent0_Eval_MaxReturn : 10.206110000610352
Agent0_Eval_MinReturn : -132.8092041015625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -49.27009201049805
Agent0_Train_StdReturn : 35.27111053466797
Agent0_Train_MaxReturn : 18.34749412536621
Agent0_Train_MinReturn : -102.15371704101562
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 82500
Agent0_TimeSinceStart : 277.2091760635376
Agent0_Critic_Loss : 0.6699696183204651
Agent0_Actor_Loss : -0.47207996249198914
Agent0_Alpha_Loss : 0.9812047481536865
Agent0_Temperature : 0.09836494246624933
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -20.620052337646484
Agent1_Eval_StdReturn : 27.130006790161133
Agent1_Eval_MaxReturn : 20.903385162353516
Agent1_Eval_MinReturn : -91.68377685546875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -41.241065979003906
Agent1_Train_StdReturn : 34.98657989501953
Agent1_Train_MaxReturn : 14.487289428710938
Agent1_Train_MinReturn : -88.92524719238281
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 82500
Agent1_TimeSinceStart : 279.8660476207733
Agent1_Critic_Loss : 0.6613867282867432
Agent1_Actor_Loss : -0.6131756901741028
Agent1_Alpha_Loss : 0.9772113561630249
Agent1_Temperature : 0.0983658674006071
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -41.617584228515625
Agent0_Eval_StdReturn : 31.597078323364258
Agent0_Eval_MaxReturn : 1.5512545108795166
Agent0_Eval_MinReturn : -96.48040008544922
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.53838348388672
Agent0_Train_StdReturn : 34.518280029296875
Agent0_Train_MaxReturn : 0.5356531143188477
Agent0_Train_MinReturn : -114.62251281738281
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 84000
Agent0_TimeSinceStart : 282.51299118995667
Agent0_Critic_Loss : 0.6671289205551147
Agent0_Actor_Loss : -0.5423383712768555
Agent0_Alpha_Loss : 0.9844100475311279
Agent0_Temperature : 0.09833556165210451
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -40.02417755126953
Agent1_Eval_StdReturn : 31.11215591430664
Agent1_Eval_MaxReturn : 12.64686393737793
Agent1_Eval_MinReturn : -89.17689514160156
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -40.7326774597168
Agent1_Train_StdReturn : 21.681303024291992
Agent1_Train_MaxReturn : -13.839909553527832
Agent1_Train_MinReturn : -93.30219268798828
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 84000
Agent1_TimeSinceStart : 285.16242361068726
Agent1_Critic_Loss : 0.6803967952728271
Agent1_Actor_Loss : -0.5976460576057434
Agent1_Alpha_Loss : 0.9763250350952148
Agent1_Temperature : 0.09833654323406245
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -57.990989685058594
Agent0_Eval_StdReturn : 29.435894012451172
Agent0_Eval_MaxReturn : -17.15825653076172
Agent0_Eval_MinReturn : -119.1812515258789
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.97332000732422
Agent0_Train_StdReturn : 24.53964614868164
Agent0_Train_MaxReturn : -17.68537139892578
Agent0_Train_MinReturn : -108.43716430664062
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 85500
Agent0_TimeSinceStart : 287.8091733455658
Agent0_Critic_Loss : 0.696967363357544
Agent0_Actor_Loss : -0.5113206505775452
Agent0_Alpha_Loss : 0.9818161725997925
Agent0_Temperature : 0.09830619145757971
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -34.164215087890625
Agent1_Eval_StdReturn : 29.869014739990234
Agent1_Eval_MaxReturn : -5.21875524520874
Agent1_Eval_MinReturn : -95.77336120605469
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.96207046508789
Agent1_Train_StdReturn : 34.07168197631836
Agent1_Train_MaxReturn : 4.257178783416748
Agent1_Train_MinReturn : -109.48002624511719
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 85500
Agent1_TimeSinceStart : 290.4634006023407
Agent1_Critic_Loss : 0.6027069091796875
Agent1_Actor_Loss : -0.6301707625389099
Agent1_Alpha_Loss : 0.9846620559692383
Agent1_Temperature : 0.09830721540973311
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -12.934916496276855
Agent0_Eval_StdReturn : 24.9720516204834
Agent0_Eval_MaxReturn : 30.02733612060547
Agent0_Eval_MinReturn : -40.38978576660156
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -50.06792068481445
Agent0_Train_StdReturn : 35.02288055419922
Agent0_Train_MaxReturn : 15.004474639892578
Agent0_Train_MinReturn : -108.21749877929688
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 87000
Agent0_TimeSinceStart : 293.1222858428955
Agent0_Critic_Loss : 0.7179665565490723
Agent0_Actor_Loss : -0.4788512587547302
Agent0_Alpha_Loss : 0.9817802309989929
Agent0_Temperature : 0.09827683157668547
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -53.14979934692383
Agent1_Eval_StdReturn : 34.41716384887695
Agent1_Eval_MaxReturn : 3.2532806396484375
Agent1_Eval_MinReturn : -95.06568908691406
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -32.08260726928711
Agent1_Train_StdReturn : 26.974267959594727
Agent1_Train_MaxReturn : 11.179049491882324
Agent1_Train_MinReturn : -68.50126647949219
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 87000
Agent1_TimeSinceStart : 295.7766909599304
Agent1_Critic_Loss : 0.7545191049575806
Agent1_Actor_Loss : -0.5954207181930542
Agent1_Alpha_Loss : 0.9762346744537354
Agent1_Temperature : 0.09827790592038813
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -52.28245162963867
Agent0_Eval_StdReturn : 21.764936447143555
Agent0_Eval_MaxReturn : -8.149843215942383
Agent0_Eval_MinReturn : -80.50288391113281
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -41.35548400878906
Agent0_Train_StdReturn : 20.75236701965332
Agent0_Train_MaxReturn : -15.456398010253906
Agent0_Train_MinReturn : -94.09422302246094
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 88500
Agent0_TimeSinceStart : 298.4375512599945
Agent0_Critic_Loss : 0.7705745100975037
Agent0_Actor_Loss : -0.45699363946914673
Agent0_Alpha_Loss : 0.979751706123352
Agent0_Temperature : 0.09824748667282572
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -27.12771987915039
Agent0_Train_EnvstepsSoFar : 96000
Agent0_TimeSinceStart : 317.00241208076477
Agent0_Critic_Loss : 1.7942681312561035
Agent0_Actor_Loss : -3.938969135284424
Agent0_Alpha_Loss : 4.919707298278809
Agent0_Temperature : 0.49050205350575077
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -49.09637451171875
Agent1_Eval_StdReturn : 29.430753707885742
Agent1_Eval_MaxReturn : 0.3110160827636719
Agent1_Eval_MinReturn : -87.9996337890625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -40.844268798828125
Agent1_Train_StdReturn : 26.839962005615234
Agent1_Train_MaxReturn : 7.244363784790039
Agent1_Train_MinReturn : -86.16514587402344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 96000
Agent1_TimeSinceStart : 319.6724910736084
Agent1_Critic_Loss : 1.5161141157150269
Agent1_Actor_Loss : -4.036515712738037
Agent1_Alpha_Loss : 4.916514873504639
Agent1_Temperature : 0.4905039670505116
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -49.772438049316406
Agent0_Eval_StdReturn : 55.05436325073242
Agent0_Eval_MaxReturn : 27.193424224853516
Agent0_Eval_MinReturn : -147.1686248779297
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -41.631919860839844
Agent0_Train_StdReturn : 26.649738311767578
Agent0_Train_MaxReturn : 16.767126083374023
Agent0_Train_MinReturn : -79.17093658447266
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 97500
Agent0_TimeSinceStart : 322.327832698822
Agent0_Critic_Loss : 1.802323341369629
Agent0_Actor_Loss : -3.90689754486084
Agent0_Alpha_Loss : 4.932576656341553
Agent0_Temperature : 0.4903555380300015
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -50.93427276611328
Agent1_Eval_StdReturn : 35.87242889404297
Agent1_Eval_MaxReturn : 25.550901412963867
Agent1_Eval_MinReturn : -94.73614501953125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -28.140422821044922
Agent1_Train_StdReturn : 33.089332580566406
Agent1_Train_MaxReturn : 33.57632064819336
Agent1_Train_MinReturn : -86.21517944335938
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 97500
Agent1_TimeSinceStart : 324.97733426094055
Agent1_Critic_Loss : 1.9576411247253418
Agent1_Actor_Loss : -3.970710277557373
Agent1_Alpha_Loss : 4.9022698402404785
Agent1_Temperature : 0.4903575090980371
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.08827209472656
Agent0_Eval_StdReturn : 26.496774673461914
Agent0_Eval_MaxReturn : 16.316184997558594
Agent0_Eval_MinReturn : -75.5367431640625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -46.79169845581055
Agent0_Train_StdReturn : 29.701377868652344
Agent0_Train_MaxReturn : -0.5998268127441406
Agent0_Train_MinReturn : -91.1838150024414
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 99000
Agent0_TimeSinceStart : 327.63458132743835
Agent0_Critic_Loss : 1.6130037307739258
Agent0_Actor_Loss : -3.801240921020508
Agent0_Alpha_Loss : 4.893662929534912
Agent0_Temperature : 0.4902091308132572
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -45.275882720947266
Agent1_Eval_StdReturn : 34.97979736328125
Agent1_Eval_MaxReturn : 12.536977767944336
Agent1_Eval_MinReturn : -85.96478271484375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -35.766136169433594
Agent1_Train_StdReturn : 34.681705474853516
Agent1_Train_MaxReturn : 28.006351470947266
Agent1_Train_MinReturn : -82.10392761230469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 99000
Agent1_TimeSinceStart : 330.292484998703
Agent1_Critic_Loss : 1.8869311809539795
Agent1_Actor_Loss : -4.092001438140869
Agent1_Alpha_Loss : 4.8861470222473145
Agent1_Temperature : 0.4902111660506062
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -54.89202880859375
Agent0_Eval_StdReturn : 46.97794723510742
Agent0_Eval_MaxReturn : 37.641265869140625
Agent0_Eval_MinReturn : -116.06414794921875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -42.502830505371094
Agent0_Train_StdReturn : 28.940168380737305
Agent0_Train_MaxReturn : 1.6704730987548828
Agent0_Train_MinReturn : -91.7480239868164
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 100500
Agent0_TimeSinceStart : 332.9617328643799
Agent0_Critic_Loss : 1.7240519523620605
Agent0_Actor_Loss : -3.800222396850586
Agent0_Alpha_Loss : 4.890984535217285
Agent0_Temperature : 0.49006283048067717
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -41.63915252685547
Agent1_Eval_StdReturn : 24.606678009033203
Agent1_Eval_MaxReturn : 13.168611526489258
Agent1_Eval_MinReturn : -71.81512451171875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -50.39778518676758
Agent1_Train_StdReturn : 27.949464797973633
Agent1_Train_MaxReturn : -12.38994026184082
Agent1_Train_MinReturn : -103.4409408569336
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 100500
Agent1_TimeSinceStart : 335.6277618408203
Agent1_Critic_Loss : 2.0047600269317627
Agent1_Actor_Loss : -3.99265718460083
Agent1_Alpha_Loss : 4.896571159362793
Agent1_Temperature : 0.4900649025988612
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -36.866214752197266
Agent0_Eval_StdReturn : 34.953189849853516
Agent0_Eval_MaxReturn : 9.028059005737305
Agent0_Eval_MinReturn : -94.84031677246094
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -46.231937408447266
Agent0_Train_StdReturn : 28.30548095703125
Agent0_Train_MaxReturn : 15.535266876220703
Agent0_Train_MinReturn : -87.4531021118164
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 102000
Agent0_TimeSinceStart : 338.29962944984436
Agent0_Critic_Loss : 1.718463659286499
Agent0_Actor_Loss : -3.848186492919922
Agent0_Alpha_Loss : 4.912929534912109
Agent0_Temperature : 0.48991657345742323
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -54.33305740356445
Agent1_Eval_StdReturn : 32.50122833251953
Agent1_Eval_MaxReturn : 0.2510528564453125
Agent1_Eval_MinReturn : -107.56735229492188
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.362037658691406
Agent1_Train_StdReturn : 26.335763931274414
Agent1_Train_MaxReturn : -14.540891647338867
Agent1_Train_MinReturn : -88.29920959472656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 102000
Agent1_TimeSinceStart : 340.95882654190063
Agent1_Eval_StdReturn : 24.35211181640625
Agent1_Eval_MaxReturn : 11.020330429077148
Agent1_Eval_MinReturn : -54.487274169921875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -31.892925262451172
Agent1_Train_StdReturn : 27.54820442199707
Agent1_Train_MaxReturn : 14.50973892211914
Agent1_Train_MinReturn : -62.975318908691406
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 88500
Agent1_TimeSinceStart : 301.11115980148315
Agent1_Critic_Loss : 0.6302261352539062
Agent1_Actor_Loss : -0.6058294773101807
Agent1_Alpha_Loss : 0.9793769121170044
Agent1_Temperature : 0.09824860562734121
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -34.99679183959961
Agent0_Eval_StdReturn : 41.65092086791992
Agent0_Eval_MaxReturn : 38.994873046875
Agent0_Eval_MinReturn : -94.3120346069336
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -47.172489166259766
Agent0_Train_StdReturn : 32.49749755859375
Agent0_Train_MaxReturn : -13.465686798095703
Agent0_Train_MinReturn : -99.6753158569336
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 90000
Agent0_TimeSinceStart : 303.76961493492126
Agent0_Critic_Loss : 0.7635342478752136
Agent0_Actor_Loss : -0.4735555052757263
Agent0_Alpha_Loss : 0.9802738428115845
Agent0_Temperature : 0.09821815455317097
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -41.428245544433594
Agent1_Eval_StdReturn : 30.96263313293457
Agent1_Eval_MaxReturn : 20.586618423461914
Agent1_Eval_MinReturn : -82.14124298095703
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -28.641765594482422
Agent1_Train_StdReturn : 30.944929122924805
Agent1_Train_MaxReturn : 27.61991310119629
Agent1_Train_MinReturn : -71.401611328125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 90000
Agent1_TimeSinceStart : 306.4347434043884
Agent1_Critic_Loss : 0.6253376007080078
Agent1_Actor_Loss : -0.591113805770874
Agent1_Alpha_Loss : 0.9724702835083008
Agent1_Temperature : 0.09821933141241382
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -35.43810272216797
Agent0_Eval_StdReturn : 28.25821876525879
Agent0_Eval_MaxReturn : 17.269561767578125
Agent0_Eval_MinReturn : -82.63243865966797
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.33186340332031
Agent0_Train_StdReturn : 41.8337516784668
Agent0_Train_MaxReturn : 12.837090492248535
Agent0_Train_MinReturn : -126.9164047241211
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 91500
Agent0_TimeSinceStart : 309.092520236969
Agent0_Critic_Loss : 0.5895416140556335
Agent0_Actor_Loss : -0.46267783641815186
Agent0_Alpha_Loss : 0.9714757800102234
Agent0_Temperature : 0.09818885650606402
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -54.3508415222168
Agent1_Eval_StdReturn : 26.618545532226562
Agent1_Eval_MaxReturn : -20.342906951904297
Agent1_Eval_MinReturn : -94.53730773925781
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -28.570343017578125
Agent1_Train_StdReturn : 24.69554901123047
Agent1_Train_MaxReturn : 7.795436859130859
Agent1_Train_MinReturn : -79.03033447265625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 91500
Agent1_TimeSinceStart : 311.75473952293396
Agent1_Critic_Loss : 0.5809615850448608
Agent1_Actor_Loss : -0.5882190465927124
Agent1_Alpha_Loss : 0.9661771059036255
Agent1_Temperature : 0.09819009677131038
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.99658966064453
Agent0_Eval_StdReturn : 32.615264892578125
Agent0_Eval_MaxReturn : -3.998563766479492
Agent0_Eval_MinReturn : -129.5914306640625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.2403450012207
Agent0_Train_StdReturn : 34.46637725830078
Agent0_Train_MaxReturn : 15.183862686157227
Agent0_Train_MinReturn : -96.82620239257812
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 93000
Agent0_TimeSinceStart : 314.426983833313
Agent0_Critic_Loss : 0.5773106813430786
Agent0_Actor_Loss : -0.4779178500175476
Agent0_Alpha_Loss : 0.9781142473220825
Agent0_Temperature : 0.09815957284005311
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -25.692066192626953
Agent1_Eval_StdReturn : 28.497371673583984
Agent1_Eval_MaxReturn : 35.18939208984375
Agent1_Eval_MinReturn : -65.80235290527344
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -47.739463806152344
Agent1_Train_StdReturn : 47.282230377197266
Agent1_Train_MaxReturn : 9.828606605529785
Agent1_Train_MinReturn : -125.01478576660156
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 93000
Agent1_TimeSinceStart : 317.0996718406677
Agent1_Critic_Loss : 0.5268328785896301
Agent1_Actor_Loss : -0.5665503144264221
Agent1_Alpha_Loss : 0.9839881658554077
Agent1_Temperature : 0.09816085331420425
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -45.25416946411133
Agent0_Eval_StdReturn : 17.82811164855957
Agent0_Eval_MaxReturn : -17.712543487548828
Agent0_Eval_MinReturn : -71.26042175292969
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -50.856414794921875
Agent0_Train_StdReturn : 32.868125915527344
Agent0_Train_MaxReturn : -3.959047317504883
Agent0_Train_MinReturn : -103.40086364746094
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 94500
Agent0_TimeSinceStart : 319.77502059936523
Agent0_Critic_Loss : 0.5923678278923035
Agent0_Actor_Loss : -0.3836771547794342
Agent0_Alpha_Loss : 0.9597541093826294
Agent0_Temperature : 0.09813034879903502
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -27.099979400634766
Agent1_Eval_StdReturn : 23.84467124938965
Agent1_Eval_MaxReturn : 3.2954959869384766
Agent1_Eval_MinReturn : -70.57489776611328
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.598876953125
Agent1_Train_StdReturn : 36.77858352661133
Agent1_Train_MaxReturn : 18.807697296142578
Agent1_Train_MinReturn : -90.99468994140625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 94500
Agent1_TimeSinceStart : 322.4323310852051
Agent1_Critic_Loss : 0.6151413917541504
Agent1_Actor_Loss : -0.5500917434692383
Agent1_Alpha_Loss : 0.9681272506713867
Agent1_Temperature : 0.0981316425430843
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.60110855102539
Agent0_Eval_StdReturn : 35.79277038574219
Agent1_Critic_Loss : 1.724648118019104
Agent1_Actor_Loss : -4.076355934143066
Agent1_Alpha_Loss : 4.943227767944336
Agent1_Temperature : 0.4899185956795734
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -41.951839447021484
Agent0_Eval_StdReturn : 33.54251480102539
Agent0_Eval_MaxReturn : 14.789159774780273
Agent0_Eval_MinReturn : -102.41264343261719
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -48.93510818481445
Agent0_Train_StdReturn : 41.87514114379883
Agent0_Train_MaxReturn : 4.955936431884766
Agent0_Train_MinReturn : -115.29167175292969
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 103500
Agent0_TimeSinceStart : 343.6232032775879
Agent0_Critic_Loss : 1.6910333633422852
Agent0_Actor_Loss : -3.832036256790161
Agent0_Alpha_Loss : 4.92251443862915
Agent0_Temperature : 0.48977033454336394
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -31.223602294921875
Agent1_Eval_StdReturn : 29.864316940307617
Agent1_Eval_MaxReturn : 18.057416915893555
Agent1_Eval_MinReturn : -76.6173324584961
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -57.474639892578125
Agent1_Train_StdReturn : 32.691776275634766
Agent1_Train_MaxReturn : 31.056116104125977
Agent1_Train_MinReturn : -90.73892211914062
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 103500
Agent1_TimeSinceStart : 346.2930552959442
Agent1_Critic_Loss : 1.8778914213180542
Agent1_Actor_Loss : -4.012215614318848
Agent1_Alpha_Loss : 4.909424781799316
Agent1_Temperature : 0.48977233985629964
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -29.681278228759766
Agent0_Eval_StdReturn : 22.692392349243164
Agent0_Eval_MaxReturn : 7.839428424835205
Agent0_Eval_MinReturn : -61.581268310546875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -57.47046661376953
Agent0_Train_StdReturn : 25.409852981567383
Agent0_Train_MaxReturn : -20.47056770324707
Agent0_Train_MinReturn : -112.12552642822266
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 105000
Agent0_TimeSinceStart : 348.96776580810547
Agent0_Critic_Loss : 1.8774868249893188
Agent0_Actor_Loss : -3.8501811027526855
Agent0_Alpha_Loss : 4.94354248046875
Agent0_Temperature : 0.4896240623315781
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -47.1319580078125
Agent1_Eval_StdReturn : 25.792814254760742
Agent1_Eval_MaxReturn : -9.92281436920166
Agent1_Eval_MinReturn : -85.09928894042969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -61.61395263671875
Agent1_Train_StdReturn : 20.73448371887207
Agent1_Train_MaxReturn : -30.579200744628906
Agent1_Train_MinReturn : -100.3666763305664
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 105000
Agent1_TimeSinceStart : 351.63477754592896
Agent1_Critic_Loss : 1.9336051940917969
Agent1_Actor_Loss : -4.017819404602051
Agent1_Alpha_Loss : 4.93439245223999
Agent1_Temperature : 0.4896260699994387
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -47.91270065307617
Agent0_Eval_StdReturn : 24.769376754760742
Agent0_Eval_MaxReturn : -3.7153968811035156
Agent0_Eval_MinReturn : -83.17365264892578
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.807703018188477
Agent0_Train_StdReturn : 28.372486114501953
Agent0_Train_MaxReturn : 17.369831085205078
Agent0_Train_MinReturn : -64.50523376464844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 106500
Agent0_TimeSinceStart : 354.30944561958313
Agent0_Critic_Loss : 1.9292933940887451
Agent0_Actor_Loss : -3.826956272125244
Agent0_Alpha_Loss : 4.8860321044921875
Agent0_Temperature : 0.4894779109774523
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -51.0579948425293
Agent1_Eval_StdReturn : 23.792585372924805
Agent1_Eval_MaxReturn : -7.945789337158203
Agent1_Eval_MinReturn : -84.28163146972656
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -29.841039657592773
Agent1_Train_StdReturn : 28.772872924804688
Agent1_Train_MaxReturn : 7.973234176635742
Agent1_Train_MinReturn : -74.18285369873047
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 106500
Agent1_TimeSinceStart : 356.9716022014618
Agent1_Critic_Loss : 1.7973825931549072
Agent1_Actor_Loss : -3.940469741821289
Agent1_Alpha_Loss : 4.9474334716796875
Agent1_Temperature : 0.4894797586117753
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -38.151309967041016
Agent0_Eval_StdReturn : 36.4110221862793
Agent0_Eval_MaxReturn : 28.142303466796875
Agent0_Eval_MinReturn : -110.18106842041016
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.873878479003906
Agent0_Train_StdReturn : 48.485809326171875
Agent0_Train_MaxReturn : 32.59564971923828
Agent0_Train_MinReturn : -106.7938232421875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 108000
Agent0_TimeSinceStart : 359.643923997879
Agent0_Critic_Loss : 1.9489731788635254
Agent0_Actor_Loss : -3.8174681663513184
Agent0_Alpha_Loss : 4.91378116607666
Agent0_Temperature : 0.4893318001833447
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -45.47039031982422
Agent1_Eval_StdReturn : 20.578292846679688
Agent1_Eval_MaxReturn : -8.307633399963379
Agent1_Eval_MinReturn : -74.47303771972656
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -37.66057205200195
Agent1_Train_StdReturn : 30.025089263916016
Agent1_Train_MaxReturn : 46.00120162963867
Agent1_Train_MinReturn : -66.3529052734375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 108000
Agent1_TimeSinceStart : 362.320458650589
Agent1_Critic_Loss : 1.4687566757202148
Agent1_Actor_Loss : -4.04879093170166
Agent1_Alpha_Loss : 4.906222343444824
Agent1_Temperature : 0.4893335195835663
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.187782287597656
Agent0_Eval_StdReturn : 33.058387756347656
Agent0_Eval_MaxReturn : 6.089773178100586
Agent0_Eval_MinReturn : -114.36333465576172
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -50.62272262573242
Agent0_Train_StdReturn : 27.469648361206055
Agent0_Train_MaxReturn : -14.34157943725586
Agent0_Train_MinReturn : -97.47917175292969
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 109500
Agent0_TimeSinceStart : 364.98828887939453
Agent0_Critic_Loss : 1.7506812810897827
Agent0_Actor_Loss : -4.0368242263793945
Agent0_Eval_MaxReturn : 16.534936904907227
Agent0_Eval_MinReturn : -100.61528778076172
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -42.304786682128906
Agent0_Train_StdReturn : 27.415550231933594
Agent0_Train_MaxReturn : -9.707525253295898
Agent0_Train_MinReturn : -94.63246154785156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 96000
Agent0_TimeSinceStart : 325.0997006893158
Agent0_Critic_Loss : 0.6742565631866455
Agent0_Actor_Loss : -0.4162498712539673
Agent0_Alpha_Loss : 0.9728021025657654
Agent0_Temperature : 0.09810114548697148
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.830032348632812
Agent1_Eval_StdReturn : 21.356563568115234
Agent1_Eval_MaxReturn : 2.300067901611328
Agent1_Eval_MinReturn : -64.3079833984375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -22.84015464782715
Agent1_Train_StdReturn : 26.794591903686523
Agent1_Train_MaxReturn : 9.160209655761719
Agent1_Train_MinReturn : -85.82186889648438
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 96000
Agent1_TimeSinceStart : 327.7765245437622
Agent1_Critic_Loss : 0.612669825553894
Agent1_Actor_Loss : -0.5858088731765747
Agent1_Alpha_Loss : 0.9704984426498413
Agent1_Temperature : 0.09810245547145764
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.468223571777344
Agent0_Eval_StdReturn : 37.303855895996094
Agent0_Eval_MaxReturn : 36.06819152832031
Agent0_Eval_MinReturn : -102.70060729980469
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -35.28144073486328
Agent0_Train_StdReturn : 23.671951293945312
Agent0_Train_MaxReturn : 0.6085596084594727
Agent0_Train_MinReturn : -67.9231185913086
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 97500
Agent0_TimeSinceStart : 330.4612138271332
Agent0_Critic_Loss : 0.6392619609832764
Agent0_Actor_Loss : -0.43869125843048096
Agent0_Alpha_Loss : 0.975383460521698
Agent0_Temperature : 0.09807195473141467
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -42.87728500366211
Agent1_Eval_StdReturn : 32.92969512939453
Agent1_Eval_MaxReturn : 13.5536470413208
Agent1_Eval_MinReturn : -102.23307800292969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -45.8784065246582
Agent1_Train_StdReturn : 46.38227081298828
Agent1_Train_MaxReturn : 5.650411605834961
Agent1_Train_MinReturn : -129.16717529296875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 97500
Agent1_TimeSinceStart : 333.139769077301
Agent1_Critic_Loss : 0.6606443524360657
Agent1_Actor_Loss : -0.515739917755127
Agent1_Alpha_Loss : 0.9593783617019653
Agent1_Temperature : 0.09807331821042525
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -38.07372283935547
Agent0_Eval_StdReturn : 35.37553405761719
Agent0_Eval_MaxReturn : 22.70573616027832
Agent0_Eval_MinReturn : -96.80654907226562
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -32.22836685180664
Agent0_Train_StdReturn : 33.653480529785156
Agent0_Train_MaxReturn : 37.82988739013672
Agent0_Train_MinReturn : -74.65618896484375
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 99000
Agent0_TimeSinceStart : 335.8241925239563
Agent0_Critic_Loss : 0.5875464677810669
Agent0_Actor_Loss : -0.46662437915802
Agent0_Alpha_Loss : 0.9681609869003296
Agent0_Temperature : 0.09804279406325414
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -40.321815490722656
Agent1_Eval_StdReturn : 37.96345138549805
Agent1_Eval_MaxReturn : 34.89079284667969
Agent1_Eval_MinReturn : -101.45855712890625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -28.030597686767578
Agent1_Train_StdReturn : 31.98188591003418
Agent1_Train_MaxReturn : 46.01447296142578
Agent1_Train_MinReturn : -74.0857162475586
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 99000
Agent1_TimeSinceStart : 338.4988958835602
Agent1_Critic_Loss : 0.5992273092269897
Agent1_Actor_Loss : -0.5912624001502991
Agent1_Alpha_Loss : 0.9679995775222778
Agent1_Temperature : 0.09804420390657398
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.64038848876953
Agent0_Eval_StdReturn : 42.789310455322266
Agent0_Eval_MaxReturn : 17.284381866455078
Agent0_Eval_MinReturn : -122.0291748046875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -56.351470947265625
Agent0_Train_StdReturn : 32.482826232910156
Agent0_Train_MaxReturn : -2.947071075439453
Agent0_Train_MinReturn : -122.04788208007812
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 100500
Agent0_TimeSinceStart : 341.1776053905487
Agent0_Critic_Loss : 0.6739203929901123
Agent0_Actor_Loss : -0.44401460886001587
Agent0_Alpha_Loss : 0.9689699411392212
Agent0_Temperature : 0.09801365869898108
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -47.8343620300293
Agent1_Eval_StdReturn : 26.363338470458984
Agent1_Eval_MaxReturn : -13.517950057983398
Agent1_Eval_MinReturn : -93.05778503417969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -32.2314338684082
Agent1_Train_StdReturn : 32.56869888305664
Agent1_Train_MaxReturn : 22.07733726501465
Agent1_Train_MinReturn : -81.17642974853516
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 100500
Agent1_TimeSinceStart : 343.8528664112091
Agent1_Critic_Loss : 0.6019673943519592
Agent1_Actor_Loss : -0.5842863917350769
Agent1_Alpha_Loss : 0.9658538699150085
Agent1_Temperature : 0.0980151160196646
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -22.510889053344727
Agent0_Eval_StdReturn : 28.07352638244629
Agent0_Eval_MaxReturn : 3.15786075592041
Agent0_Eval_MinReturn : -86.20074462890625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.805503845214844
Agent0_Train_StdReturn : 33.32005310058594
Agent0_Train_MaxReturn : 31.023948669433594
Agent0_Train_MinReturn : -87.70423889160156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 102000
Agent0_TimeSinceStart : 346.53341484069824
Agent0_Critic_Loss : 0.524465799331665
Agent0_Actor_Loss : -0.5121303796768188
Agent0_Alpha_Loss : 0.9720110893249512
Agent0_Temperature : 0.09798453869274105
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -19.388469696044922
Agent1_Eval_StdReturn : 34.499507904052734
Agent1_Eval_MaxReturn : 40.28694152832031

Agent0_Alpha_Loss : 4.924961090087891
Agent0_Temperature : 0.48918570086435903
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -35.88896942138672
Agent1_Eval_StdReturn : 30.396268844604492
Agent1_Eval_MaxReturn : 6.473733425140381
Agent1_Eval_MinReturn : -109.34697723388672
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -55.7252082824707
Agent1_Train_StdReturn : 24.322242736816406
Agent1_Train_MaxReturn : -8.82191276550293
Agent1_Train_MinReturn : -95.49966430664062
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 109500
Agent1_TimeSinceStart : 367.6980175971985
Agent1_Critic_Loss : 1.820309042930603
Agent1_Actor_Loss : -4.070054054260254
Agent1_Alpha_Loss : 4.927223205566406
Agent1_Temperature : 0.48918729534460664
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -45.267372131347656
Agent0_Eval_StdReturn : 29.87009048461914
Agent0_Eval_MaxReturn : 12.831411361694336
Agent0_Eval_MinReturn : -80.77799224853516
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.01746368408203
Agent0_Train_StdReturn : 38.762657165527344
Agent0_Train_MaxReturn : 31.7242374420166
Agent0_Train_MinReturn : -82.07391357421875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 111000
Agent0_TimeSinceStart : 370.463502407074
Agent0_Critic_Loss : 1.8406705856323242
Agent0_Actor_Loss : -3.9446983337402344
Agent0_Alpha_Loss : 4.896215438842773
Agent0_Temperature : 0.4890396893566892
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -49.41260528564453
Agent1_Eval_StdReturn : 26.66508674621582
Agent1_Eval_MaxReturn : 0.2469482421875
Agent1_Eval_MinReturn : -90.13434600830078
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -35.57587432861328
Agent1_Train_StdReturn : 25.07282066345215
Agent1_Train_MaxReturn : 0.2663912773132324
Agent1_Train_MinReturn : -72.2384033203125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 111000
Agent1_TimeSinceStart : 373.18850898742676
Agent1_Critic_Loss : 1.782336950302124
Agent1_Actor_Loss : -3.974266767501831
Agent1_Alpha_Loss : 4.902263641357422
Agent1_Temperature : 0.4890411523277157
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -36.65879440307617
Agent0_Eval_StdReturn : 26.238487243652344
Agent0_Eval_MaxReturn : 12.134892463684082
Agent0_Eval_MinReturn : -86.72610473632812
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -33.56892013549805
Agent0_Train_StdReturn : 23.48848533630371
Agent0_Train_MaxReturn : 3.719864845275879
Agent0_Train_MinReturn : -77.00426483154297
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 112500
Agent0_TimeSinceStart : 375.904767036438
Agent0_Critic_Loss : 2.090620994567871
Agent0_Actor_Loss : -3.9963860511779785
Agent0_Alpha_Loss : 4.900886535644531
Agent0_Temperature : 0.488893747885918
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.776634216308594
Agent1_Eval_StdReturn : 27.91350746154785
Agent1_Eval_MaxReturn : 20.49146270751953
Agent1_Eval_MinReturn : -88.1207504272461
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -27.13167953491211
Agent1_Train_StdReturn : 23.643766403198242
Agent1_Train_MaxReturn : 6.390312671661377
Agent1_Train_MinReturn : -74.71162414550781
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 112500
Agent1_TimeSinceStart : 378.6414406299591
Agent1_Critic_Loss : 1.862945318222046
Agent1_Actor_Loss : -3.952619791030884
Agent1_Alpha_Loss : 4.893573760986328
Agent1_Temperature : 0.48889510793196356
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.935298919677734
Agent0_Eval_StdReturn : 20.903379440307617
Agent0_Eval_MaxReturn : -6.280534744262695
Agent0_Eval_MinReturn : -65.1283187866211
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -49.56915283203125
Agent0_Train_StdReturn : 41.527809143066406
Agent0_Train_MaxReturn : 2.799121856689453
Agent0_Train_MinReturn : -134.12574768066406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 114000
Agent0_TimeSinceStart : 381.3728907108307
Agent0_Critic_Loss : 1.8956153392791748
Agent0_Actor_Loss : -4.023871898651123
Agent0_Alpha_Loss : 4.881011009216309
Agent0_Temperature : 0.48874792370971715
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -36.25650405883789
Agent1_Eval_StdReturn : 29.6994571685791
Agent1_Eval_MaxReturn : 4.4839982986450195
Agent1_Eval_MinReturn : -85.4367904663086
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -35.7418098449707
Agent1_Train_StdReturn : 24.40693473815918
Agent1_Train_MaxReturn : 9.817742347717285
Agent1_Train_MinReturn : -70.6094970703125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 114000
Agent1_TimeSinceStart : 384.0797953605652
Agent1_Critic_Loss : 1.8232250213623047
Agent1_Actor_Loss : -4.014257431030273
Agent1_Alpha_Loss : 4.878793716430664
Agent1_Temperature : 0.48874919327811245
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -47.08143997192383
Agent0_Eval_StdReturn : 29.842304229736328
Agent0_Eval_MaxReturn : -5.899096488952637
Agent0_Eval_MinReturn : -109.34638977050781
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -23.809085845947266
Agent0_Train_StdReturn : 34.61033630371094
Agent0_Train_MaxReturn : 46.06792449951172
Agent0_Train_MinReturn : -68.67118072509766
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 115500
Agent0_TimeSinceStart : 386.78030800819397
Agent0_Critic_Loss : 1.8468034267425537
Agent0_Actor_Loss : -4.0130438804626465
Agent0_Alpha_Loss : 4.90011739730835
Agent0_Temperature : 0.4886021584668564
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -36.76430892944336
Agent1_Eval_StdReturn : 28.39906120300293
Agent1_Eval_MaxReturn : 17.207975387573242
Agent1_Eval_MinReturn : -88.82076263427734
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -38.80607986450195
Agent1_Train_StdReturn : 32.27194595336914
Agent1_Train_MaxReturn : 5.890310287475586
Agent1_Train_MinReturn : -83.57730102539062
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 115500
Agent1_TimeSinceStart : 389.47765493392944
Agent1_Critic_Loss : 1.643601417541504
Agent1_Actor_Loss : -3.972902536392212
Agent1_Alpha_Loss : 4.88134765625
Agent1_Temperature : 0.48860339135862246Agent1_Eval_MinReturn : -65.90595245361328
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -33.14281463623047
Agent1_Train_StdReturn : 34.086978912353516
Agent1_Train_MaxReturn : 29.13959503173828
Agent1_Train_MinReturn : -92.1943588256836
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 102000
Agent1_TimeSinceStart : 349.2670245170593
Agent1_Critic_Loss : 0.584758996963501
Agent1_Actor_Loss : -0.6027820706367493
Agent1_Alpha_Loss : 0.9771741032600403
Agent1_Temperature : 0.09798602333754992
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -27.814767837524414
Agent0_Eval_StdReturn : 18.45758628845215
Agent0_Eval_MaxReturn : 1.99665105342865
Agent0_Eval_MinReturn : -61.768821716308594
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.024131774902344
Agent0_Train_StdReturn : 24.844955444335938
Agent0_Train_MaxReturn : 2.136056900024414
Agent0_Train_MinReturn : -83.86627960205078
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 103500
Agent0_TimeSinceStart : 352.0289616584778
Agent0_Critic_Loss : 0.5903111696243286
Agent0_Actor_Loss : -0.4751891493797302
Agent0_Alpha_Loss : 0.9682276248931885
Agent0_Temperature : 0.09795544258606363
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -21.217845916748047
Agent1_Eval_StdReturn : 29.068082809448242
Agent1_Eval_MaxReturn : 30.528331756591797
Agent1_Eval_MinReturn : -72.77071380615234
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -47.80753707885742
Agent1_Train_StdReturn : 31.535673141479492
Agent1_Train_MaxReturn : 10.244451522827148
Agent1_Train_MinReturn : -107.53329467773438
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 103500
Agent1_TimeSinceStart : 354.7687840461731
Agent1_Critic_Loss : 0.6457509994506836
Agent1_Actor_Loss : -0.6266014575958252
Agent1_Alpha_Loss : 0.9735358953475952
Agent1_Temperature : 0.09795693624926811
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -13.140579223632812
Agent0_Eval_StdReturn : 20.937864303588867
Agent0_Eval_MaxReturn : 25.342819213867188
Agent0_Eval_MinReturn : -51.141868591308594
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -61.297584533691406
Agent0_Train_StdReturn : 37.746070861816406
Agent0_Train_MaxReturn : -1.5781631469726562
Agent0_Train_MinReturn : -155.16571044921875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 105000
Agent0_TimeSinceStart : 357.5021765232086
Agent0_Critic_Loss : 0.6520518064498901
Agent0_Actor_Loss : -0.47824323177337646
Agent0_Alpha_Loss : 0.9758855700492859
Agent0_Temperature : 0.09792634874881116
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -35.920127868652344
Agent1_Eval_StdReturn : 24.531879425048828
Agent1_Eval_MaxReturn : 5.829751968383789
Agent1_Eval_MinReturn : -93.1507568359375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -39.149749755859375
Agent1_Train_StdReturn : 24.83491325378418
Agent1_Train_MaxReturn : -0.6943230628967285
Agent1_Train_MinReturn : -90.29920196533203
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 105000
Agent1_TimeSinceStart : 360.2454686164856
Agent1_Critic_Loss : 0.5706294775009155
Agent1_Actor_Loss : -0.6471538543701172
Agent1_Alpha_Loss : 0.9700140953063965
Agent1_Temperature : 0.09792786371642441
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.499412536621094
Agent0_Eval_StdReturn : 30.80374526977539
Agent0_Eval_MaxReturn : 9.232784271240234
Agent0_Eval_MinReturn : -86.2989273071289
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -27.519229888916016
Agent0_Train_StdReturn : 38.997230529785156
Agent0_Train_MaxReturn : 46.77618408203125
Agent0_Train_MinReturn : -77.92044067382812
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 106500
Agent0_TimeSinceStart : 362.9891707897186
Agent0_Critic_Loss : 0.6806211471557617
Agent0_Actor_Loss : -0.4377500116825104
Agent0_Alpha_Loss : 0.9645776748657227
Agent0_Temperature : 0.09789728648627298
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -46.49864196777344
Agent1_Eval_StdReturn : 35.503849029541016
Agent1_Eval_MaxReturn : 1.1654167175292969
Agent1_Eval_MinReturn : -93.45645141601562
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -32.28722381591797
Agent1_Train_StdReturn : 24.281118392944336
Agent1_Train_MaxReturn : 16.4534854888916
Agent1_Train_MinReturn : -76.58723449707031
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 106500
Agent1_TimeSinceStart : 365.7157242298126
Agent1_Critic_Loss : 0.50568687915802
Agent1_Actor_Loss : -0.5937519073486328
Agent1_Alpha_Loss : 0.9748713970184326
Agent1_Temperature : 0.09789879225536167
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.838947296142578
Agent0_Eval_StdReturn : 28.013322830200195
Agent0_Eval_MaxReturn : 13.507493019104004
Agent0_Eval_MinReturn : -71.20001983642578
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.46915054321289
Agent0_Train_StdReturn : 37.688140869140625
Agent0_Train_MaxReturn : 7.149610996246338
Agent0_Train_MinReturn : -118.17865753173828
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 108000
Agent0_TimeSinceStart : 368.43286085128784
Agent0_Critic_Loss : 0.5030659437179565
Agent0_Actor_Loss : -0.4098897874355316
Agent0_Alpha_Loss : 0.9658328890800476
Agent0_Temperature : 0.09786824964232722
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -50.503971099853516
Agent1_Eval_StdReturn : 28.33392906188965
Agent1_Eval_MaxReturn : -13.641633033752441
Agent1_Eval_MinReturn : -106.1507568359375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -35.06592559814453
Agent1_Train_StdReturn : 23.881351470947266
Agent1_Train_MaxReturn : 5.463536262512207
Agent1_Train_MinReturn : -70.03404998779297
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 108000
Agent1_TimeSinceStart : 371.15626430511475
Agent1_Critic_Loss : 0.5708179473876953
Agent1_Actor_Loss : -0.6130071878433228
Agent1_Alpha_Loss : 0.9619988203048706
Agent1_Temperature : 0.09786975539648174
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -50.67598342895508
Agent0_Eval_StdReturn : 30.13433265686035
Agent0_Eval_MaxReturn : -18.310022354125977
Agent0_Eval_MinReturn : -118.75672912597656
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -25.012348175048828
Agent0_Eval_StdReturn : 27.29018211364746
Agent0_Eval_MaxReturn : 15.305068969726562
Agent0_Eval_MinReturn : -73.10404205322266
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.185089111328125
Agent0_Train_StdReturn : 36.940650939941406
Agent0_Train_MaxReturn : 13.22249984741211
Agent0_Train_MinReturn : -90.3023910522461
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 117000
Agent0_TimeSinceStart : 392.1891827583313
Agent0_Critic_Loss : 1.9214955568313599
Agent0_Actor_Loss : -4.06855583190918
Agent0_Alpha_Loss : 4.896661281585693
Agent0_Temperature : 0.4884564583802715
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -54.6300163269043
Agent1_Eval_StdReturn : 33.09880065917969
Agent1_Eval_MaxReturn : -13.043601989746094
Agent1_Eval_MinReturn : -103.04420471191406
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -33.92924118041992
Agent1_Train_StdReturn : 34.442169189453125
Agent1_Train_MaxReturn : 58.6237678527832
Agent1_Train_MinReturn : -66.55119323730469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 117000
Agent1_TimeSinceStart : 394.89874029159546
Agent1_Critic_Loss : 1.7110857963562012
Agent1_Actor_Loss : -4.1133952140808105
Agent1_Alpha_Loss : 4.886068344116211
Agent1_Temperature : 0.4884576813774782
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -54.07448196411133
Agent0_Eval_StdReturn : 30.51317596435547
Agent0_Eval_MaxReturn : 1.1552143096923828
Agent0_Eval_MinReturn : -118.15190124511719
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.98163604736328
Agent0_Train_StdReturn : 42.21275329589844
Agent0_Train_MaxReturn : 12.84875774383545
Agent0_Train_MinReturn : -145.7277069091797
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 118500
Agent0_TimeSinceStart : 397.6088020801544
Agent0_Critic_Loss : 1.6650047302246094
Agent0_Actor_Loss : -4.108745574951172
Agent0_Alpha_Loss : 4.907116413116455
Agent0_Temperature : 0.4883107930529482
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -44.346153259277344
Agent1_Eval_StdReturn : 36.9136962890625
Agent1_Eval_MaxReturn : 26.940841674804688
Agent1_Eval_MinReturn : -102.82767486572266
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -51.465003967285156
Agent1_Train_StdReturn : 38.30213165283203
Agent1_Train_MaxReturn : 1.0825858116149902
Agent1_Train_MinReturn : -132.9632110595703
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 118500
Agent1_TimeSinceStart : 400.3149962425232
Agent1_Critic_Loss : 1.7290549278259277
Agent1_Actor_Loss : -3.9829471111297607
Agent1_Alpha_Loss : 4.874144554138184
Agent1_Temperature : 0.4883120878142462
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -48.81207275390625
Agent0_Eval_StdReturn : 43.266929626464844
Agent0_Eval_MaxReturn : 6.6453680992126465
Agent0_Eval_MinReturn : -105.15547180175781
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.26681900024414
Agent0_Train_StdReturn : 38.439815521240234
Agent0_Train_MaxReturn : 12.943076133728027
Agent0_Train_MinReturn : -101.39651489257812
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 120000
Agent0_TimeSinceStart : 403.0306408405304
Agent0_Critic_Loss : 1.9370185136795044
Agent0_Actor_Loss : -4.082190036773682
Agent0_Alpha_Loss : 4.890590667724609
Agent0_Temperature : 0.48816520523020646
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -46.34510040283203
Agent1_Eval_StdReturn : 26.226375579833984
Agent1_Eval_MaxReturn : -5.208215236663818
Agent1_Eval_MinReturn : -89.75022888183594
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -55.94023513793945
Agent1_Train_StdReturn : 27.764175415039062
Agent1_Train_MaxReturn : -15.199670791625977
Agent1_Train_MinReturn : -99.17724609375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 120000
Agent1_TimeSinceStart : 405.74162769317627
Agent1_Critic_Loss : 1.7124333381652832
Agent1_Actor_Loss : -4.077850341796875
Agent1_Alpha_Loss : 4.872042655944824
Agent1_Temperature : 0.4881666069866393
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -36.86534881591797
Agent0_Eval_StdReturn : 28.601573944091797
Agent0_Eval_MaxReturn : 16.628835678100586
Agent0_Eval_MinReturn : -76.1097412109375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -23.3481502532959
Agent0_Train_StdReturn : 29.413225173950195
Agent0_Train_MaxReturn : 46.34221649169922
Agent0_Train_MinReturn : -60.09374237060547
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 121500
Agent0_TimeSinceStart : 408.4563674926758
Agent0_Critic_Loss : 1.9358704090118408
Agent0_Actor_Loss : -4.085762977600098
Agent0_Alpha_Loss : 4.906844139099121
Agent0_Temperature : 0.48801964802999465
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -43.832008361816406
Agent1_Eval_StdReturn : 30.553804397583008
Agent1_Eval_MaxReturn : -7.194636344909668
Agent1_Eval_MinReturn : -84.46974182128906
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -48.95825958251953
Agent1_Train_StdReturn : 25.96637725830078
Agent1_Train_MaxReturn : -7.631272315979004
Agent1_Train_MinReturn : -84.89265441894531
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 121500
Agent1_TimeSinceStart : 411.1682012081146
Agent1_Critic_Loss : 1.4183601140975952
Agent1_Actor_Loss : -4.175758361816406
Agent1_Alpha_Loss : 4.896141052246094
Agent1_Temperature : 0.48802116760849196
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -56.84320068359375
Agent0_Eval_StdReturn : 27.492557525634766
Agent0_Eval_MaxReturn : -19.86767578125
Agent0_Eval_MinReturn : -108.07763671875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -56.3347282409668
Agent0_Train_StdReturn : 22.1630916595459
Agent0_Train_MaxReturn : -20.48304557800293
Agent0_Train_MinReturn : -95.67237854003906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 123000
Agent0_TimeSinceStart : 413.8849766254425
Agent0_Critic_Loss : 2.0677027702331543
Agent0_Actor_Loss : -4.134800434112549
Agent0_Alpha_Loss : 4.877336025238037
Agent0_Temperature : 0.48787419847646135
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.380523681640625
Agent0_Train_StdReturn : 28.4458065032959
Agent0_Train_MaxReturn : 26.273298263549805
Agent0_Train_MinReturn : -79.7920150756836
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 109500
Agent0_TimeSinceStart : 373.8908565044403
Agent0_Critic_Loss : 0.5756655335426331
Agent0_Actor_Loss : -0.3844909965991974
Agent0_Alpha_Loss : 0.9680339097976685
Agent0_Temperature : 0.09783923029084614
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -8.474302291870117
Agent1_Eval_StdReturn : 27.040727615356445
Agent1_Eval_MaxReturn : 22.60860824584961
Agent1_Eval_MinReturn : -77.7319564819336
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -43.59761428833008
Agent1_Train_StdReturn : 20.69764518737793
Agent1_Train_MaxReturn : 1.896104335784912
Agent1_Train_MinReturn : -69.07638549804688
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 109500
Agent1_TimeSinceStart : 376.6213524341583
Agent1_Critic_Loss : 0.4363989531993866
Agent1_Actor_Loss : -0.6792398691177368
Agent1_Alpha_Loss : 0.9660792350769043
Agent1_Temperature : 0.09784073934791461
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.71731948852539
Agent0_Eval_StdReturn : 37.07933807373047
Agent0_Eval_MaxReturn : 6.158341407775879
Agent0_Eval_MinReturn : -90.83534240722656
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -40.97785568237305
Agent0_Train_StdReturn : 25.38275718688965
Agent0_Train_MaxReturn : -5.084897994995117
Agent0_Train_MinReturn : -79.27871704101562
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 111000
Agent0_TimeSinceStart : 379.35016989707947
Agent0_Critic_Loss : 0.6313425302505493
Agent0_Actor_Loss : -0.3854514956474304
Agent0_Alpha_Loss : 0.9599683284759521
Agent0_Temperature : 0.09781024784701778
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -30.411632537841797
Agent1_Eval_StdReturn : 39.11329650878906
Agent1_Eval_MaxReturn : 42.207481384277344
Agent1_Eval_MinReturn : -81.8486099243164
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -44.48577117919922
Agent1_Train_StdReturn : 23.001684188842773
Agent1_Train_MaxReturn : -20.573270797729492
Agent1_Train_MinReturn : -85.37515258789062
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 111000
Agent1_TimeSinceStart : 382.0818200111389
Agent1_Critic_Loss : 0.5089062452316284
Agent1_Actor_Loss : -0.5434522032737732
Agent1_Alpha_Loss : 0.9658734798431396
Agent1_Temperature : 0.09781174287873479
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -45.058197021484375
Agent0_Eval_StdReturn : 27.252132415771484
Agent0_Eval_MaxReturn : 3.1950016021728516
Agent0_Eval_MinReturn : -85.96646118164062
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -14.072575569152832
Agent0_Train_StdReturn : 19.41531753540039
Agent0_Train_MaxReturn : 15.220958709716797
Agent0_Train_MinReturn : -42.141597747802734
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 112500
Agent0_TimeSinceStart : 384.81557989120483
Agent0_Critic_Loss : 0.5898637771606445
Agent0_Actor_Loss : -0.4074825048446655
Agent0_Alpha_Loss : 0.962313711643219
Agent0_Temperature : 0.09778129267957991
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -24.076454162597656
Agent1_Eval_StdReturn : 29.719266891479492
Agent1_Eval_MaxReturn : 2.9836277961730957
Agent1_Eval_MinReturn : -100.28785705566406
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -10.582745552062988
Agent1_Train_StdReturn : 19.253808975219727
Agent1_Train_MaxReturn : 9.473307609558105
Agent1_Train_MinReturn : -53.899375915527344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 112500
Agent1_TimeSinceStart : 387.5424270629883
Agent1_Critic_Loss : 0.481450617313385
Agent1_Actor_Loss : -0.5775660872459412
Agent1_Alpha_Loss : 0.9616029858589172
Agent1_Temperature : 0.09778277539956072
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -34.998619079589844
Agent0_Eval_StdReturn : 22.87194061279297
Agent0_Eval_MaxReturn : 13.98460865020752
Agent0_Eval_MinReturn : -63.540855407714844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -50.40623474121094
Agent0_Train_StdReturn : 29.224931716918945
Agent0_Train_MaxReturn : -3.436988353729248
Agent0_Train_MinReturn : -92.051513671875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 114000
Agent0_TimeSinceStart : 390.2713840007782
Agent0_Critic_Loss : 0.5214999914169312
Agent0_Actor_Loss : -0.3839840292930603
Agent0_Alpha_Loss : 0.9602178931236267
Agent0_Temperature : 0.0977523677019965
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -20.627288818359375
Agent1_Eval_StdReturn : 29.976205825805664
Agent1_Eval_MaxReturn : 11.965232849121094
Agent1_Eval_MinReturn : -76.16360473632812
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -27.443639755249023
Agent1_Train_StdReturn : 18.441295623779297
Agent1_Train_MaxReturn : -4.6899003982543945
Agent1_Train_MinReturn : -59.17518615722656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 114000
Agent1_TimeSinceStart : 393.01249146461487
Agent1_Critic_Loss : 0.5187784433364868
Agent1_Actor_Loss : -0.60558021068573
Agent1_Alpha_Loss : 0.9416055083274841
Agent1_Temperature : 0.09775388614771266
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -12.328638076782227
Agent0_Eval_StdReturn : 22.134788513183594
Agent0_Eval_MaxReturn : 24.44959259033203
Agent0_Eval_MinReturn : -46.892608642578125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.69633483886719
Agent0_Train_StdReturn : 27.959497451782227
Agent0_Train_MaxReturn : -1.812361717224121
Agent0_Train_MinReturn : -93.95730590820312
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 115500
Agent0_TimeSinceStart : 395.7518877983093
Agent0_Critic_Loss : 0.5434963703155518
Agent0_Actor_Loss : -0.429730087518692
Agent0_Alpha_Loss : 0.964424729347229
Agent0_Temperature : 0.09772345913819085
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.4752254486084
Agent1_Eval_StdReturn : 20.43853759765625
Agent1_Eval_MaxReturn : 7.898062705993652
Agent1_Eval_MinReturn : -63.914764404296875
Agent1_Eval_AverageEpLen : 150.0
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -16.294782638549805
Agent1_Eval_StdReturn : 41.78068542480469
Agent1_Eval_MaxReturn : 54.18373107910156
Agent1_Eval_MinReturn : -106.61534118652344
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -35.61724090576172
Agent1_Train_StdReturn : 40.12514114379883
Agent1_Train_MaxReturn : 23.97901725769043
Agent1_Train_MinReturn : -82.34427642822266
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 123000
Agent1_TimeSinceStart : 416.6034345626831
Agent1_Critic_Loss : 1.6235530376434326
Agent1_Actor_Loss : -4.142545700073242
Agent1_Alpha_Loss : 4.886980056762695
Agent1_Temperature : 0.48787579257434405
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -52.55036544799805
Agent0_Eval_StdReturn : 14.28852367401123
Agent0_Eval_MaxReturn : -29.296314239501953
Agent0_Eval_MinReturn : -75.00335693359375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.64067840576172
Agent0_Train_StdReturn : 43.621097564697266
Agent0_Train_MaxReturn : 37.852237701416016
Agent0_Train_MinReturn : -105.96417999267578
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 124500
Agent0_TimeSinceStart : 419.3355076313019
Agent0_Critic_Loss : 1.7311606407165527
Agent0_Actor_Loss : -4.052177429199219
Agent0_Alpha_Loss : 4.880335807800293
Agent0_Temperature : 0.4877288406406841
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -54.20502471923828
Agent1_Eval_StdReturn : 31.091732025146484
Agent1_Eval_MaxReturn : -11.606571197509766
Agent1_Eval_MinReturn : -110.99243927001953
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -44.462501525878906
Agent1_Train_StdReturn : 33.84550476074219
Agent1_Train_MaxReturn : 2.9972944259643555
Agent1_Train_MinReturn : -96.55250549316406
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 124500
Agent1_TimeSinceStart : 422.05054116249084
Agent1_Critic_Loss : 1.7718579769134521
Agent1_Actor_Loss : -4.210177421569824
Agent1_Alpha_Loss : 4.881546974182129
Agent1_Temperature : 0.4877304925795882
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -29.941082000732422
Agent0_Eval_StdReturn : 25.06524658203125
Agent0_Eval_MaxReturn : 9.871092796325684
Agent0_Eval_MinReturn : -65.86558532714844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.98872756958008
Agent0_Train_StdReturn : 20.392480850219727
Agent0_Train_MaxReturn : -10.665955543518066
Agent0_Train_MinReturn : -67.89683532714844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 126000
Agent0_TimeSinceStart : 424.76535272598267
Agent0_Critic_Loss : 1.6417375802993774
Agent0_Actor_Loss : -4.069928169250488
Agent0_Alpha_Loss : 4.891936779022217
Agent0_Temperature : 0.48758353790824954
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -28.39471435546875
Agent1_Eval_StdReturn : 17.6887264251709
Agent1_Eval_MaxReturn : -0.035241127014160156
Agent1_Eval_MinReturn : -50.8026123046875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.00965881347656
Agent1_Train_StdReturn : 23.086870193481445
Agent1_Train_MaxReturn : -8.663618087768555
Agent1_Train_MinReturn : -90.00056457519531
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 126000
Agent1_TimeSinceStart : 427.48876118659973
Agent1_Critic_Loss : 1.8070831298828125
Agent1_Actor_Loss : -4.150343894958496
Agent1_Alpha_Loss : 4.8690900802612305
Agent1_Temperature : 0.4875852954425098
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -43.38065719604492
Agent0_Eval_StdReturn : 44.62866973876953
Agent0_Eval_MaxReturn : 32.55607604980469
Agent0_Eval_MinReturn : -120.29083251953125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -45.889976501464844
Agent0_Train_StdReturn : 22.647022247314453
Agent0_Train_MaxReturn : -15.950603485107422
Agent0_Train_MinReturn : -93.63301086425781
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 127500
Agent0_TimeSinceStart : 430.21650528907776
Agent0_Critic_Loss : 1.8770012855529785
Agent0_Actor_Loss : -4.048770427703857
Agent0_Alpha_Loss : 4.857418537139893
Agent0_Temperature : 0.48743837792259354
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -24.397024154663086
Agent1_Eval_StdReturn : 38.21141052246094
Agent1_Eval_MaxReturn : 28.103580474853516
Agent1_Eval_MinReturn : -81.12613677978516
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -59.94609832763672
Agent1_Train_StdReturn : 33.31064224243164
Agent1_Train_MaxReturn : -14.084773063659668
Agent1_Train_MinReturn : -134.52572631835938
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 127500
Agent1_TimeSinceStart : 432.9290087223053
Agent1_Critic_Loss : 1.8113887310028076
Agent1_Actor_Loss : -4.252519130706787
Agent1_Alpha_Loss : 4.881108283996582
Agent1_Temperature : 0.48744016204769275
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.976417541503906
Agent0_Eval_StdReturn : 35.89194107055664
Agent0_Eval_MaxReturn : 52.55050277709961
Agent0_Eval_MinReturn : -77.48773193359375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.444604873657227
Agent0_Train_StdReturn : 20.831018447875977
Agent0_Train_MaxReturn : 4.790839195251465
Agent0_Train_MinReturn : -51.180397033691406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 129000
Agent0_TimeSinceStart : 435.6370315551758
Agent0_Critic_Loss : 1.923619031906128
Agent0_Actor_Loss : -4.135842323303223
Agent0_Alpha_Loss : 4.917396545410156
Agent0_Temperature : 0.4872931919812735
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -49.591270446777344
Agent1_Eval_StdReturn : 25.01902961730957
Agent1_Eval_MaxReturn : -5.7119293212890625
Agent1_Eval_MinReturn : -88.59512329101562
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.585391998291016
Agent1_Train_StdReturn : 44.97726058959961
Agent1_Train_MaxReturn : 23.916748046875
Agent1_Train_MinReturn : -116.10917663574219
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 129000
Agent1_TimeSinceStart : 438.35077810287476
Agent1_Critic_Loss : 1.7336909770965576
Agent1_Actor_Loss : -4.174886703491211
Agent1_Alpha_Loss : 4.896817207336426
Agent1_Temperature : 0.4872950479100512
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...
Agent1_Train_AverageReturn : -23.331867218017578
Agent1_Train_StdReturn : 18.699047088623047
Agent1_Train_MaxReturn : 2.9697980880737305
Agent1_Train_MinReturn : -61.305030822753906
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 115500
Agent1_TimeSinceStart : 398.4917747974396
Agent1_Critic_Loss : 0.4613353908061981
Agent1_Actor_Loss : -0.6232298612594604
Agent1_Alpha_Loss : 0.9520425796508789
Agent1_Temperature : 0.09772503976219353
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -28.54042625427246
Agent0_Eval_StdReturn : 19.59354591369629
Agent0_Eval_MaxReturn : -0.8128302097320557
Agent0_Eval_MinReturn : -70.32185363769531
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -37.89405059814453
Agent0_Train_StdReturn : 29.539873123168945
Agent0_Train_MaxReturn : 0.1716461181640625
Agent0_Train_MinReturn : -98.03062438964844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 117000
Agent0_TimeSinceStart : 401.24931931495667
Agent0_Critic_Loss : 0.6545776128768921
Agent0_Actor_Loss : -0.5018224716186523
Agent0_Alpha_Loss : 0.9540908336639404
Agent0_Temperature : 0.09769459253248743
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.48664093017578
Agent1_Eval_StdReturn : 30.542856216430664
Agent1_Eval_MaxReturn : 7.506873607635498
Agent1_Eval_MinReturn : -85.07359313964844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -27.200336456298828
Agent1_Train_StdReturn : 24.202356338500977
Agent1_Train_MaxReturn : 20.87875747680664
Agent1_Train_MinReturn : -59.63916015625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 117000
Agent1_TimeSinceStart : 403.98571848869324
Agent1_Critic_Loss : 0.5238219499588013
Agent1_Actor_Loss : -0.6926277875900269
Agent1_Alpha_Loss : 0.9511952996253967
Agent1_Temperature : 0.09769623412838226
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -57.49797821044922
Agent0_Eval_StdReturn : 27.496788024902344
Agent0_Eval_MaxReturn : -6.923480033874512
Agent0_Eval_MinReturn : -96.53594970703125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -47.316551208496094
Agent0_Train_StdReturn : 23.298307418823242
Agent0_Train_MaxReturn : -10.504572868347168
Agent0_Train_MinReturn : -101.61772918701172
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 118500
Agent0_TimeSinceStart : 406.7214617729187
Agent0_Critic_Loss : 0.5431082248687744
Agent0_Actor_Loss : -0.5363177061080933
Agent0_Alpha_Loss : 0.9489297866821289
Agent0_Temperature : 0.09766577717178254
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -31.86862564086914
Agent1_Eval_StdReturn : 28.130475997924805
Agent1_Eval_MaxReturn : 12.748795509338379
Agent1_Eval_MinReturn : -84.10884094238281
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.87986755371094
Agent1_Train_StdReturn : 28.27227210998535
Agent1_Train_MaxReturn : 3.4246559143066406
Agent1_Train_MinReturn : -92.43744659423828
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 118500
Agent1_TimeSinceStart : 409.4718163013458
Agent1_Critic_Loss : 0.5116401314735413
Agent1_Actor_Loss : -0.6578731536865234
Agent1_Alpha_Loss : 0.9487186670303345
Agent1_Temperature : 0.09766747160061726
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.736671447753906
Agent0_Eval_StdReturn : 24.26848030090332
Agent0_Eval_MaxReturn : 9.089704513549805
Agent0_Eval_MinReturn : -73.98170471191406
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.874835968017578
Agent0_Train_StdReturn : 23.29424476623535
Agent0_Train_MaxReturn : -4.335898399353027
Agent0_Train_MinReturn : -90.3936538696289
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 120000
Agent0_TimeSinceStart : 412.20783519744873
Agent0_Critic_Loss : 0.5384037494659424
Agent0_Actor_Loss : -0.4731862545013428
Agent0_Alpha_Loss : 0.9544785022735596
Agent0_Temperature : 0.09763699330618139
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.631637573242188
Agent1_Eval_StdReturn : 26.568870544433594
Agent1_Eval_MaxReturn : 10.011341094970703
Agent1_Eval_MinReturn : -76.45103454589844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.22319793701172
Agent1_Train_StdReturn : 32.8685188293457
Agent1_Train_MaxReturn : -6.759007453918457
Agent1_Train_MinReturn : -110.99688720703125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 120000
Agent1_TimeSinceStart : 414.9397523403168
Agent1_Critic_Loss : 0.4538031220436096
Agent1_Actor_Loss : -0.6840982437133789
Agent1_Alpha_Loss : 0.9369735717773438
Agent1_Temperature : 0.09763877856857935
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -32.02086639404297
Agent0_Eval_StdReturn : 17.97751235961914
Agent0_Eval_MaxReturn : -5.325610160827637
Agent0_Eval_MinReturn : -69.31488037109375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -48.95055389404297
Agent0_Train_StdReturn : 35.528106689453125
Agent0_Train_MaxReturn : 5.27310848236084
Agent0_Train_MinReturn : -101.77130889892578
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 121500
Agent0_TimeSinceStart : 417.6738679409027
Agent0_Critic_Loss : 0.5196987390518188
Agent0_Actor_Loss : -0.62415611743927
Agent0_Alpha_Loss : 0.9524526596069336
Agent0_Temperature : 0.09760824314917213
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -25.461671829223633
Agent1_Eval_StdReturn : 22.515411376953125
Agent1_Eval_MaxReturn : 5.200689315795898
Agent1_Eval_MinReturn : -73.9047622680664
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -27.492664337158203
Agent1_Train_StdReturn : 27.532081604003906
Agent1_Train_MaxReturn : 9.26667308807373
Agent1_Train_MinReturn : -68.81916046142578
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 121500
Agent1_TimeSinceStart : 420.4176449775696
Agent1_Critic_Loss : 0.5269864797592163
Agent1_Actor_Loss : -0.6335028409957886
Agent1_Alpha_Loss : 0.9560683965682983
Agent1_Temperature : 0.09761009755088794
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -34.98664855957031
Agent0_Eval_StdReturn : 24.603458404541016
Agent0_Eval_MaxReturn : 3.229426860809326
Agent0_Eval_MinReturn : -76.30425262451172
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.676475524902344

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.89858627319336
Agent0_Eval_StdReturn : 34.38841247558594
Agent0_Eval_MaxReturn : 13.685676574707031
Agent0_Eval_MinReturn : -108.86900329589844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -39.234535217285156
Agent0_Train_StdReturn : 26.178001403808594
Agent0_Train_MaxReturn : -11.799484252929688
Agent0_Train_MinReturn : -105.3418960571289
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 130500
Agent0_TimeSinceStart : 441.07939624786377
Agent0_Critic_Loss : 1.7323310375213623
Agent0_Actor_Loss : -4.056844234466553
Agent0_Alpha_Loss : 4.867331027984619
Agent0_Temperature : 0.48714811746094056
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -33.28028106689453
Agent1_Eval_StdReturn : 28.122854232788086
Agent1_Eval_MaxReturn : 3.460139274597168
Agent1_Eval_MinReturn : -100.1440658569336
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -32.59427261352539
Agent1_Train_StdReturn : 37.66793441772461
Agent1_Train_MaxReturn : 52.63124084472656
Agent1_Train_MinReturn : -97.35708618164062
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 130500
Agent1_TimeSinceStart : 443.8109631538391
Agent1_Critic_Loss : 1.771314024925232
Agent1_Actor_Loss : -4.269956588745117
Agent1_Alpha_Loss : 4.882547378540039
Agent1_Temperature : 0.48714999181453483
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -28.524978637695312
Agent0_Eval_StdReturn : 26.420204162597656
Agent0_Eval_MaxReturn : 15.48646068572998
Agent0_Eval_MinReturn : -77.76737976074219
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -42.246849060058594
Agent0_Train_StdReturn : 30.788591384887695
Agent0_Train_MaxReturn : 17.47251319885254
Agent0_Train_MinReturn : -88.38839721679688
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 132000
Agent0_TimeSinceStart : 446.543958902359
Agent0_Critic_Loss : 1.9346885681152344
Agent0_Actor_Loss : -4.115180969238281
Agent0_Alpha_Loss : 4.883159637451172
Agent0_Temperature : 0.4870031042759002
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -45.506736755371094
Agent1_Eval_StdReturn : 31.399425506591797
Agent1_Eval_MaxReturn : 12.178665161132812
Agent1_Eval_MinReturn : -109.13307189941406
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -48.90297317504883
Agent1_Train_StdReturn : 35.07338333129883
Agent1_Train_MaxReturn : -0.048424720764160156
Agent1_Train_MinReturn : -110.28343200683594
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 132000
Agent1_TimeSinceStart : 449.27367877960205
Agent1_Critic_Loss : 1.9910860061645508
Agent1_Actor_Loss : -4.172895431518555
Agent1_Alpha_Loss : 4.877472400665283
Agent1_Temperature : 0.4870050042506966
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -50.12071228027344
Agent0_Eval_StdReturn : 20.486854553222656
Agent0_Eval_MaxReturn : -22.34461212158203
Agent0_Eval_MinReturn : -77.31825256347656
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -48.31800079345703
Agent0_Train_StdReturn : 43.7397575378418
Agent0_Train_MaxReturn : 21.450645446777344
Agent0_Train_MinReturn : -121.18463897705078
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 133500
Agent0_TimeSinceStart : 452.01011848449707
Agent0_Critic_Loss : 1.7568809986114502
Agent0_Actor_Loss : -4.118921756744385
Agent0_Alpha_Loss : 4.897319316864014
Agent0_Temperature : 0.48685811217567604
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -14.109354972839355
Agent1_Eval_StdReturn : 34.695011138916016
Agent1_Eval_MaxReturn : 44.53923034667969
Agent1_Eval_MinReturn : -79.21317291259766
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -59.47174072265625
Agent1_Train_StdReturn : 27.629112243652344
Agent1_Train_MaxReturn : -29.561620712280273
Agent1_Train_MinReturn : -120.33626556396484
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 133500
Agent1_TimeSinceStart : 454.74696612358093
Agent1_Critic_Loss : 1.634742021560669
Agent1_Actor_Loss : -4.184309005737305
Agent1_Alpha_Loss : 4.871772766113281
Agent1_Temperature : 0.48686009619515414
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -35.04439163208008
Agent0_Eval_StdReturn : 33.074073791503906
Agent0_Eval_MaxReturn : 35.314876556396484
Agent0_Eval_MinReturn : -78.463134765625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.4706916809082
Agent0_Train_StdReturn : 18.057252883911133
Agent0_Train_MaxReturn : -17.45160675048828
Agent0_Train_MinReturn : -77.12353515625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 135000
Agent0_TimeSinceStart : 457.48362922668457
Agent0_Critic_Loss : 1.4221495389938354
Agent0_Actor_Loss : -4.082334041595459
Agent0_Alpha_Loss : 4.896364688873291
Agent0_Temperature : 0.48671314496758383
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.658729553222656
Agent1_Eval_StdReturn : 33.50048065185547
Agent1_Eval_MaxReturn : 17.132831573486328
Agent1_Eval_MinReturn : -90.88336181640625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.87687110900879
Agent1_Train_StdReturn : 28.796850204467773
Agent1_Train_MaxReturn : 8.162552833557129
Agent1_Train_MinReturn : -70.66783142089844
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 135000
Agent1_TimeSinceStart : 460.2120954990387
Agent1_Critic_Loss : 2.3824522495269775
Agent1_Actor_Loss : -4.20700740814209
Agent1_Alpha_Loss : 4.8768720626831055
Agent1_Temperature : 0.4867152490587523
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -52.36582565307617
Agent0_Eval_StdReturn : 33.07178497314453
Agent0_Eval_MaxReturn : -16.36392593383789
Agent0_Eval_MinReturn : -121.0300064086914
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -47.964420318603516
Agent0_Train_StdReturn : 27.855510711669922
Agent0_Train_MaxReturn : -6.045401096343994
Agent0_Train_MinReturn : -101.55000305175781
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 136500
Agent0_TimeSinceStart : 462.9528365135193
Agent0_Critic_Loss : 2.018120288848877
Agent0_Actor_Loss : -4.0724897384643555
Agent0_Alpha_Loss : 4.866395950317383
Agent0_Temperature : 0.4865682822819632
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...
Agent0_Train_StdReturn : 25.988985061645508
Agent0_Train_MaxReturn : -17.33951187133789
Agent0_Train_MinReturn : -85.93061828613281
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 123000
Agent0_TimeSinceStart : 423.1652476787567
Agent0_Critic_Loss : 0.6044923067092896
Agent0_Actor_Loss : -0.5081075429916382
Agent0_Alpha_Loss : 0.9569292068481445
Agent0_Temperature : 0.09757951160485175
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.63884925842285
Agent1_Eval_StdReturn : 20.695865631103516
Agent1_Eval_MaxReturn : 6.444761276245117
Agent1_Eval_MinReturn : -69.75897216796875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -15.533506393432617
Agent1_Train_StdReturn : 24.697938919067383
Agent1_Train_MaxReturn : 16.52873992919922
Agent1_Train_MinReturn : -64.113037109375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 123000
Agent1_TimeSinceStart : 425.91881036758423
Agent1_Critic_Loss : 0.4368757903575897
Agent1_Actor_Loss : -0.5879377126693726
Agent1_Alpha_Loss : 0.9425821304321289
Agent1_Temperature : 0.09758146294875719
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.871925354003906
Agent0_Eval_StdReturn : 34.89061737060547
Agent0_Eval_MaxReturn : 11.187419891357422
Agent0_Eval_MinReturn : -96.70933532714844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -45.97175979614258
Agent0_Train_StdReturn : 41.054317474365234
Agent0_Train_MaxReturn : 18.895437240600586
Agent0_Train_MinReturn : -117.61559295654297
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 124500
Agent0_TimeSinceStart : 428.6682062149048
Agent0_Critic_Loss : 0.49409520626068115
Agent0_Actor_Loss : -0.4811435341835022
Agent0_Alpha_Loss : 0.9398256540298462
Agent0_Temperature : 0.09755084194942137
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -20.698741912841797
Agent1_Eval_StdReturn : 20.276050567626953
Agent1_Eval_MaxReturn : 9.928445816040039
Agent1_Eval_MinReturn : -66.3145751953125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.14872360229492
Agent1_Train_StdReturn : 21.657785415649414
Agent1_Train_MaxReturn : -17.639244079589844
Agent1_Train_MinReturn : -90.79447174072266
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 124500
Agent1_TimeSinceStart : 431.4118208885193
Agent1_Critic_Loss : 0.5286111831665039
Agent1_Actor_Loss : -0.589419960975647
Agent1_Alpha_Loss : 0.9404013156890869
Agent1_Temperature : 0.09755287566449738
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -17.425512313842773
Agent0_Eval_StdReturn : 24.743776321411133
Agent0_Eval_MaxReturn : 9.014952659606934
Agent0_Eval_MinReturn : -82.10476684570312
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -16.89983558654785
Agent0_Train_StdReturn : 25.274946212768555
Agent0_Train_MaxReturn : 33.02083969116211
Agent0_Train_MinReturn : -48.4710693359375
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 126000
Agent0_TimeSinceStart : 434.16501688957214
Agent0_Critic_Loss : 0.5599605441093445
Agent0_Actor_Loss : -0.476049542427063
Agent0_Alpha_Loss : 0.9551920890808105
Agent0_Temperature : 0.09752218721110956
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -30.120685577392578
Agent1_Eval_StdReturn : 30.073793411254883
Agent1_Eval_MaxReturn : 15.981701850891113
Agent1_Eval_MinReturn : -83.76509857177734
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -18.634126663208008
Agent1_Train_StdReturn : 13.21294116973877
Agent1_Train_MaxReturn : 1.940922737121582
Agent1_Train_MinReturn : -35.137657165527344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 126000
Agent1_TimeSinceStart : 436.91757011413574
Agent1_Critic_Loss : 0.46234652400016785
Agent1_Actor_Loss : -0.6043769121170044
Agent1_Alpha_Loss : 0.931221604347229
Agent1_Temperature : 0.09752435499353496
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.47214889526367
Agent0_Eval_StdReturn : 43.66189956665039
Agent0_Eval_MaxReturn : 32.786659240722656
Agent0_Eval_MinReturn : -139.84512329101562
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -46.88466262817383
Agent0_Train_StdReturn : 28.79792022705078
Agent0_Train_MaxReturn : 13.780720710754395
Agent0_Train_MinReturn : -88.15308380126953
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 127500
Agent0_TimeSinceStart : 439.66580510139465
Agent0_Critic_Loss : 0.5951149463653564
Agent0_Actor_Loss : -0.42130622267723083
Agent0_Alpha_Loss : 0.9314887523651123
Agent0_Temperature : 0.09749360863675512
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -13.276329040527344
Agent1_Eval_StdReturn : 26.883901596069336
Agent1_Eval_MaxReturn : 42.82299041748047
Agent1_Eval_MinReturn : -69.4725341796875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -27.83919334411621
Agent1_Train_StdReturn : 25.712337493896484
Agent1_Train_MaxReturn : 13.262516021728516
Agent1_Train_MinReturn : -85.34855651855469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 127500
Agent1_TimeSinceStart : 442.4097123146057
Agent1_Critic_Loss : 0.5234955549240112
Agent1_Actor_Loss : -0.6228640079498291
Agent1_Alpha_Loss : 0.9327862858772278
Agent1_Temperature : 0.09749588966753332
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -5.9277215003967285
Agent0_Eval_StdReturn : 20.039209365844727
Agent0_Eval_MaxReturn : 42.95069885253906
Agent0_Eval_MinReturn : -27.704469680786133
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -25.353763580322266
Agent0_Train_StdReturn : 31.485820770263672
Agent0_Train_MaxReturn : 26.70624542236328
Agent0_Train_MinReturn : -63.402488708496094
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 129000
Agent0_TimeSinceStart : 445.1685748100281
Agent0_Critic_Loss : 0.5591696500778198
Agent0_Actor_Loss : -0.3788936138153076
Agent0_Alpha_Loss : 0.9462078213691711
Agent0_Temperature : 0.09746505917381158
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -24.1169376373291
Agent1_Eval_StdReturn : 22.649749755859375
Agent1_Eval_MaxReturn : 28.73812484741211
Agent1_Eval_MinReturn : -55.81647491455078
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -25.240886688232422
Agent1_Train_StdReturn : 31.581729888916016

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.70105743408203
Agent1_Eval_StdReturn : 24.492416381835938
Agent1_Eval_MaxReturn : -2.448206901550293
Agent1_Eval_MinReturn : -81.08113861083984
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.6861572265625
Agent1_Train_StdReturn : 21.564682006835938
Agent1_Train_MaxReturn : -9.348657608032227
Agent1_Train_MinReturn : -80.57008361816406
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 136500
Agent1_TimeSinceStart : 465.68904161453247
Agent1_Critic_Loss : 1.8286583423614502
Agent1_Actor_Loss : -4.117301940917969
Agent1_Alpha_Loss : 4.885232448577881
Agent1_Temperature : 0.486570437701871
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -38.35877990722656
Agent0_Eval_StdReturn : 31.12961196899414
Agent0_Eval_MaxReturn : 25.16550064086914
Agent0_Eval_MinReturn : -94.10659790039062
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.730133056640625
Agent0_Train_StdReturn : 34.6202392578125
Agent0_Train_MaxReturn : 9.436946868896484
Agent0_Train_MinReturn : -85.60069274902344
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 138000
Agent0_TimeSinceStart : 468.4224934577942
Agent0_Critic_Loss : 2.0102744102478027
Agent0_Actor_Loss : -4.14109992980957
Agent0_Alpha_Loss : 4.875951766967773
Agent0_Temperature : 0.4864234911107243
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -46.833290100097656
Agent1_Eval_StdReturn : 28.942577362060547
Agent1_Eval_MaxReturn : -7.049307823181152
Agent1_Eval_MinReturn : -92.90760803222656
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -39.52916717529297
Agent1_Train_StdReturn : 27.118284225463867
Agent1_Train_MaxReturn : 7.4791669845581055
Agent1_Train_MinReturn : -93.43523406982422
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 138000
Agent1_TimeSinceStart : 471.1686384677887
Agent1_Critic_Loss : 1.8087060451507568
Agent1_Actor_Loss : -4.037870407104492
Agent1_Alpha_Loss : 4.878602504730225
Agent1_Temperature : 0.48642567917282686
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -33.17902374267578
Agent0_Eval_StdReturn : 19.358688354492188
Agent0_Eval_MaxReturn : 1.7706451416015625
Agent0_Eval_MinReturn : -62.166839599609375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -38.59485626220703
Agent0_Train_StdReturn : 36.590736389160156
Agent0_Train_MaxReturn : -0.44098711013793945
Agent0_Train_MinReturn : -101.3850326538086
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 139500
Agent0_TimeSinceStart : 473.89784383773804
Agent0_Critic_Loss : 1.642237663269043
Agent0_Actor_Loss : -4.121583461761475
Agent0_Alpha_Loss : 4.847938537597656
Agent0_Temperature : 0.4862788409736643
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -43.10127639770508
Agent1_Eval_StdReturn : 31.86155891418457
Agent1_Eval_MaxReturn : 13.062910079956055
Agent1_Eval_MinReturn : -100.1937026977539
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -47.265419006347656
Agent1_Train_StdReturn : 29.93377685546875
Agent1_Train_MaxReturn : -2.78840708732605
Agent1_Train_MinReturn : -92.22832489013672
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 139500
Agent1_TimeSinceStart : 476.62740087509155
Agent1_Critic_Loss : 1.576296091079712
Agent1_Actor_Loss : -4.182064056396484
Agent1_Alpha_Loss : 4.90776252746582
Agent1_Temperature : 0.4862808944264458
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -28.207189559936523
Agent0_Eval_StdReturn : 29.348169326782227
Agent0_Eval_MaxReturn : 10.265588760375977
Agent0_Eval_MinReturn : -73.40513610839844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.96653366088867
Agent0_Train_StdReturn : 30.53653907775879
Agent0_Train_MaxReturn : 19.539487838745117
Agent0_Train_MinReturn : -75.8941421508789
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 141000
Agent0_TimeSinceStart : 479.3702828884125
Agent0_Critic_Loss : 1.599156379699707
Agent0_Actor_Loss : -4.123127460479736
Agent0_Alpha_Loss : 4.878678321838379
Agent0_Temperature : 0.4861342388428423
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -42.27309036254883
Agent1_Eval_StdReturn : 26.492124557495117
Agent1_Eval_MaxReturn : -0.19948673248291016
Agent1_Eval_MinReturn : -89.90493774414062
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -14.978246688842773
Agent1_Train_StdReturn : 26.874845504760742
Agent1_Train_MaxReturn : 27.486753463745117
Agent1_Train_MinReturn : -69.8062744140625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 141000
Agent1_TimeSinceStart : 482.11015582084656
Agent1_Critic_Loss : 1.6159164905548096
Agent1_Actor_Loss : -4.222746849060059
Agent1_Alpha_Loss : 4.870995998382568
Agent1_Temperature : 0.4861361868528286
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -45.09414291381836
Agent0_Eval_StdReturn : 39.104366302490234
Agent0_Eval_MaxReturn : 9.192314147949219
Agent0_Eval_MinReturn : -115.157470703125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -18.105249404907227
Agent0_Train_StdReturn : 27.81313133239746
Agent0_Train_MaxReturn : 13.99590015411377
Agent0_Train_MinReturn : -59.33974075317383
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 142500
Agent0_TimeSinceStart : 484.86078929901123
Agent0_Critic_Loss : 1.6478283405303955
Agent0_Actor_Loss : -4.053770542144775
Agent0_Alpha_Loss : 4.863921165466309
Agent0_Temperature : 0.485989721878403
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -50.091522216796875
Agent1_Eval_StdReturn : 36.93206787109375
Agent1_Eval_MaxReturn : 7.571575164794922
Agent1_Eval_MinReturn : -108.80549621582031
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -41.60674285888672
Agent1_Train_StdReturn : 21.766237258911133
Agent1_Train_MaxReturn : -9.534934997558594
Agent1_Train_MinReturn : -78.41270446777344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 142500
Agent1_TimeSinceStart : 487.59798431396484
Agent1_Critic_Loss : 1.8363009691238403
Agent1_Actor_Loss : -4.138974666595459
Agent1_Alpha_Loss : 4.853941917419434
Agent1_Temperature : 0.4859915966172205
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Train_MaxReturn : 16.489748001098633
Agent1_Train_MinReturn : -97.33369445800781
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 129000
Agent1_TimeSinceStart : 447.91533041000366
Agent1_Critic_Loss : 0.5535963177680969
Agent1_Actor_Loss : -0.6519038677215576
Agent1_Alpha_Loss : 0.95606529712677
Agent1_Temperature : 0.0974674120582646
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -17.06996726989746
Agent0_Eval_StdReturn : 22.93058204650879
Agent0_Eval_MaxReturn : 8.314577102661133
Agent0_Eval_MinReturn : -47.64574432373047
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -35.60208511352539
Agent0_Train_StdReturn : 22.611892700195312
Agent0_Train_MaxReturn : 13.056061744689941
Agent0_Train_MinReturn : -70.27169799804688
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 130500
Agent0_TimeSinceStart : 450.66786670684814
Agent0_Critic_Loss : 0.5944368839263916
Agent0_Actor_Loss : -0.40573573112487793
Agent0_Alpha_Loss : 0.9369065165519714
Agent0_Temperature : 0.09743656049949884
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -36.322513580322266
Agent1_Eval_StdReturn : 26.897571563720703
Agent1_Eval_MaxReturn : 9.843531608581543
Agent1_Eval_MinReturn : -93.28585815429688
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -17.684785842895508
Agent1_Train_StdReturn : 34.410099029541016
Agent1_Train_MaxReturn : 46.77580261230469
Agent1_Train_MinReturn : -77.22564697265625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 130500
Agent1_TimeSinceStart : 453.4261178970337
Agent1_Critic_Loss : 0.4467378258705139
Agent1_Actor_Loss : -0.6760590076446533
Agent1_Alpha_Loss : 0.934342086315155
Agent1_Temperature : 0.0974389812733986
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -23.5334415435791
Agent0_Eval_StdReturn : 19.674684524536133
Agent0_Eval_MaxReturn : 7.122536659240723
Agent0_Eval_MinReturn : -70.2626724243164
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.54890060424805
Agent0_Train_StdReturn : 22.38774299621582
Agent0_Train_MaxReturn : -0.04775500297546387
Agent0_Train_MinReturn : -71.27145385742188
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 132000
Agent0_TimeSinceStart : 456.1775960922241
Agent0_Critic_Loss : 0.5177896022796631
Agent0_Actor_Loss : -0.4579501748085022
Agent0_Alpha_Loss : 0.940440833568573
Agent0_Temperature : 0.09740809785706427
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -14.032613754272461
Agent1_Eval_StdReturn : 18.935474395751953
Agent1_Eval_MaxReturn : 8.906929016113281
Agent1_Eval_MinReturn : -46.144561767578125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -20.014751434326172
Agent1_Train_StdReturn : 25.093170166015625
Agent1_Train_MaxReturn : 21.588706970214844
Agent1_Train_MinReturn : -61.45136260986328
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 132000
Agent1_TimeSinceStart : 458.9301850795746
Agent1_Critic_Loss : 0.4157911539077759
Agent1_Actor_Loss : -0.6801031827926636
Agent1_Alpha_Loss : 0.9286056160926819
Agent1_Temperature : 0.09741060758605592
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -24.1445369720459
Agent0_Eval_StdReturn : 22.888307571411133
Agent0_Eval_MaxReturn : 10.381124496459961
Agent0_Eval_MinReturn : -62.4449462890625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -30.76398277282715
Agent0_Train_StdReturn : 16.892898559570312
Agent0_Train_MaxReturn : -13.252914428710938
Agent0_Train_MinReturn : -71.66383361816406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 133500
Agent0_TimeSinceStart : 461.69660472869873
Agent0_Critic_Loss : 0.48898905515670776
Agent0_Actor_Loss : -0.5363364219665527
Agent0_Alpha_Loss : 0.9272381067276001
Agent0_Temperature : 0.0973797026128623
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -19.430782318115234
Agent1_Eval_StdReturn : 40.59101104736328
Agent1_Eval_MaxReturn : 41.101844787597656
Agent1_Eval_MinReturn : -74.72293853759766
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -13.29211139678955
Agent1_Train_StdReturn : 18.4559326171875
Agent1_Train_MaxReturn : 14.715940475463867
Agent1_Train_MinReturn : -39.808563232421875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 133500
Agent1_TimeSinceStart : 464.4603841304779
Agent1_Critic_Loss : 0.42794713377952576
Agent1_Actor_Loss : -0.6800395250320435
Agent1_Alpha_Loss : 0.920986533164978
Agent1_Temperature : 0.09738230518989764
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -25.55832290649414
Agent0_Eval_StdReturn : 12.85811996459961
Agent0_Eval_MaxReturn : -5.126916885375977
Agent0_Eval_MinReturn : -48.792449951171875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -24.72065544128418
Agent0_Train_StdReturn : 14.416106224060059
Agent0_Train_MaxReturn : -0.7406167984008789
Agent0_Train_MinReturn : -44.74394989013672
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 135000
Agent0_TimeSinceStart : 467.23312067985535
Agent0_Critic_Loss : 0.4256993532180786
Agent0_Actor_Loss : -0.5728851556777954
Agent0_Alpha_Loss : 0.920607328414917
Agent0_Temperature : 0.09735138525078356
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -16.333934783935547
Agent1_Eval_StdReturn : 21.63800811767578
Agent1_Eval_MaxReturn : 10.161274909973145
Agent1_Eval_MinReturn : -50.688323974609375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -22.60624885559082
Agent1_Train_StdReturn : 18.732177734375
Agent1_Train_MaxReturn : 7.708792686462402
Agent1_Train_MinReturn : -62.42292785644531
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 135000
Agent1_TimeSinceStart : 469.9940197467804
Agent1_Critic_Loss : 0.4190923571586609
Agent1_Actor_Loss : -0.6784764528274536
Agent1_Alpha_Loss : 0.9265161752700806
Agent1_Temperature : 0.09735405155634219
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.238317489624023
Agent0_Eval_StdReturn : 17.986188888549805
Agent0_Eval_MaxReturn : -6.551607131958008
Agent0_Eval_MinReturn : -57.04317855834961
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -24.71066665649414
Agent0_Train_StdReturn : 24.949451446533203
Agent0_Train_MaxReturn : 13.213279724121094
Agent0_Eval_AverageReturn : -37.42631530761719
Agent0_Eval_StdReturn : 24.18283462524414
Agent0_Eval_MaxReturn : -7.344479084014893
Agent0_Eval_MinReturn : -71.82528686523438
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.52894973754883
Agent0_Train_StdReturn : 33.09469223022461
Agent0_Train_MaxReturn : -0.7538895606994629
Agent0_Train_MinReturn : -109.50637817382812
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 144000
Agent0_TimeSinceStart : 490.3407552242279
Agent0_Critic_Loss : 1.4344249963760376
Agent0_Actor_Loss : -4.159082412719727
Agent0_Alpha_Loss : 4.843689918518066
Agent0_Temperature : 0.4858453376847202
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -41.53190612792969
Agent1_Eval_StdReturn : 32.20504379272461
Agent1_Eval_MaxReturn : -3.8634557723999023
Agent1_Eval_MinReturn : -98.9343032836914
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.465368270874023
Agent1_Train_StdReturn : 26.450071334838867
Agent1_Train_MaxReturn : 31.164264678955078
Agent1_Train_MinReturn : -74.25586700439453
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 144000
Agent1_TimeSinceStart : 493.0800156593323
Agent1_Critic_Loss : 1.8898296356201172
Agent1_Actor_Loss : -4.201510429382324
Agent1_Alpha_Loss : 4.85392951965332
Agent1_Temperature : 0.4858471144383795
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -59.67876434326172
Agent0_Eval_StdReturn : 43.41039276123047
Agent0_Eval_MaxReturn : 1.0463368892669678
Agent0_Eval_MinReturn : -137.01840209960938
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -54.70389938354492
Agent0_Train_StdReturn : 29.548036575317383
Agent0_Train_MaxReturn : 0.5970754623413086
Agent0_Train_MinReturn : -115.81842803955078
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 145500
Agent0_TimeSinceStart : 495.8297941684723
Agent0_Critic_Loss : 1.8643498420715332
Agent0_Actor_Loss : -4.079827785491943
Agent0_Alpha_Loss : 4.867094039916992
Agent0_Temperature : 0.4857010131829968
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.76405334472656
Agent1_Eval_StdReturn : 30.31812858581543
Agent1_Eval_MaxReturn : 7.302523612976074
Agent1_Eval_MinReturn : -101.41741180419922
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -43.73567581176758
Agent1_Train_StdReturn : 38.03219985961914
Agent1_Train_MaxReturn : 2.314085006713867
Agent1_Train_MinReturn : -124.74406433105469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 145500
Agent1_TimeSinceStart : 498.5781877040863
Agent1_Critic_Loss : 1.691767930984497
Agent1_Actor_Loss : -4.189853668212891
Agent1_Alpha_Loss : 4.86197566986084
Agent1_Temperature : 0.4857027106649542
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -51.69804000854492
Agent0_Eval_StdReturn : 41.266841888427734
Agent0_Eval_MaxReturn : 14.2359619140625
Agent0_Eval_MinReturn : -110.75343322753906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -49.46615219116211
Agent0_Train_StdReturn : 27.384157180786133
Agent0_Train_MaxReturn : 8.809345245361328
Agent0_Train_MinReturn : -85.62983703613281
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 147000
Agent0_TimeSinceStart : 501.3390257358551
Agent0_Critic_Loss : 1.6163034439086914
Agent0_Actor_Loss : -4.13645076751709
Agent0_Alpha_Loss : 4.871916770935059
Agent0_Temperature : 0.4855567324209293
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.109596252441406
Agent1_Eval_StdReturn : 21.502408981323242
Agent1_Eval_MaxReturn : -1.952341079711914
Agent1_Eval_MinReturn : -64.17324829101562
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.19283676147461
Agent1_Train_StdReturn : 18.943103790283203
Agent1_Train_MaxReturn : -3.893542766571045
Agent1_Train_MinReturn : -73.61958312988281
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 147000
Agent1_TimeSinceStart : 504.1026601791382
Agent1_Critic_Loss : 1.8556841611862183
Agent1_Actor_Loss : -4.243626594543457
Agent1_Alpha_Loss : 4.882237911224365
Agent1_Temperature : 0.4855583264883781
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -35.24481964111328
Agent0_Eval_StdReturn : 14.121868133544922
Agent0_Eval_MaxReturn : -8.323287963867188
Agent0_Eval_MinReturn : -49.0383415222168
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -55.710365295410156
Agent0_Train_StdReturn : 23.75084686279297
Agent0_Train_MaxReturn : -4.0385637283325195
Agent0_Train_MinReturn : -102.19561767578125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 148500
Agent0_TimeSinceStart : 506.8629174232483
Agent0_Critic_Loss : 1.537663459777832
Agent0_Actor_Loss : -4.114903926849365
Agent0_Alpha_Loss : 4.88284969329834
Agent0_Temperature : 0.48541246499569773
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -36.00620651245117
Agent1_Eval_StdReturn : 18.379535675048828
Agent1_Eval_MaxReturn : -4.406262397766113
Agent1_Eval_MinReturn : -66.44171905517578
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -40.3423957824707
Agent1_Train_StdReturn : 24.320419311523438
Agent1_Train_MaxReturn : -5.855132102966309
Agent1_Train_MinReturn : -87.60140991210938
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 148500
Agent1_TimeSinceStart : 509.6249055862427
Agent1_Critic_Loss : 2.024667739868164
Agent1_Actor_Loss : -4.2237348556518555
Agent1_Alpha_Loss : 4.83463191986084
Agent1_Temperature : 0.4854140894475433
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -31.255971908569336
Agent0_Eval_StdReturn : 14.7269868850708
Agent0_Eval_MaxReturn : -4.05867862701416
Agent0_Eval_MinReturn : -53.73715591430664
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -51.093345642089844
Agent0_Train_StdReturn : 24.233684539794922
Agent0_Train_MaxReturn : -23.43173599243164
Agent0_Train_MinReturn : -94.7928466796875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 150000
Agent0_TimeSinceStart : 512.3808834552765
Agent0_Critic_Loss : 1.792182445526123
Agent0_Actor_Loss : -4.085780143737793
Agent0_Alpha_Loss : 4.837878704071045
Agent0_Temperature : 0.48526833214413534
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -42.59363555908203
Agent1_Eval_StdReturn : 37.782527923583984
Agent0_Train_MinReturn : -73.43341064453125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 136500
Agent0_TimeSinceStart : 472.7633149623871
Agent0_Critic_Loss : 0.540086030960083
Agent0_Actor_Loss : -0.4978868365287781
Agent0_Alpha_Loss : 0.9137117266654968
Agent0_Temperature : 0.09732315583133504
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -16.355396270751953
Agent1_Eval_StdReturn : 22.893850326538086
Agent1_Eval_MaxReturn : 22.080963134765625
Agent1_Eval_MinReturn : -66.3814468383789
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -20.874752044677734
Agent1_Train_StdReturn : 26.1030330657959
Agent1_Train_MaxReturn : 19.987171173095703
Agent1_Train_MinReturn : -72.40914916992188
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 136500
Agent1_TimeSinceStart : 475.5313620567322
Agent1_Critic_Loss : 0.3938885033130646
Agent1_Actor_Loss : -0.6279236078262329
Agent1_Alpha_Loss : 0.9349766373634338
Agent1_Temperature : 0.09732581870725322
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -27.286006927490234
Agent0_Eval_StdReturn : 16.856834411621094
Agent0_Eval_MaxReturn : 4.427346229553223
Agent0_Eval_MinReturn : -48.5699577331543
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -26.438329696655273
Agent0_Train_StdReturn : 23.146076202392578
Agent0_Train_MaxReturn : 19.973033905029297
Agent0_Train_MinReturn : -60.58634948730469
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 138000
Agent0_TimeSinceStart : 478.2997796535492
Agent0_Critic_Loss : 0.5226173996925354
Agent0_Actor_Loss : -0.4979202449321747
Agent0_Alpha_Loss : 0.9273437261581421
Agent0_Temperature : 0.09729496824672348
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -15.630090713500977
Agent1_Eval_StdReturn : 17.43802833557129
Agent1_Eval_MaxReturn : 9.503493309020996
Agent1_Eval_MinReturn : -44.855464935302734
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -31.755630493164062
Agent1_Train_StdReturn : 25.704124450683594
Agent1_Train_MaxReturn : 1.774315357208252
Agent1_Train_MinReturn : -78.44670867919922
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 138000
Agent1_TimeSinceStart : 481.0809407234192
Agent1_Critic_Loss : 0.4345741868019104
Agent1_Actor_Loss : -0.605309784412384
Agent1_Alpha_Loss : 0.9056826233863831
Agent1_Temperature : 0.09729768299654404
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -24.913738250732422
Agent0_Eval_StdReturn : 26.45997428894043
Agent0_Eval_MaxReturn : 21.876541137695312
Agent0_Eval_MinReturn : -69.68862915039062
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -26.10837173461914
Agent0_Train_StdReturn : 15.591451644897461
Agent0_Train_MaxReturn : 4.110922336578369
Agent0_Train_MinReturn : -51.64878845214844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 139500
Agent0_TimeSinceStart : 483.856689453125
Agent0_Critic_Loss : 0.4793945550918579
Agent0_Actor_Loss : -0.4432438313961029
Agent0_Alpha_Loss : 0.9043779373168945
Agent0_Temperature : 0.09726687962101878
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.18512535095215
Agent1_Eval_StdReturn : 20.227313995361328
Agent1_Eval_MaxReturn : -4.751785755157471
Agent1_Eval_MinReturn : -74.29541778564453
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -24.72637176513672
Agent1_Train_StdReturn : 18.847471237182617
Agent1_Train_MaxReturn : 2.9499778747558594
Agent1_Train_MinReturn : -54.89835739135742
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 139500
Agent1_TimeSinceStart : 486.6316294670105
Agent1_Critic_Loss : 0.41362300515174866
Agent1_Actor_Loss : -0.6200110912322998
Agent1_Alpha_Loss : 0.9321560859680176
Agent1_Temperature : 0.09726956259985929
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -23.36887550354004
Agent0_Eval_StdReturn : 16.112215042114258
Agent0_Eval_MaxReturn : 5.143350601196289
Agent0_Eval_MinReturn : -59.081199645996094
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.407752990722656
Agent0_Train_StdReturn : 26.594329833984375
Agent0_Train_MaxReturn : -2.720496654510498
Agent0_Train_MinReturn : -92.88622283935547
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 141000
Agent0_TimeSinceStart : 489.40509486198425
Agent0_Critic_Loss : 0.4812074899673462
Agent0_Actor_Loss : -0.42231208086013794
Agent0_Alpha_Loss : 0.9117258787155151
Agent0_Temperature : 0.09723885934089785
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -18.05085563659668
Agent1_Eval_StdReturn : 19.498727798461914
Agent1_Eval_MaxReturn : 4.477959156036377
Agent1_Eval_MinReturn : -63.7203369140625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -18.838241577148438
Agent1_Train_StdReturn : 18.838266372680664
Agent1_Train_MaxReturn : 4.286777496337891
Agent1_Train_MinReturn : -54.077491760253906
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 141000
Agent1_TimeSinceStart : 492.1953046321869
Agent1_Critic_Loss : 0.4234750270843506
Agent1_Actor_Loss : -0.667879045009613
Agent1_Alpha_Loss : 0.9009405970573425
Agent1_Temperature : 0.09724153986304385
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -20.292285919189453
Agent0_Eval_StdReturn : 16.1856689453125
Agent0_Eval_MaxReturn : 9.16702651977539
Agent0_Eval_MinReturn : -48.706539154052734
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.41899871826172
Agent0_Train_StdReturn : 24.881725311279297
Agent0_Train_MaxReturn : -11.244693756103516
Agent0_Train_MinReturn : -81.37252807617188
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 142500
Agent0_TimeSinceStart : 494.9749536514282
Agent0_Critic_Loss : 0.5123757719993591
Agent0_Actor_Loss : -0.4140743911266327
Agent0_Alpha_Loss : 0.9120383858680725
Agent0_Temperature : 0.09721089899825615
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -21.674564361572266
Agent1_Eval_StdReturn : 25.298748016357422
Agent1_Eval_MaxReturn : 3.2511632442474365
Agent1_Eval_MinReturn : -68.97624206542969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.459503173828125
Agent1_Train_StdReturn : 27.816631317138672
Agent1_Train_MaxReturn : 46.70536804199219
Agent1_Train_MinReturn : -60.56920623779297
Agent1_Eval_MaxReturn : 14.879470825195312
Agent1_Eval_MinReturn : -105.96649932861328
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -39.48610305786133
Agent1_Train_StdReturn : 22.527055740356445
Agent1_Train_MaxReturn : -9.01633071899414
Agent1_Train_MinReturn : -87.87210083007812
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 150000
Agent1_TimeSinceStart : 515.1286191940308
Agent1_Critic_Loss : 1.7530477046966553
Agent1_Actor_Loss : -4.283242225646973
Agent1_Alpha_Loss : 4.864020347595215
Agent1_Temperature : 0.48526990883896703
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -60.4299201965332
Agent0_Eval_StdReturn : 35.639183044433594
Agent0_Eval_MaxReturn : 11.549741744995117
Agent0_Eval_MinReturn : -123.16539764404297
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.37431716918945
Agent0_Train_StdReturn : 24.010726928710938
Agent0_Train_MaxReturn : -9.755497932434082
Agent0_Train_MinReturn : -105.47456359863281
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 151500
Agent0_TimeSinceStart : 517.8842945098877
Agent0_Critic_Loss : 1.6510494947433472
Agent0_Actor_Loss : -4.14870548248291
Agent0_Alpha_Loss : 4.858325004577637
Agent0_Temperature : 0.485124268194523
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -52.009002685546875
Agent1_Eval_StdReturn : 21.418851852416992
Agent1_Eval_MaxReturn : -14.250568389892578
Agent1_Eval_MinReturn : -91.72935485839844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -31.071765899658203
Agent1_Train_StdReturn : 23.965375900268555
Agent1_Train_MaxReturn : -7.920730113983154
Agent1_Train_MinReturn : -95.20372772216797
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 151500
Agent1_TimeSinceStart : 520.6403090953827
Agent1_Critic_Loss : 1.6539151668548584
Agent1_Actor_Loss : -4.205385684967041
Agent1_Alpha_Loss : 4.833188056945801
Agent1_Temperature : 0.48512586378817296
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -40.86361312866211
Agent0_Eval_StdReturn : 36.25371170043945
Agent0_Eval_MaxReturn : 28.713314056396484
Agent0_Eval_MinReturn : -98.61181640625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -55.7154655456543
Agent0_Train_StdReturn : 30.039159774780273
Agent0_Train_MaxReturn : -18.116050720214844
Agent0_Train_MinReturn : -108.94005584716797
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 153000
Agent0_TimeSinceStart : 523.4058899879456
Agent0_Critic_Loss : 1.6535987854003906
Agent0_Actor_Loss : -4.078956604003906
Agent0_Alpha_Loss : 4.838040351867676
Agent0_Temperature : 0.4849803228580333
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -33.1578483581543
Agent1_Eval_StdReturn : 31.706649780273438
Agent1_Eval_MaxReturn : 21.733184814453125
Agent1_Eval_MinReturn : -65.13156127929688
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.330604553222656
Agent1_Train_StdReturn : 32.26581573486328
Agent1_Train_MaxReturn : 4.091413974761963
Agent1_Train_MinReturn : -83.9277114868164
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 153000
Agent1_TimeSinceStart : 526.1727273464203
Agent1_Critic_Loss : 1.9754425287246704
Agent1_Actor_Loss : -4.336732864379883
Agent1_Alpha_Loss : 4.876336097717285
Agent1_Temperature : 0.484981828050902
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -59.13994598388672
Agent0_Eval_StdReturn : 30.37205696105957
Agent0_Eval_MaxReturn : 19.761470794677734
Agent0_Eval_MinReturn : -109.08892059326172
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -42.95753860473633
Agent0_Train_StdReturn : 34.122406005859375
Agent0_Train_MaxReturn : 33.09075164794922
Agent0_Train_MinReturn : -95.0435791015625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 154500
Agent0_TimeSinceStart : 528.9499409198761
Agent0_Critic_Loss : 1.5064072608947754
Agent0_Actor_Loss : -4.108487129211426
Agent0_Alpha_Loss : 4.856344223022461
Agent0_Temperature : 0.4848364377312035
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -52.708465576171875
Agent1_Eval_StdReturn : 31.3398494720459
Agent1_Eval_MaxReturn : 10.770642280578613
Agent1_Eval_MinReturn : -97.84245300292969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.25577926635742
Agent1_Train_StdReturn : 39.06943893432617
Agent1_Train_MaxReturn : 57.21040344238281
Agent1_Train_MinReturn : -71.93415832519531
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 154500
Agent1_TimeSinceStart : 531.7088754177094
Agent1_Critic_Loss : 1.4816431999206543
Agent1_Actor_Loss : -4.2939043045043945
Agent1_Alpha_Loss : 4.826338768005371
Agent1_Temperature : 0.484837937001295
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -29.657047271728516
Agent0_Eval_StdReturn : 29.427139282226562
Agent0_Eval_MaxReturn : 23.31742286682129
Agent0_Eval_MinReturn : -77.255615234375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.76712417602539
Agent0_Train_StdReturn : 34.99177932739258
Agent0_Train_MaxReturn : -4.2629780769348145
Agent0_Train_MinReturn : -109.26033020019531
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 156000
Agent0_TimeSinceStart : 534.4851126670837
Agent0_Critic_Loss : 1.5666236877441406
Agent0_Actor_Loss : -4.097318649291992
Agent0_Alpha_Loss : 4.8629608154296875
Agent0_Temperature : 0.48469259184798313
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -45.10114288330078
Agent1_Eval_StdReturn : 21.63738441467285
Agent1_Eval_MaxReturn : 7.529965400695801
Agent1_Eval_MinReturn : -73.00212860107422
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -33.95109939575195
Agent1_Train_StdReturn : 33.39421844482422
Agent1_Train_MaxReturn : 15.411659240722656
Agent1_Train_MinReturn : -96.36378479003906
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 156000
Agent1_TimeSinceStart : 537.2525622844696
Agent1_Critic_Loss : 1.9397635459899902
Agent1_Actor_Loss : -4.326218605041504
Agent1_Alpha_Loss : 4.851787567138672
Agent1_Temperature : 0.484694110300118
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.39101028442383
Agent0_Eval_StdReturn : 45.18393325805664
Agent0_Eval_MaxReturn : 46.56668472290039
Agent0_Eval_MinReturn : -118.38885498046875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 142500
Agent1_TimeSinceStart : 497.7375638484955
Agent1_Critic_Loss : 0.3940712809562683
Agent1_Actor_Loss : -0.6313234567642212
Agent1_Alpha_Loss : 0.9170056581497192
Agent1_Temperature : 0.09721356066331747
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -18.559907913208008
Agent0_Eval_StdReturn : 16.170711517333984
Agent0_Eval_MaxReturn : 3.423492908477783
Agent0_Eval_MinReturn : -48.07121276855469
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -25.27597427368164
Agent0_Train_StdReturn : 11.680273056030273
Agent0_Train_MaxReturn : -9.716197967529297
Agent0_Train_MinReturn : -46.59477233886719
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 144000
Agent0_TimeSinceStart : 500.52396535873413
Agent0_Critic_Loss : 0.4204651713371277
Agent0_Actor_Loss : -0.4616517424583435
Agent0_Alpha_Loss : 0.8939509987831116
Agent0_Temperature : 0.09718304084875602
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -28.876998901367188
Agent1_Eval_StdReturn : 23.28034210205078
Agent1_Eval_MaxReturn : -2.4117746353149414
Agent1_Eval_MinReturn : -85.2811279296875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -23.821430206298828
Agent1_Train_StdReturn : 24.012004852294922
Agent1_Train_MaxReturn : 26.553001403808594
Agent1_Train_MinReturn : -54.166015625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 144000
Agent1_TimeSinceStart : 503.30935621261597
Agent1_Critic_Loss : 0.3544274568557739
Agent1_Actor_Loss : -0.6779688596725464
Agent1_Alpha_Loss : 0.9040157198905945
Agent1_Temperature : 0.09718565523650677
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -14.883996963500977
Agent0_Eval_StdReturn : 15.996687889099121
Agent0_Eval_MaxReturn : 16.821006774902344
Agent0_Eval_MinReturn : -37.71453094482422
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -19.610078811645508
Agent0_Train_StdReturn : 20.70015525817871
Agent0_Train_MaxReturn : 7.237809181213379
Agent0_Train_MinReturn : -65.8358154296875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 145500
Agent0_TimeSinceStart : 506.08763909339905
Agent0_Critic_Loss : 0.40506207942962646
Agent0_Actor_Loss : -0.47021007537841797
Agent0_Alpha_Loss : 0.9028485417366028
Agent0_Temperature : 0.09715524948168935
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.697111129760742
Agent1_Eval_StdReturn : 16.616920471191406
Agent1_Eval_MaxReturn : 2.527613639831543
Agent1_Eval_MinReturn : -52.5850830078125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -25.19204330444336
Agent1_Train_StdReturn : 14.470233917236328
Agent1_Train_MaxReturn : -0.12697124481201172
Agent1_Train_MinReturn : -46.93354797363281
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 145500
Agent1_TimeSinceStart : 508.8764295578003
Agent1_Critic_Loss : 0.43989574909210205
Agent1_Actor_Loss : -0.611974835395813
Agent1_Alpha_Loss : 0.912286102771759
Agent1_Temperature : 0.097157792953473
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -20.324975967407227
Agent0_Eval_StdReturn : 18.19088363647461
Agent0_Eval_MaxReturn : 6.134929180145264
Agent0_Eval_MinReturn : -53.426780700683594
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -23.1619930267334
Agent0_Train_StdReturn : 12.661643981933594
Agent0_Train_MaxReturn : -3.883249282836914
Agent0_Train_MinReturn : -49.697532653808594
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 147000
Agent0_TimeSinceStart : 511.66463232040405
Agent0_Critic_Loss : 0.36894166469573975
Agent0_Actor_Loss : -0.556588888168335
Agent0_Alpha_Loss : 0.9002009034156799
Agent0_Temperature : 0.09712752455958663
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.357257843017578
Agent1_Eval_StdReturn : 16.432607650756836
Agent1_Eval_MaxReturn : -5.390755653381348
Agent1_Eval_MinReturn : -58.27543258666992
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -17.48201560974121
Agent1_Train_StdReturn : 13.576582908630371
Agent1_Train_MaxReturn : -2.012786865234375
Agent1_Train_MinReturn : -48.45803451538086
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 147000
Agent1_TimeSinceStart : 514.4394767284393
Agent1_Critic_Loss : 0.38824111223220825
Agent1_Actor_Loss : -0.6273313164710999
Agent1_Alpha_Loss : 0.8990598917007446
Agent1_Temperature : 0.09713000480194407
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -19.698801040649414
Agent0_Eval_StdReturn : 13.749624252319336
Agent0_Eval_MaxReturn : -3.1865320205688477
Agent0_Eval_MinReturn : -51.80763244628906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -24.23345947265625
Agent0_Train_StdReturn : 16.63722801208496
Agent0_Train_MaxReturn : 0.19294166564941406
Agent0_Train_MinReturn : -51.466773986816406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 148500
Agent0_TimeSinceStart : 517.2305345535278
Agent0_Critic_Loss : 0.3841492235660553
Agent0_Actor_Loss : -0.5672196745872498
Agent0_Alpha_Loss : 0.9075567722320557
Agent0_Temperature : 0.09709983855108839
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -18.702098846435547
Agent1_Eval_StdReturn : 17.303396224975586
Agent1_Eval_MaxReturn : 14.164732933044434
Agent1_Eval_MinReturn : -48.06064987182617
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -15.684712409973145
Agent1_Train_StdReturn : 21.41228675842285
Agent1_Train_MaxReturn : 34.19791793823242
Agent1_Train_MinReturn : -55.397010803222656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 148500
Agent1_TimeSinceStart : 520.0167887210846
Agent1_Critic_Loss : 0.36099404096603394
Agent1_Actor_Loss : -0.6218196153640747
Agent1_Alpha_Loss : 0.8849721550941467
Agent1_Temperature : 0.0971023208858074
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -28.557666778564453
Agent0_Eval_StdReturn : 16.682117462158203
Agent0_Eval_MaxReturn : -0.7811627388000488
Agent0_Eval_MinReturn : -48.633480072021484
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -19.56614112854004
Agent0_Train_StdReturn : 24.95667839050293
Agent0_Train_MaxReturn : 28.500532150268555
Agent0_Train_MinReturn : -63.17575454711914
Agent0_Train_AverageEpLen : 150.0
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -38.643516540527344
Agent0_Train_StdReturn : 36.71797561645508
Agent0_Train_MaxReturn : 36.300167083740234
Agent0_Train_MinReturn : -99.80873107910156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 157500
Agent0_TimeSinceStart : 540.0293419361115
Agent0_Critic_Loss : 1.772115707397461
Agent0_Actor_Loss : -4.149441719055176
Agent0_Alpha_Loss : 4.842280864715576
Agent0_Temperature : 0.48454883927634296
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.529823303222656
Agent1_Eval_StdReturn : 24.5628662109375
Agent1_Eval_MaxReturn : -4.032769680023193
Agent1_Eval_MinReturn : -84.75885772705078
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -37.71790313720703
Agent1_Train_StdReturn : 36.78450393676758
Agent1_Train_MaxReturn : 14.274497985839844
Agent1_Train_MinReturn : -102.46470642089844
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 157500
Agent1_TimeSinceStart : 542.7929267883301
Agent1_Critic_Loss : 1.4839229583740234
Agent1_Actor_Loss : -4.270708084106445
Agent1_Alpha_Loss : 4.8481645584106445
Agent1_Temperature : 0.48455035380867234
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -43.9620361328125
Agent0_Eval_StdReturn : 22.456382751464844
Agent0_Eval_MaxReturn : 7.645174026489258
Agent0_Eval_MinReturn : -78.45835876464844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -40.955718994140625
Agent0_Train_StdReturn : 24.26340103149414
Agent0_Train_MaxReturn : -5.07963752746582
Agent0_Train_MinReturn : -78.93849182128906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 159000
Agent0_TimeSinceStart : 545.5744082927704
Agent0_Critic_Loss : 1.759397029876709
Agent0_Actor_Loss : -4.161314964294434
Agent0_Alpha_Loss : 4.843276023864746
Agent0_Temperature : 0.48440517040209363
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.382572174072266
Agent1_Eval_StdReturn : 19.317787170410156
Agent1_Eval_MaxReturn : 15.228529930114746
Agent1_Eval_MinReturn : -45.58964920043945
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.61518859863281
Agent1_Train_StdReturn : 26.261552810668945
Agent1_Train_MaxReturn : 3.530427932739258
Agent1_Train_MinReturn : -84.76490783691406
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 159000
Agent1_TimeSinceStart : 548.3185424804688
Agent1_Critic_Loss : 1.7182226181030273
Agent1_Actor_Loss : -4.366316795349121
Agent1_Alpha_Loss : 4.847891330718994
Agent1_Temperature : 0.484406663807985
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -39.95030975341797
Agent0_Eval_StdReturn : 23.852418899536133
Agent0_Eval_MaxReturn : 7.47838830947876
Agent0_Eval_MinReturn : -73.70240020751953
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -41.72304153442383
Agent0_Train_StdReturn : 29.61971664428711
Agent0_Train_MaxReturn : 22.713287353515625
Agent0_Train_MinReturn : -73.40989685058594
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 160500
Agent0_TimeSinceStart : 551.0864164829254
Agent0_Critic_Loss : 1.8492941856384277
Agent0_Actor_Loss : -4.160009384155273
Agent0_Alpha_Loss : 4.876431465148926
Agent0_Temperature : 0.4842614907765552
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.141998291015625
Agent1_Eval_StdReturn : 30.660411834716797
Agent1_Eval_MaxReturn : 28.803186416625977
Agent1_Eval_MinReturn : -78.2507095336914
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -24.314682006835938
Agent1_Train_StdReturn : 36.8133430480957
Agent1_Train_MaxReturn : 35.23788070678711
Agent1_Train_MinReturn : -99.55137634277344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 160500
Agent1_TimeSinceStart : 553.8557002544403
Agent1_Critic_Loss : 1.5966877937316895
Agent1_Actor_Loss : -4.371681213378906
Agent1_Alpha_Loss : 4.858544826507568
Agent1_Temperature : 0.48426300778594883
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -48.04787826538086
Agent0_Eval_StdReturn : 45.105350494384766
Agent0_Eval_MaxReturn : 4.103585720062256
Agent0_Eval_MinReturn : -145.51278686523438
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -31.096887588500977
Agent0_Train_StdReturn : 20.628582000732422
Agent0_Train_MaxReturn : 4.9685821533203125
Agent0_Train_MinReturn : -67.85147094726562
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 162000
Agent0_TimeSinceStart : 556.6394212245941
Agent0_Critic_Loss : 1.8756382465362549
Agent0_Actor_Loss : -4.119920253753662
Agent0_Alpha_Loss : 4.838549613952637
Agent0_Temperature : 0.4841179059611633
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -33.813453674316406
Agent1_Eval_StdReturn : 25.456581115722656
Agent1_Eval_MaxReturn : -3.9215707778930664
Agent1_Eval_MinReturn : -87.75528717041016
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -47.76274108886719
Agent1_Train_StdReturn : 26.3413143157959
Agent1_Train_MaxReturn : -13.299276351928711
Agent1_Train_MinReturn : -102.25733947753906
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 162000
Agent1_TimeSinceStart : 559.4179131984711
Agent1_Critic_Loss : 1.9062230587005615
Agent1_Actor_Loss : -4.327621936798096
Agent1_Alpha_Loss : 4.830313682556152
Agent1_Temperature : 0.4841194607025314
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.822933197021484
Agent0_Eval_StdReturn : 39.49443435668945
Agent0_Eval_MaxReturn : 24.692157745361328
Agent0_Eval_MinReturn : -106.33360290527344
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -31.839258193969727
Agent0_Train_StdReturn : 32.48277282714844
Agent0_Train_MaxReturn : 13.186408996582031
Agent0_Train_MinReturn : -109.39478302001953
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 163500
Agent0_TimeSinceStart : 562.1920018196106
Agent0_Critic_Loss : 1.4848687648773193
Agent0_Actor_Loss : -4.122459888458252
Agent0_Alpha_Loss : 4.863314628601074
Agent0_Temperature : 0.48397434261692707
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -44.289100646972656
Agent1_Eval_StdReturn : 37.9926643371582
Agent1_Eval_MaxReturn : 11.017303466796875
Agent1_Eval_MinReturn : -108.8645248413086
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -38.50292205810547
Agent0_Train_EnvstepsSoFar : 150000
Agent0_TimeSinceStart : 522.8069198131561
Agent0_Critic_Loss : 0.38074734807014465
Agent0_Actor_Loss : -0.5662225484848022
Agent0_Alpha_Loss : 0.8709820508956909
Agent0_Temperature : 0.09707228665842564
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -11.711962699890137
Agent1_Eval_StdReturn : 11.804526329040527
Agent1_Eval_MaxReturn : 7.701828479766846
Agent1_Eval_MinReturn : -31.555057525634766
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -16.013118743896484
Agent1_Train_StdReturn : 21.4144229888916
Agent1_Train_MaxReturn : 9.844315528869629
Agent1_Train_MinReturn : -53.123199462890625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 150000
Agent1_TimeSinceStart : 525.6084954738617
Agent1_Critic_Loss : 0.3219014108181
Agent1_Actor_Loss : -0.5268295407295227
Agent1_Alpha_Loss : 0.8921148777008057
Agent1_Temperature : 0.09707471013305265
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -32.59197235107422
Agent0_Eval_StdReturn : 23.999025344848633
Agent0_Eval_MaxReturn : 2.690768003463745
Agent0_Eval_MinReturn : -91.6175765991211
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -15.93828010559082
Agent0_Train_StdReturn : 10.671207427978516
Agent0_Train_MaxReturn : -1.5952119827270508
Agent0_Train_MinReturn : -39.31641387939453
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 151500
Agent0_TimeSinceStart : 528.3997411727905
Agent0_Critic_Loss : 0.41071900725364685
Agent0_Actor_Loss : -0.600660502910614
Agent0_Alpha_Loss : 0.8792341947555542
Agent0_Temperature : 0.0970448314857423
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -16.35516929626465
Agent1_Eval_StdReturn : 22.111797332763672
Agent1_Eval_MaxReturn : 34.92647933959961
Agent1_Eval_MinReturn : -43.659934997558594
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -24.181699752807617
Agent1_Train_StdReturn : 17.71782112121582
Agent1_Train_MaxReturn : 1.6402359008789062
Agent1_Train_MinReturn : -58.330169677734375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 151500
Agent1_TimeSinceStart : 531.1764433383942
Agent1_Critic_Loss : 0.3411027789115906
Agent1_Actor_Loss : -0.5358374118804932
Agent1_Alpha_Loss : 0.8724430799484253
Agent1_Temperature : 0.0970472181211219
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -8.525150299072266
Agent0_Eval_StdReturn : 10.309374809265137
Agent0_Eval_MaxReturn : 7.206986427307129
Agent0_Eval_MinReturn : -24.289058685302734
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -24.95853042602539
Agent0_Train_StdReturn : 10.476248741149902
Agent0_Train_MaxReturn : -16.75749969482422
Agent0_Train_MinReturn : -53.35354995727539
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 153000
Agent0_TimeSinceStart : 533.9808275699615
Agent0_Critic_Loss : 0.4567255675792694
Agent0_Actor_Loss : -0.46286022663116455
Agent0_Alpha_Loss : 0.8711020350456238
Agent0_Temperature : 0.097017484457617
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -14.922329902648926
Agent1_Eval_StdReturn : 20.972915649414062
Agent1_Eval_MaxReturn : 11.461807250976562
Agent1_Eval_MinReturn : -41.54685592651367
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -20.65091896057129
Agent1_Train_StdReturn : 23.91273307800293
Agent1_Train_MaxReturn : -1.2800254821777344
Agent1_Train_MinReturn : -90.082763671875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 153000
Agent1_TimeSinceStart : 536.7759749889374
Agent1_Critic_Loss : 0.3220489025115967
Agent1_Actor_Loss : -0.6050223708152771
Agent1_Alpha_Loss : 0.8877025246620178
Agent1_Temperature : 0.09701978978291632
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -18.011030197143555
Agent0_Eval_StdReturn : 16.437116622924805
Agent0_Eval_MaxReturn : 12.552849769592285
Agent0_Eval_MinReturn : -46.87604522705078
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -20.26045799255371
Agent0_Train_StdReturn : 16.295822143554688
Agent0_Train_MaxReturn : 9.157034873962402
Agent0_Train_MinReturn : -42.97652053833008
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 154500
Agent0_TimeSinceStart : 539.5836119651794
Agent0_Critic_Loss : 0.4034852385520935
Agent0_Actor_Loss : -0.38651835918426514
Agent0_Alpha_Loss : 0.8483317494392395
Agent0_Temperature : 0.09699029613200345
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -21.546905517578125
Agent1_Eval_StdReturn : 22.584686279296875
Agent1_Eval_MaxReturn : 2.2159571647644043
Agent1_Eval_MinReturn : -80.72526550292969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -16.409128189086914
Agent1_Train_StdReturn : 18.73330307006836
Agent1_Train_MaxReturn : 7.839657306671143
Agent1_Train_MinReturn : -51.80110549926758
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 154500
Agent1_TimeSinceStart : 542.3759305477142
Agent1_Critic_Loss : 0.3783237040042877
Agent1_Actor_Loss : -0.6252727508544922
Agent1_Alpha_Loss : 0.8517570495605469
Agent1_Temperature : 0.09699251665903229
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -7.375658988952637
Agent0_Eval_StdReturn : 18.50356674194336
Agent0_Eval_MaxReturn : 18.694896697998047
Agent0_Eval_MinReturn : -52.39531326293945
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -21.566211700439453
Agent0_Train_StdReturn : 15.450748443603516
Agent0_Train_MaxReturn : 1.348292350769043
Agent0_Train_MinReturn : -54.232933044433594
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 156000
Agent0_TimeSinceStart : 545.1718592643738
Agent0_Critic_Loss : 0.4528430104255676
Agent0_Actor_Loss : -0.33009573817253113
Agent0_Alpha_Loss : 0.8589749336242676
Agent0_Temperature : 0.09696321956717623
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.548221588134766
Agent1_Eval_StdReturn : 17.98331069946289
Agent1_Eval_MaxReturn : 1.4631118774414062
Agent1_Eval_MinReturn : -50.78193664550781
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -16.24617576599121
Agent1_Train_StdReturn : 15.46690845489502
Agent1_Train_MaxReturn : 0.6532487869262695
Agent1_Train_MinReturn : -43.157806396484375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 156000
Agent1_Train_StdReturn : 41.98108673095703
Agent1_Train_MaxReturn : 26.369613647460938
Agent1_Train_MinReturn : -104.84410095214844
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 163500
Agent1_TimeSinceStart : 565.042729139328
Agent1_Critic_Loss : 1.61796236038208
Agent1_Actor_Loss : -4.261000156402588
Agent1_Alpha_Loss : 4.82369327545166
Agent1_Temperature : 0.483976031600569
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.5763053894043
Agent0_Eval_StdReturn : 28.238643646240234
Agent0_Eval_MaxReturn : -5.9010772705078125
Agent0_Eval_MinReturn : -94.92036437988281
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.26892852783203
Agent0_Train_StdReturn : 25.905256271362305
Agent0_Train_MaxReturn : 5.005561351776123
Agent0_Train_MinReturn : -73.45997619628906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 165000
Agent0_TimeSinceStart : 567.8313658237457
Agent0_Critic_Loss : 1.7210068702697754
Agent0_Actor_Loss : -4.118706703186035
Agent0_Alpha_Loss : 4.83192253112793
Agent0_Temperature : 0.48383088560616605
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.667747497558594
Agent1_Eval_StdReturn : 39.455196380615234
Agent1_Eval_MaxReturn : 16.714271545410156
Agent1_Eval_MinReturn : -117.21249389648438
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.57695770263672
Agent1_Train_StdReturn : 42.709468841552734
Agent1_Train_MaxReturn : 13.924399375915527
Agent1_Train_MinReturn : -141.57196044921875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 165000
Agent1_TimeSinceStart : 570.625020980835
Agent1_Critic_Loss : 1.613476276397705
Agent1_Actor_Loss : -4.2140913009643555
Agent1_Alpha_Loss : 4.873770713806152
Agent1_Temperature : 0.4838325766904409
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.25226593017578
Agent0_Eval_StdReturn : 27.044334411621094
Agent0_Eval_MaxReturn : -7.747275352478027
Agent0_Eval_MinReturn : -87.93119049072266
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.77982711791992
Agent0_Train_StdReturn : 15.034045219421387
Agent0_Train_MaxReturn : -1.725809097290039
Agent0_Train_MinReturn : -56.601905822753906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 166500
Agent0_TimeSinceStart : 573.4152565002441
Agent0_Critic_Loss : 1.5784502029418945
Agent0_Actor_Loss : -4.210576057434082
Agent0_Alpha_Loss : 4.852242946624756
Agent0_Temperature : 0.48368747212582913
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -44.79651641845703
Agent1_Eval_StdReturn : 34.048973083496094
Agent1_Eval_MaxReturn : 2.3363852500915527
Agent1_Eval_MinReturn : -122.04570007324219
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -8.534177780151367
Agent1_Train_StdReturn : 30.7193546295166
Agent1_Train_MaxReturn : 42.99384689331055
Agent1_Train_MinReturn : -39.7440071105957
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 166500
Agent1_TimeSinceStart : 576.2079124450684
Agent1_Critic_Loss : 1.7633159160614014
Agent1_Actor_Loss : -4.283748626708984
Agent1_Alpha_Loss : 4.870765686035156
Agent1_Temperature : 0.4836891099429607
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -39.22382736206055
Agent0_Eval_StdReturn : 21.18613052368164
Agent0_Eval_MaxReturn : -6.440244674682617
Agent0_Eval_MinReturn : -72.05137634277344
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -38.38029479980469
Agent0_Train_StdReturn : 41.252777099609375
Agent0_Train_MaxReturn : 30.911710739135742
Agent0_Train_MinReturn : -120.82566833496094
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 168000
Agent0_TimeSinceStart : 579.0003705024719
Agent0_Critic_Loss : 1.664968490600586
Agent0_Actor_Loss : -4.1402692794799805
Agent0_Alpha_Loss : 4.818505764007568
Agent0_Temperature : 0.4835441910433208
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -41.23771286010742
Agent1_Eval_StdReturn : 30.839698791503906
Agent1_Eval_MaxReturn : 21.649885177612305
Agent1_Eval_MinReturn : -84.48469543457031
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.53201675415039
Agent1_Train_StdReturn : 33.82197189331055
Agent1_Train_MaxReturn : 14.576350212097168
Agent1_Train_MinReturn : -92.84953308105469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 168000
Agent1_TimeSinceStart : 581.7906363010406
Agent1_Critic_Loss : 1.7862646579742432
Agent1_Actor_Loss : -4.215318202972412
Agent1_Alpha_Loss : 4.819421768188477
Agent1_Temperature : 0.4835457734713129
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -39.711585998535156
Agent0_Eval_StdReturn : 25.590147018432617
Agent0_Eval_MaxReturn : -10.286764144897461
Agent0_Eval_MinReturn : -83.06344604492188
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -49.26465606689453
Agent0_Train_StdReturn : 21.776142120361328
Agent0_Train_MaxReturn : -7.627303123474121
Agent0_Train_MinReturn : -93.67977905273438
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 169500
Agent0_TimeSinceStart : 584.581999540329
Agent0_Critic_Loss : 1.5903011560440063
Agent0_Actor_Loss : -4.257002830505371
Agent0_Alpha_Loss : 4.833047866821289
Agent0_Temperature : 0.4834009920979794
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -46.70977020263672
Agent1_Eval_StdReturn : 29.996553421020508
Agent1_Eval_MaxReturn : 10.616153717041016
Agent1_Eval_MinReturn : -88.03215026855469
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -40.665565490722656
Agent1_Train_StdReturn : 40.72394561767578
Agent1_Train_MaxReturn : 22.80843734741211
Agent1_Train_MinReturn : -86.3768310546875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 169500
Agent1_TimeSinceStart : 587.3892388343811
Agent1_Critic_Loss : 1.776934266090393
Agent1_Actor_Loss : -4.2964396476745605
Agent1_Alpha_Loss : 4.8746161460876465
Agent1_Temperature : 0.4834024082368733
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -41.907325744628906
Agent0_Eval_StdReturn : 28.232126235961914
Agent0_Eval_MaxReturn : -0.5753874778747559
Agent0_Eval_MinReturn : -86.97875213623047
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -40.144927978515625
Agent0_Train_StdReturn : 36.63536834716797
Agent0_Train_MaxReturn : 26.282880783081055
Agent1_TimeSinceStart : 548.0242550373077
Agent1_Critic_Loss : 0.3307674825191498
Agent1_Actor_Loss : -0.6553260684013367
Agent1_Alpha_Loss : 0.8733457326889038
Agent1_Temperature : 0.09696532203142487
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -16.79124641418457
Agent0_Eval_StdReturn : 7.710488796234131
Agent0_Eval_MaxReturn : -6.34317684173584
Agent0_Eval_MinReturn : -32.927494049072266
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -25.977188110351562
Agent0_Train_StdReturn : 14.326340675354004
Agent0_Train_MaxReturn : -3.6660218238830566
Agent0_Train_MinReturn : -47.57481384277344
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 157500
Agent0_TimeSinceStart : 550.8451645374298
Agent0_Critic_Loss : 0.40713706612586975
Agent0_Actor_Loss : -0.31227046251296997
Agent0_Alpha_Loss : 0.8398833274841309
Agent0_Temperature : 0.09693629494077739
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -20.48626708984375
Agent1_Eval_StdReturn : 19.275299072265625
Agent1_Eval_MaxReturn : 23.63834571838379
Agent1_Eval_MinReturn : -40.483070373535156
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -13.31785774230957
Agent1_Train_StdReturn : 17.46311378479004
Agent1_Train_MaxReturn : 18.562166213989258
Agent1_Train_MinReturn : -41.36109161376953
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 157500
Agent1_TimeSinceStart : 553.6552300453186
Agent1_Critic_Loss : 0.3470046818256378
Agent1_Actor_Loss : -0.6165245771408081
Agent1_Alpha_Loss : 0.8723210096359253
Agent1_Temperature : 0.09693819964507958
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -21.92613410949707
Agent0_Eval_StdReturn : 14.750090599060059
Agent0_Eval_MaxReturn : 3.136147975921631
Agent0_Eval_MinReturn : -47.2425537109375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -14.95714282989502
Agent0_Train_StdReturn : 19.799747467041016
Agent0_Train_MaxReturn : 24.229557037353516
Agent0_Train_MinReturn : -46.61305236816406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 159000
Agent0_TimeSinceStart : 556.4709198474884
Agent0_Critic_Loss : 0.35632431507110596
Agent0_Actor_Loss : -0.4224861264228821
Agent0_Alpha_Loss : 0.8231381177902222
Agent0_Temperature : 0.09690955184717892
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -11.956745147705078
Agent1_Eval_StdReturn : 16.56682014465332
Agent1_Eval_MaxReturn : 12.697653770446777
Agent1_Eval_MinReturn : -48.20982360839844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -26.440673828125
Agent1_Train_StdReturn : 29.21976089477539
Agent1_Train_MaxReturn : 11.837883949279785
Agent1_Train_MinReturn : -92.11473083496094
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 159000
Agent1_TimeSinceStart : 559.2803707122803
Agent1_Critic_Loss : 0.37610775232315063
Agent1_Actor_Loss : -0.6468713879585266
Agent1_Alpha_Loss : 0.8557764291763306
Agent1_Temperature : 0.09691118684669506
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -12.973856925964355
Agent0_Eval_StdReturn : 23.996435165405273
Agent0_Eval_MaxReturn : 46.769832611083984
Agent0_Eval_MinReturn : -48.190948486328125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -18.082263946533203
Agent0_Train_StdReturn : 14.511202812194824
Agent0_Train_MaxReturn : 1.0038094520568848
Agent0_Train_MinReturn : -41.58303451538086
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 160500
Agent0_TimeSinceStart : 562.0939564704895
Agent0_Critic_Loss : 0.34650397300720215
Agent0_Actor_Loss : -0.47333353757858276
Agent0_Alpha_Loss : 0.8396416902542114
Agent0_Temperature : 0.09688292419005116
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -21.377559661865234
Agent1_Eval_StdReturn : 17.016393661499023
Agent1_Eval_MaxReturn : 5.949179172515869
Agent1_Eval_MinReturn : -52.40158462524414
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -23.364931106567383
Agent1_Train_StdReturn : 18.924530029296875
Agent1_Train_MaxReturn : -3.0600850582122803
Agent1_Train_MinReturn : -63.08403778076172
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 160500
Agent1_TimeSinceStart : 564.8839664459229
Agent1_Critic_Loss : 0.37753593921661377
Agent1_Actor_Loss : -0.5595992803573608
Agent1_Alpha_Loss : 0.8434008359909058
Agent1_Temperature : 0.09688430557430311
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -25.1363468170166
Agent0_Eval_StdReturn : 19.60844612121582
Agent0_Eval_MaxReturn : 18.478347778320312
Agent0_Eval_MinReturn : -52.179752349853516
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -15.142587661743164
Agent0_Train_StdReturn : 7.752534866333008
Agent0_Train_MaxReturn : 1.8099982738494873
Agent0_Train_MinReturn : -27.262178421020508
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 162000
Agent0_TimeSinceStart : 567.7104043960571
Agent0_Critic_Loss : 0.3920467495918274
Agent0_Actor_Loss : -0.4954432249069214
Agent0_Alpha_Loss : 0.8134956955909729
Agent0_Temperature : 0.09685647171230749
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -20.983154296875
Agent1_Eval_StdReturn : 15.764581680297852
Agent1_Eval_MaxReturn : 0.01649951934814453
Agent1_Eval_MinReturn : -48.03563690185547
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -13.844965934753418
Agent1_Train_StdReturn : 12.159614562988281
Agent1_Train_MaxReturn : 13.162700653076172
Agent1_Train_MinReturn : -35.06797790527344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 162000
Agent1_TimeSinceStart : 570.5316352844238
Agent1_Critic_Loss : 0.3582359552383423
Agent1_Actor_Loss : -0.5276827216148376
Agent1_Alpha_Loss : 0.8458018898963928
Agent1_Temperature : 0.09685753433879911
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -13.580917358398438
Agent0_Eval_StdReturn : 15.335174560546875
Agent0_Eval_MaxReturn : 8.397500038146973
Agent0_Eval_MinReturn : -35.12921142578125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -24.901124954223633
Agent0_Train_StdReturn : 22.052114486694336
Agent0_Train_MaxReturn : 13.271087646484375
Agent0_Train_MinReturn : -59.24340057373047
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 163500
Agent0_TimeSinceStart : 573.3558757305145
Agent0_Train_MinReturn : -104.18516540527344
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 171000
Agent0_TimeSinceStart : 590.1946835517883
Agent0_Critic_Loss : 1.4761364459991455
Agent0_Actor_Loss : -4.291021823883057
Agent0_Alpha_Loss : 4.8472137451171875
Agent0_Temperature : 0.4832578314266055
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.61838150024414
Agent1_Eval_StdReturn : 22.717254638671875
Agent1_Eval_MaxReturn : -11.238574981689453
Agent1_Eval_MinReturn : -86.78732299804688
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -53.4924201965332
Agent1_Train_StdReturn : 36.574424743652344
Agent1_Train_MaxReturn : 14.988563537597656
Agent1_Train_MinReturn : -117.3822250366211
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 171000
Agent1_TimeSinceStart : 592.9865493774414
Agent1_Critic_Loss : 2.0672740936279297
Agent1_Actor_Loss : -4.3563714027404785
Agent1_Alpha_Loss : 4.838279724121094
Agent1_Temperature : 0.4832591180548677
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.642738342285156
Agent0_Eval_StdReturn : 26.150205612182617
Agent0_Eval_MaxReturn : 2.426422357559204
Agent0_Eval_MinReturn : -79.98956298828125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -57.198760986328125
Agent0_Train_StdReturn : 35.52669143676758
Agent0_Train_MaxReturn : -6.439095497131348
Agent0_Train_MinReturn : -124.15760803222656
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 172500
Agent0_TimeSinceStart : 595.7911672592163
Agent0_Critic_Loss : 1.6490628719329834
Agent0_Actor_Loss : -4.247748374938965
Agent0_Alpha_Loss : 4.837380886077881
Agent0_Temperature : 0.4831147343524296
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -43.40093994140625
Agent1_Eval_StdReturn : 45.07555389404297
Agent1_Eval_MaxReturn : 41.07904052734375
Agent1_Eval_MinReturn : -123.69767761230469
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -41.297096252441406
Agent1_Train_StdReturn : 19.9608097076416
Agent1_Train_MaxReturn : -9.917194366455078
Agent1_Train_MinReturn : -76.18314361572266
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 172500
Agent1_TimeSinceStart : 598.6042041778564
Agent1_Critic_Loss : 2.0001165866851807
Agent1_Actor_Loss : -4.326838493347168
Agent1_Alpha_Loss : 4.8635382652282715
Agent1_Temperature : 0.48311583013848386
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.99891662597656
Agent0_Eval_StdReturn : 33.41022872924805
Agent0_Eval_MaxReturn : 7.151666164398193
Agent0_Eval_MinReturn : -102.82489013671875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -63.60289764404297
Agent0_Train_StdReturn : 35.77417755126953
Agent0_Train_MaxReturn : 10.495504379272461
Agent0_Train_MinReturn : -125.9238510131836
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 174000
Agent0_TimeSinceStart : 601.4428057670593
Agent0_Critic_Loss : 1.4044873714447021
Agent0_Actor_Loss : -4.27125883102417
Agent0_Alpha_Loss : 4.8428802490234375
Agent0_Temperature : 0.48297168226559517
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -30.50015640258789
Agent1_Eval_StdReturn : 33.219886779785156
Agent1_Eval_MaxReturn : 20.869277954101562
Agent1_Eval_MinReturn : -78.32551574707031
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -55.10050582885742
Agent1_Train_StdReturn : 32.33082962036133
Agent1_Train_MaxReturn : -5.871969223022461
Agent1_Train_MinReturn : -105.77976989746094
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 174000
Agent1_TimeSinceStart : 604.2300970554352
Agent1_Critic_Loss : 1.5403703451156616
Agent1_Actor_Loss : -4.373828887939453
Agent1_Alpha_Loss : 4.829563140869141
Agent1_Temperature : 0.4829726387600757
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -40.57907485961914
Agent0_Eval_StdReturn : 35.74624252319336
Agent0_Eval_MaxReturn : 2.5155959129333496
Agent0_Eval_MinReturn : -111.40232849121094
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -30.899587631225586
Agent0_Train_StdReturn : 28.793922424316406
Agent0_Train_MaxReturn : 39.46641540527344
Agent0_Train_MinReturn : -65.24308776855469
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 175500
Agent0_TimeSinceStart : 606.9854183197021
Agent0_Critic_Loss : 1.8121492862701416
Agent0_Actor_Loss : -4.144814491271973
Agent0_Alpha_Loss : 4.826724052429199
Agent0_Temperature : 0.48282871680880435
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -52.06147384643555
Agent1_Eval_StdReturn : 29.832290649414062
Agent1_Eval_MaxReturn : -6.1642656326293945
Agent1_Eval_MinReturn : -89.30908203125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.65534591674805
Agent1_Train_StdReturn : 29.45162010192871
Agent1_Train_MaxReturn : -10.581243515014648
Agent1_Train_MinReturn : -102.69883728027344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 175500
Agent1_TimeSinceStart : 609.6864845752716
Agent1_Critic_Loss : 1.8143458366394043
Agent1_Actor_Loss : -4.307553291320801
Agent1_Alpha_Loss : 4.816994667053223
Agent1_Temperature : 0.48282957048991887
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -38.882564544677734
Agent0_Eval_StdReturn : 29.927043914794922
Agent0_Eval_MaxReturn : 18.097742080688477
Agent0_Eval_MinReturn : -81.62742614746094
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -37.8758659362793
Agent0_Train_StdReturn : 19.246339797973633
Agent0_Train_MaxReturn : -7.866363525390625
Agent0_Train_MinReturn : -67.883056640625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 177000
Agent0_TimeSinceStart : 612.3955719470978
Agent0_Critic_Loss : 1.7162599563598633
Agent0_Actor_Loss : -4.092131614685059
Agent0_Alpha_Loss : 4.819299697875977
Agent0_Temperature : 0.48268585164422884
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.435752868652344
Agent1_Eval_StdReturn : 27.305288314819336
Agent1_Eval_MaxReturn : 2.786160469055176
Agent1_Eval_MinReturn : -97.84416198730469
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -52.291961669921875
Agent1_Train_StdReturn : 39.16746520996094
Agent1_Train_MaxReturn : 3.374728202819824
Agent1_Train_MinReturn : -126.86166381835938

Agent0_Critic_Loss : 0.3809281587600708
Agent0_Actor_Loss : -0.49150824546813965
Agent0_Alpha_Loss : 0.8122986555099487
Agent0_Temperature : 0.09683017820245923
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -25.000110626220703
Agent1_Eval_StdReturn : 21.71042251586914
Agent1_Eval_MaxReturn : 3.466360092163086
Agent1_Eval_MinReturn : -68.9949951171875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -26.59176254272461
Agent1_Train_StdReturn : 15.42752456665039
Agent1_Train_MaxReturn : -5.743538856506348
Agent1_Train_MinReturn : -53.75375747680664
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 163500
Agent1_TimeSinceStart : 576.17715883255
Agent1_Critic_Loss : 0.3792639970779419
Agent1_Actor_Loss : -0.5147063136100769
Agent1_Alpha_Loss : 0.8370230793952942
Agent1_Temperature : 0.0968308850417123
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -11.21804141998291
Agent0_Eval_StdReturn : 10.537177085876465
Agent0_Eval_MaxReturn : 4.117192268371582
Agent0_Eval_MinReturn : -29.69736099243164
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -20.996185302734375
Agent0_Train_StdReturn : 11.129687309265137
Agent0_Train_MaxReturn : -6.825129985809326
Agent0_Train_MinReturn : -37.33131408691406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 165000
Agent0_TimeSinceStart : 579.0068092346191
Agent0_Critic_Loss : 0.3808196783065796
Agent0_Actor_Loss : -0.4880962371826172
Agent0_Alpha_Loss : 0.8147912621498108
Agent0_Temperature : 0.09680401871209234
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.588809967041016
Agent1_Eval_StdReturn : 12.847780227661133
Agent1_Eval_MaxReturn : -1.887354850769043
Agent1_Eval_MinReturn : -44.618011474609375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -22.93955421447754
Agent1_Train_StdReturn : 17.5985164642334
Agent1_Train_MaxReturn : 8.107617378234863
Agent1_Train_MinReturn : -49.74695587158203
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 165000
Agent1_TimeSinceStart : 581.8676421642303
Agent1_Critic_Loss : 0.3213353753089905
Agent1_Actor_Loss : -0.5644720792770386
Agent1_Alpha_Loss : 0.8489542603492737
Agent1_Temperature : 0.09680431043475525
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -25.107519149780273
Agent0_Eval_StdReturn : 22.99742317199707
Agent0_Eval_MaxReturn : 2.676743507385254
Agent0_Eval_MinReturn : -71.18184661865234
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -26.632671356201172
Agent0_Train_StdReturn : 15.847027778625488
Agent0_Train_MaxReturn : -3.078672409057617
Agent0_Train_MinReturn : -54.962188720703125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 166500
Agent0_TimeSinceStart : 584.692137002945
Agent0_Critic_Loss : 0.3889181315898895
Agent0_Actor_Loss : -0.4515151083469391
Agent0_Alpha_Loss : 0.7753526568412781
Agent0_Temperature : 0.09677808905320881
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -16.586463928222656
Agent1_Eval_StdReturn : 12.11883544921875
Agent1_Eval_MaxReturn : 0.7022956609725952
Agent1_Eval_MinReturn : -35.161216735839844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -19.930614471435547
Agent1_Train_StdReturn : 14.06433391571045
Agent1_Train_MaxReturn : 10.293148040771484
Agent1_Train_MinReturn : -40.87436294555664
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 166500
Agent1_TimeSinceStart : 587.5091435909271
Agent1_Critic_Loss : 0.30000123381614685
Agent1_Actor_Loss : -0.5696359872817993
Agent1_Alpha_Loss : 0.853119969367981
Agent1_Temperature : 0.09677778989208767
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -28.86977767944336
Agent0_Eval_StdReturn : 14.206568717956543
Agent0_Eval_MaxReturn : -3.829422950744629
Agent0_Eval_MinReturn : -44.093345642089844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -24.59665298461914
Agent0_Train_StdReturn : 12.38444995880127
Agent0_Train_MaxReturn : -3.9470152854919434
Agent0_Train_MinReturn : -44.91541290283203
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 168000
Agent0_TimeSinceStart : 590.2119958400726
Agent0_Critic_Loss : 0.34922534227371216
Agent0_Actor_Loss : -0.3144069015979767
Agent0_Alpha_Loss : 0.7929254770278931
Agent0_Temperature : 0.09675231411415883
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.7669734954834
Agent1_Eval_StdReturn : 14.20536994934082
Agent1_Eval_MaxReturn : 9.58251667022705
Agent1_Eval_MinReturn : -46.44306182861328
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -23.383737564086914
Agent1_Train_StdReturn : 17.900623321533203
Agent1_Train_MaxReturn : 15.360645294189453
Agent1_Train_MinReturn : -51.4313850402832
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 168000
Agent1_TimeSinceStart : 592.9166855812073
Agent1_Critic_Loss : 0.3513861298561096
Agent1_Actor_Loss : -0.5321762561798096
Agent1_Alpha_Loss : 0.8306487202644348
Agent1_Temperature : 0.09675137960572572
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -10.53624153137207
Agent0_Eval_StdReturn : 15.92349624633789
Agent0_Eval_MaxReturn : 15.816272735595703
Agent0_Eval_MinReturn : -28.721723556518555
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -17.09225082397461
Agent0_Train_StdReturn : 13.13753604888916
Agent0_Train_MaxReturn : 8.097618103027344
Agent0_Train_MinReturn : -34.21490478515625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 169500
Agent0_TimeSinceStart : 595.6354553699493
Agent0_Critic_Loss : 0.3844767212867737
Agent0_Actor_Loss : -0.3599568009376526
Agent0_Alpha_Loss : 0.8010834455490112
Agent0_Temperature : 0.0967266530915885
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -25.643630981445312
Agent1_Eval_StdReturn : 16.053495407104492
Agent1_Eval_MaxReturn : -3.423088550567627
Agent1_Eval_MinReturn : -55.243629455566406
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -14.05284309387207
Agent1_Train_StdReturn : 8.7128267288208
Agent1_Train_MaxReturn : -1.7729382514953613
Agent1_Train_MinReturn : -32.94374084472656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 169500
Agent1_TimeSinceStart : 598.3474342823029
Agent1_Critic_Loss : 0.2933371663093567Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 177000
Agent1_TimeSinceStart : 615.1101191043854
Agent1_Critic_Loss : 1.5309478044509888
Agent1_Actor_Loss : -4.323226451873779
Agent1_Alpha_Loss : 4.839236259460449
Agent1_Temperature : 0.4826865552775084
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.91522979736328
Agent0_Eval_StdReturn : 38.15471267700195
Agent0_Eval_MaxReturn : 9.304235458374023
Agent0_Eval_MinReturn : -101.77545166015625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -54.560447692871094
Agent0_Train_StdReturn : 33.08528137207031
Agent0_Train_MaxReturn : -12.090660095214844
Agent0_Train_MinReturn : -106.84368896484375
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 178500
Agent0_TimeSinceStart : 617.8359332084656
Agent0_Critic_Loss : 1.9356751441955566
Agent0_Actor_Loss : -4.17980432510376
Agent0_Alpha_Loss : 4.833500385284424
Agent0_Temperature : 0.48254304068544307
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -27.994380950927734
Agent1_Eval_StdReturn : 26.15038299560547
Agent1_Eval_MaxReturn : 18.960777282714844
Agent1_Eval_MinReturn : -83.42013549804688
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -38.106849670410156
Agent1_Train_StdReturn : 20.76534652709961
Agent1_Train_MaxReturn : -0.17723703384399414
Agent1_Train_MinReturn : -70.93082427978516
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 178500
Agent1_TimeSinceStart : 620.5807147026062
Agent1_Critic_Loss : 1.9793119430541992
Agent1_Actor_Loss : -4.305238246917725
Agent1_Alpha_Loss : 4.844328880310059
Agent1_Temperature : 0.48254357678649346
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -35.125404357910156
Agent0_Eval_StdReturn : 32.43391418457031
Agent0_Eval_MaxReturn : 17.059282302856445
Agent0_Eval_MinReturn : -83.98245239257812
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.72124481201172
Agent0_Train_StdReturn : 31.87492561340332
Agent0_Train_MaxReturn : 19.616182327270508
Agent0_Train_MinReturn : -93.01521301269531
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 180000
Agent0_TimeSinceStart : 623.308468580246
Agent0_Critic_Loss : 1.4007118940353394
Agent0_Actor_Loss : -4.104560375213623
Agent0_Alpha_Loss : 4.808139801025391
Agent0_Temperature : 0.4824003494236518
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -46.6885986328125
Agent1_Eval_StdReturn : 36.006473541259766
Agent1_Eval_MaxReturn : 3.7050256729125977
Agent1_Eval_MinReturn : -108.30056762695312
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -50.003414154052734
Agent1_Train_StdReturn : 25.875484466552734
Agent1_Train_MaxReturn : -9.827661514282227
Agent1_Train_MinReturn : -93.43578338623047
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 180000
Agent1_TimeSinceStart : 626.0335042476654
Agent1_Critic_Loss : 1.7272748947143555
Agent1_Actor_Loss : -4.27360725402832
Agent1_Alpha_Loss : 4.826417922973633
Agent1_Temperature : 0.48240068243959383
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.683013916015625
Agent0_Eval_StdReturn : 32.465850830078125
Agent0_Eval_MaxReturn : 25.69841766357422
Agent0_Eval_MinReturn : -91.03265380859375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.74716567993164
Agent0_Train_StdReturn : 35.16109848022461
Agent0_Train_MaxReturn : 21.00240135192871
Agent0_Train_MinReturn : -101.09420013427734
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 181500
Agent0_TimeSinceStart : 628.7606647014618
Agent0_Critic_Loss : 1.6755783557891846
Agent0_Actor_Loss : -4.099380016326904
Agent0_Alpha_Loss : 4.797817707061768
Agent0_Temperature : 0.48225779574347805
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -33.4486198425293
Agent1_Eval_StdReturn : 41.20616912841797
Agent1_Eval_MaxReturn : 59.528358459472656
Agent1_Eval_MinReturn : -84.76838684082031
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -40.46731948852539
Agent1_Train_StdReturn : 28.05789566040039
Agent1_Train_MaxReturn : 17.969985961914062
Agent1_Train_MinReturn : -87.12417602539062
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 181500
Agent1_TimeSinceStart : 631.497710943222
Agent1_Critic_Loss : 1.5175371170043945
Agent1_Actor_Loss : -4.305334091186523
Agent1_Alpha_Loss : 4.83242654800415
Agent1_Temperature : 0.48225785005531
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.74870300292969
Agent0_Eval_StdReturn : 28.876482009887695
Agent0_Eval_MaxReturn : -1.0467286109924316
Agent0_Eval_MinReturn : -90.7099609375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -49.350250244140625
Agent0_Train_StdReturn : 17.094837188720703
Agent0_Train_MaxReturn : -20.759349822998047
Agent0_Train_MinReturn : -74.03693389892578
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 183000
Agent0_TimeSinceStart : 634.2316763401031
Agent0_Critic_Loss : 1.5747778415679932
Agent0_Actor_Loss : -4.227191925048828
Agent0_Alpha_Loss : 4.824687957763672
Agent0_Temperature : 0.482115295155783
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.035934448242188
Agent1_Eval_StdReturn : 22.183460235595703
Agent1_Eval_MaxReturn : 17.385622024536133
Agent1_Eval_MinReturn : -51.90159606933594
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -31.56622886657715
Agent1_Train_StdReturn : 28.55374526977539
Agent1_Train_MaxReturn : 8.094076156616211
Agent1_Train_MinReturn : -88.86480712890625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 183000
Agent1_TimeSinceStart : 636.9631445407867
Agent1_Critic_Loss : 1.8578436374664307
Agent1_Actor_Loss : -4.310382843017578
Agent1_Alpha_Loss : 4.81468391418457
Agent1_Temperature : 0.4821151239175483
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -32.65876007080078
Agent0_Eval_StdReturn : 45.48939514160156
Agent0_Eval_MaxReturn : 55.048133850097656
Agent0_Eval_MinReturn : -95.10249328613281
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -27.659915924072266
Agent0_Train_StdReturn : 26.234479904174805
Agent0_Train_MaxReturn : 14.075037002563477
Agent0_Train_MinReturn : -63.8057975769043
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 184500
Agent1_Actor_Loss : -0.5933736562728882
Agent1_Alpha_Loss : 0.8363297581672668
Agent1_Temperature : 0.0967250508514369
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -25.066246032714844
Agent0_Eval_StdReturn : 14.2558012008667
Agent0_Eval_MaxReturn : -6.314515590667725
Agent0_Eval_MinReturn : -54.60944366455078
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -27.19357681274414
Agent0_Train_StdReturn : 12.338343620300293
Agent0_Train_MaxReturn : -15.157564163208008
Agent0_Train_MinReturn : -58.04191207885742
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 171000
Agent0_TimeSinceStart : 601.0884926319122
Agent0_Critic_Loss : 0.3067842125892639
Agent0_Actor_Loss : -0.2971637547016144
Agent0_Alpha_Loss : 0.8097115755081177
Agent0_Temperature : 0.09670106800889816
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -24.588132858276367
Agent1_Eval_StdReturn : 23.1926326751709
Agent1_Eval_MaxReturn : 16.4721736907959
Agent1_Eval_MinReturn : -61.613555908203125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -16.524593353271484
Agent1_Train_StdReturn : 16.72564125061035
Agent1_Train_MaxReturn : 12.559442520141602
Agent1_Train_MinReturn : -40.07919692993164
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 171000
Agent1_TimeSinceStart : 603.8233561515808
Agent1_Critic_Loss : 0.3516913652420044
Agent1_Actor_Loss : -0.5493429899215698
Agent1_Alpha_Loss : 0.8314944505691528
Agent1_Temperature : 0.09669880746806404
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -13.245590209960938
Agent0_Eval_StdReturn : 14.800371170043945
Agent0_Eval_MaxReturn : 3.758646249771118
Agent0_Eval_MinReturn : -49.34526824951172
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -23.428447723388672
Agent0_Train_StdReturn : 9.097708702087402
Agent0_Train_MaxReturn : -15.18892765045166
Agent0_Train_MinReturn : -43.07746505737305
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 172500
Agent0_TimeSinceStart : 606.5588335990906
Agent0_Critic_Loss : 0.39381611347198486
Agent0_Actor_Loss : -0.2997967004776001
Agent0_Alpha_Loss : 0.8062435388565063
Agent0_Temperature : 0.09667555911670425
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.696331024169922
Agent1_Eval_StdReturn : 17.730186462402344
Agent1_Eval_MaxReturn : -4.980025291442871
Agent1_Eval_MinReturn : -49.016563415527344
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -24.450214385986328
Agent1_Train_StdReturn : 15.542928695678711
Agent1_Train_MaxReturn : -3.175631523132324
Agent1_Train_MinReturn : -52.454376220703125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 172500
Agent1_TimeSinceStart : 609.2888009548187
Agent1_Critic_Loss : 0.37917304039001465
Agent1_Actor_Loss : -0.5131041407585144
Agent1_Alpha_Loss : 0.8197762966156006
Agent1_Temperature : 0.09667267232931429
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -15.400041580200195
Agent0_Eval_StdReturn : 14.097209930419922
Agent0_Eval_MaxReturn : 8.758538246154785
Agent0_Eval_MinReturn : -35.94178009033203
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -14.943913459777832
Agent0_Train_StdReturn : 11.51025676727295
Agent0_Train_MaxReturn : 4.307987213134766
Agent0_Train_MinReturn : -31.58326530456543
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 174000
Agent0_TimeSinceStart : 612.0470042228699
Agent0_Critic_Loss : 0.3209410011768341
Agent0_Actor_Loss : -0.35188743472099304
Agent0_Alpha_Loss : 0.8207720518112183
Agent0_Temperature : 0.09665007548196641
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -20.305267333984375
Agent1_Eval_StdReturn : 11.875748634338379
Agent1_Eval_MaxReturn : -5.9307451248168945
Agent1_Eval_MinReturn : -50.38419723510742
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -25.78434181213379
Agent1_Train_StdReturn : 15.113822937011719
Agent1_Train_MaxReturn : -10.60986042022705
Agent1_Train_MinReturn : -65.12834930419922
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 174000
Agent1_TimeSinceStart : 614.7922713756561
Agent1_Critic_Loss : 0.29739445447921753
Agent1_Actor_Loss : -0.44659262895584106
Agent1_Alpha_Loss : 0.8232784867286682
Agent1_Temperature : 0.09664662289569811
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -15.135653495788574
Agent0_Eval_StdReturn : 11.350025177001953
Agent0_Eval_MaxReturn : 8.71976089477539
Agent0_Eval_MinReturn : -33.983821868896484
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -18.649951934814453
Agent0_Train_StdReturn : 10.179974555969238
Agent0_Train_MaxReturn : -1.5953104496002197
Agent0_Train_MinReturn : -32.249839782714844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 175500
Agent0_TimeSinceStart : 617.5345933437347
Agent0_Critic_Loss : 0.3042668104171753
Agent0_Actor_Loss : -0.39897292852401733
Agent0_Alpha_Loss : 0.7778605222702026
Agent0_Temperature : 0.09662473509482922
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.896255493164062
Agent1_Eval_StdReturn : 12.970332145690918
Agent1_Eval_MaxReturn : -7.823735237121582
Agent1_Eval_MinReturn : -49.16838836669922
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -18.83237075805664
Agent1_Train_StdReturn : 15.39830493927002
Agent1_Train_MaxReturn : -0.24759554862976074
Agent1_Train_MinReturn : -48.337188720703125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 175500
Agent1_TimeSinceStart : 620.267648935318
Agent1_Critic_Loss : 0.2720853388309479
Agent1_Actor_Loss : -0.5133999586105347
Agent1_Alpha_Loss : 0.8269416093826294
Agent1_Temperature : 0.09662063847933296
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -25.529407501220703
Agent0_Eval_StdReturn : 13.202125549316406
Agent0_Eval_MaxReturn : -3.4788990020751953
Agent0_Eval_MinReturn : -40.74269104003906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -23.603242874145508
Agent0_Train_StdReturn : 18.23109245300293
Agent0_Train_MaxReturn : -2.6824283599853516
Agent0_Train_MinReturn : -62.93716049194336
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 177000
Agent0_TimeSinceStart : 622.9987440109253
Agent0_Critic_Loss : 0.3522413372993469
Agent0_Actor_Loss : -0.38245072960853577
Agent0_TimeSinceStart : 639.695155620575
Agent0_Critic_Loss : 1.45188307762146
Agent0_Actor_Loss : -4.214200496673584
Agent0_Alpha_Loss : 4.843360424041748
Agent0_Temperature : 0.48197279437932206
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -28.583526611328125
Agent1_Eval_StdReturn : 30.211580276489258
Agent1_Eval_MaxReturn : 10.461301803588867
Agent1_Eval_MinReturn : -94.61071014404297
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.2780647277832
Agent1_Train_StdReturn : 24.527685165405273
Agent1_Train_MaxReturn : -14.417737007141113
Agent1_Train_MinReturn : -107.29327392578125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 184500
Agent1_TimeSinceStart : 642.4228429794312
Agent1_Critic_Loss : 1.648236632347107
Agent1_Actor_Loss : -4.358019828796387
Agent1_Alpha_Loss : 4.805977821350098
Agent1_Temperature : 0.4819725191470232
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -48.197044372558594
Agent0_Eval_StdReturn : 34.8585319519043
Agent0_Eval_MaxReturn : 25.434818267822266
Agent0_Eval_MinReturn : -104.37539672851562
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -40.8227424621582
Agent0_Train_StdReturn : 29.85746955871582
Agent0_Train_MaxReturn : 1.0589046478271484
Agent0_Train_MinReturn : -100.50631713867188
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 186000
Agent0_TimeSinceStart : 645.1629655361176
Agent0_Critic_Loss : 1.6981613636016846
Agent0_Actor_Loss : -4.172574996948242
Agent0_Alpha_Loss : 4.840341567993164
Agent0_Temperature : 0.48183030446732056
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -41.09820556640625
Agent1_Eval_StdReturn : 16.5054931640625
Agent1_Eval_MaxReturn : -20.32887077331543
Agent1_Eval_MinReturn : -64.85346984863281
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.27387237548828
Agent1_Train_StdReturn : 32.95344924926758
Agent1_Train_MaxReturn : 24.709257125854492
Agent1_Train_MinReturn : -91.28685760498047
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 186000
Agent1_TimeSinceStart : 647.8960285186768
Agent1_Critic_Loss : 1.783799171447754
Agent1_Actor_Loss : -4.368265151977539
Agent1_Alpha_Loss : 4.820496559143066
Agent1_Temperature : 0.48182998643273517
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -60.41695022583008
Agent0_Eval_StdReturn : 33.43657302856445
Agent0_Eval_MaxReturn : 4.821619033813477
Agent0_Eval_MinReturn : -106.95550537109375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -33.53083038330078
Agent0_Train_StdReturn : 39.93160629272461
Agent0_Train_MaxReturn : 19.870960235595703
Agent0_Train_MinReturn : -101.80180358886719
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 187500
Agent0_TimeSinceStart : 650.6303629875183
Agent0_Critic_Loss : 1.41700279712677
Agent0_Actor_Loss : -4.1942853927612305
Agent0_Alpha_Loss : 4.807072639465332
Agent0_Temperature : 0.4816879170890662
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -31.15840721130371
Agent1_Eval_StdReturn : 35.46487808227539
Agent1_Eval_MaxReturn : 22.300907135009766
Agent1_Eval_MinReturn : -76.61753845214844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -50.09870529174805
Agent1_Train_StdReturn : 31.47185707092285
Agent1_Train_MaxReturn : 11.404260635375977
Agent1_Train_MinReturn : -104.35253143310547
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 187500
Agent1_TimeSinceStart : 653.3579955101013
Agent1_Critic_Loss : 1.570935845375061
Agent1_Actor_Loss : -4.45682430267334
Agent1_Alpha_Loss : 4.833850383758545
Agent1_Temperature : 0.4816874848844287
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -39.42424011230469
Agent0_Eval_StdReturn : 24.99765396118164
Agent0_Eval_MaxReturn : 1.5345802307128906
Agent0_Eval_MinReturn : -86.94596862792969
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.04111671447754
Agent0_Train_StdReturn : 27.195878982543945
Agent0_Train_MaxReturn : 13.398439407348633
Agent0_Train_MinReturn : -62.46067428588867
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 189000
Agent0_TimeSinceStart : 656.0965268611908
Agent0_Critic_Loss : 2.2384514808654785
Agent0_Actor_Loss : -4.322022438049316
Agent0_Alpha_Loss : 4.82805061340332
Agent0_Temperature : 0.4815455673743017
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.26117706298828
Agent1_Eval_StdReturn : 17.659748077392578
Agent1_Eval_MaxReturn : -1.4265720844268799
Agent1_Eval_MinReturn : -59.89907455444336
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -34.747657775878906
Agent1_Train_StdReturn : 36.11848068237305
Agent1_Train_MaxReturn : 25.39393424987793
Agent1_Train_MinReturn : -82.81035614013672
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 189000
Agent1_TimeSinceStart : 658.8201856613159
Agent1_Critic_Loss : 2.0311789512634277
Agent1_Actor_Loss : -4.455941200256348
Agent1_Alpha_Loss : 4.82219123840332
Agent1_Temperature : 0.48154504565436884
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -36.43562698364258
Agent0_Eval_StdReturn : 24.789905548095703
Agent0_Eval_MaxReturn : 10.042777061462402
Agent0_Eval_MinReturn : -67.99370574951172
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -55.3865966796875
Agent0_Train_StdReturn : 33.50897979736328
Agent0_Train_MaxReturn : -1.2375526428222656
Agent0_Train_MinReturn : -113.47250366210938
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 190500
Agent0_TimeSinceStart : 661.5519280433655
Agent0_Critic_Loss : 1.8797781467437744
Agent0_Actor_Loss : -4.256042957305908
Agent0_Alpha_Loss : 4.815569877624512
Agent0_Temperature : 0.4814032879345309
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -43.33436584472656
Agent1_Eval_StdReturn : 32.45503616333008
Agent1_Eval_MaxReturn : -0.5268115997314453
Agent1_Eval_MinReturn : -112.9184341430664
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -22.385738372802734
Agent1_Train_StdReturn : 35.32505798339844
Agent1_Train_MaxReturn : 50.111976623535156
Agent1_Train_MinReturn : -80.92728424072266
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 190500
Agent1_TimeSinceStart : 664.2757806777954
Agent1_Critic_Loss : 2.173591136932373
Agent0_Alpha_Loss : 0.801103949546814
Agent0_Temperature : 0.09659945489445972
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -25.762954711914062
Agent1_Eval_StdReturn : 9.074722290039062
Agent1_Eval_MaxReturn : -14.493616104125977
Agent1_Eval_MinReturn : -39.24110412597656
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -12.437909126281738
Agent1_Train_StdReturn : 14.916574478149414
Agent1_Train_MaxReturn : 10.96956729888916
Agent1_Train_MinReturn : -32.516719818115234
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 177000
Agent1_TimeSinceStart : 625.7369451522827
Agent1_Critic_Loss : 0.32661283016204834
Agent1_Actor_Loss : -0.47947168350219727
Agent1_Alpha_Loss : 0.8042845129966736
Agent1_Temperature : 0.09659477525181204
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -29.161035537719727
Agent0_Eval_StdReturn : 12.041325569152832
Agent0_Eval_MaxReturn : -13.697582244873047
Agent0_Eval_MinReturn : -55.334251403808594
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -20.45771026611328
Agent0_Train_StdReturn : 13.946306228637695
Agent0_Train_MaxReturn : 7.53178596496582
Agent0_Train_MinReturn : -42.247215270996094
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 178500
Agent0_TimeSinceStart : 628.4759712219238
Agent0_Critic_Loss : 0.3102923035621643
Agent0_Actor_Loss : -0.3885170817375183
Agent0_Alpha_Loss : 0.8023146986961365
Agent0_Temperature : 0.09657422338567884
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.55902671813965
Agent1_Eval_StdReturn : 15.154779434204102
Agent1_Eval_MaxReturn : -7.196774959564209
Agent1_Eval_MinReturn : -63.3553581237793
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -20.509000778198242
Agent1_Train_StdReturn : 12.688008308410645
Agent1_Train_MaxReturn : -4.194310188293457
Agent1_Train_MinReturn : -45.912689208984375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 178500
Agent1_TimeSinceStart : 631.2115080356598
Agent1_Critic_Loss : 0.3098086416721344
Agent1_Actor_Loss : -0.5099887847900391
Agent1_Alpha_Loss : 0.8115861415863037
Agent1_Temperature : 0.0965689983712433
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.288883209228516
Agent0_Eval_StdReturn : 11.609735488891602
Agent0_Eval_MaxReturn : -8.989728927612305
Agent0_Eval_MinReturn : -45.34342956542969
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -20.077016830444336
Agent0_Train_StdReturn : 8.099329948425293
Agent0_Train_MaxReturn : -4.224114894866943
Agent0_Train_MinReturn : -35.41250991821289
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 180000
Agent0_TimeSinceStart : 633.9459586143494
Agent0_Critic_Loss : 0.3481535315513611
Agent0_Actor_Loss : -0.4045906364917755
Agent0_Alpha_Loss : 0.8037174940109253
Agent0_Temperature : 0.09654902970685102
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -25.414318084716797
Agent1_Eval_StdReturn : 17.37411117553711
Agent1_Eval_MaxReturn : 1.6029844284057617
Agent1_Eval_MinReturn : -55.75908660888672
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -16.90503692626953
Agent1_Train_StdReturn : 15.816688537597656
Agent1_Train_MaxReturn : 4.572351455688477
Agent1_Train_MinReturn : -49.73101806640625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 180000
Agent1_TimeSinceStart : 636.6718509197235
Agent1_Critic_Loss : 0.2578887641429901
Agent1_Actor_Loss : -0.5093762874603271
Agent1_Alpha_Loss : 0.816374659538269
Agent1_Temperature : 0.09654328372756862
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -19.356430053710938
Agent0_Eval_StdReturn : 13.680508613586426
Agent0_Eval_MaxReturn : -5.6117353439331055
Agent0_Eval_MinReturn : -50.81719970703125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -27.5440616607666
Agent0_Train_StdReturn : 8.025374412536621
Agent0_Train_MaxReturn : -15.325693130493164
Agent0_Train_MinReturn : -39.83248519897461
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 181500
Agent0_TimeSinceStart : 639.4082071781158
Agent0_Critic_Loss : 0.29074543714523315
Agent0_Actor_Loss : -0.3491659164428711
Agent0_Alpha_Loss : 0.7953392863273621
Agent0_Temperature : 0.09652389218437084
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -18.535160064697266
Agent1_Eval_StdReturn : 12.05312442779541
Agent1_Eval_MaxReturn : -4.514222621917725
Agent1_Eval_MinReturn : -39.047969818115234
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -18.187387466430664
Agent1_Train_StdReturn : 15.771614074707031
Agent1_Train_MaxReturn : 5.4027791023254395
Agent1_Train_MinReturn : -55.94032287597656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 181500
Agent1_TimeSinceStart : 642.1324167251587
Agent1_Critic_Loss : 0.3290644884109497
Agent1_Actor_Loss : -0.4863255023956299
Agent1_Alpha_Loss : 0.8267827033996582
Agent1_Temperature : 0.09651759366741956
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.193655014038086
Agent0_Eval_StdReturn : 22.387924194335938
Agent0_Eval_MaxReturn : -4.020785808563232
Agent0_Eval_MinReturn : -82.96681213378906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -24.029438018798828
Agent0_Train_StdReturn : 10.685930252075195
Agent0_Train_MaxReturn : -8.376558303833008
Agent0_Train_MinReturn : -38.673927307128906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 183000
Agent0_TimeSinceStart : 644.8584749698639
Agent0_Critic_Loss : 0.28091520071029663
Agent0_Actor_Loss : -0.3433712124824524
Agent0_Alpha_Loss : 0.797012984752655
Agent0_Temperature : 0.09649879838498951
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.550922393798828
Agent1_Eval_StdReturn : 12.56862735748291
Agent1_Eval_MaxReturn : 0.9180469512939453
Agent1_Eval_MinReturn : -39.74037170410156
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -17.295263290405273
Agent1_Train_StdReturn : 15.381983757019043
Agent1_Train_MaxReturn : 0.9582653045654297
Agent1_Train_MinReturn : -43.61455535888672
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 183000
Agent1_TimeSinceStart : 647.5929772853851
Agent1_Critic_Loss : 0.36543649435043335
Agent1_Actor_Loss : -0.5211077332496643
Agent1_Alpha_Loss : 0.809587836265564
Agent1_Actor_Loss : -4.314418792724609
Agent1_Alpha_Loss : 4.803481101989746
Agent1_Temperature : 0.4814027156515418
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -52.25176239013672
Agent0_Eval_StdReturn : 34.93877029418945
Agent0_Eval_MaxReturn : -15.1241455078125
Agent0_Eval_MinReturn : -117.46551513671875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.810428619384766
Agent0_Train_StdReturn : 29.479583740234375
Agent0_Train_MaxReturn : -3.099472999572754
Agent0_Train_MinReturn : -112.85335540771484
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 192000
Agent0_TimeSinceStart : 667.0178668498993
Agent0_Critic_Loss : 1.6779828071594238
Agent0_Actor_Loss : -4.312710285186768
Agent0_Alpha_Loss : 4.839958190917969
Agent0_Temperature : 0.4812610080855156
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -19.95180892944336
Agent1_Eval_StdReturn : 38.94794464111328
Agent1_Eval_MaxReturn : 30.153366088867188
Agent1_Eval_MinReturn : -72.65635681152344
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -39.20161819458008
Agent1_Train_StdReturn : 30.340572357177734
Agent1_Train_MaxReturn : 0.7292251586914062
Agent1_Train_MinReturn : -101.77131652832031
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 192000
Agent1_TimeSinceStart : 669.753279209137
Agent1_Critic_Loss : 1.770193338394165
Agent1_Actor_Loss : -4.401678085327148
Agent1_Alpha_Loss : 4.829189777374268
Agent1_Temperature : 0.48126041645530704
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -34.45241165161133
Agent0_Eval_StdReturn : 39.89638900756836
Agent0_Eval_MaxReturn : 36.53508377075195
Agent0_Eval_MinReturn : -91.09259796142578
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.056957244873047
Agent0_Train_StdReturn : 20.41046905517578
Agent0_Train_MaxReturn : 5.046857833862305
Agent0_Train_MinReturn : -66.64897155761719
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 193500
Agent0_TimeSinceStart : 672.5026097297668
Agent0_Critic_Loss : 1.6447505950927734
Agent0_Actor_Loss : -4.226034641265869
Agent0_Alpha_Loss : 4.835471153259277
Agent0_Temperature : 0.48111874293453644
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -45.697837829589844
Agent1_Eval_StdReturn : 29.300031661987305
Agent1_Eval_MaxReturn : 13.782768249511719
Agent1_Eval_MinReturn : -78.16277313232422
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -34.7608528137207
Agent1_Train_StdReturn : 30.139671325683594
Agent1_Train_MaxReturn : 26.678390502929688
Agent1_Train_MinReturn : -77.08141326904297
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 193500
Agent1_TimeSinceStart : 675.2491872310638
Agent1_Critic_Loss : 1.5649230480194092
Agent1_Actor_Loss : -4.307384490966797
Agent1_Alpha_Loss : 4.8222455978393555
Agent1_Temperature : 0.4811181664981417
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -53.743736267089844
Agent0_Eval_StdReturn : 36.49790954589844
Agent0_Eval_MaxReturn : 7.287776947021484
Agent0_Eval_MinReturn : -127.68640899658203
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -21.4764404296875
Agent0_Train_StdReturn : 37.590702056884766
Agent0_Train_MaxReturn : 25.476863861083984
Agent0_Train_MinReturn : -107.89973449707031
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 195000
Agent0_TimeSinceStart : 678.0326523780823
Agent0_Critic_Loss : 1.6262298822402954
Agent0_Actor_Loss : -4.277443885803223
Agent0_Alpha_Loss : 4.8116912841796875
Agent0_Temperature : 0.4809765582732415
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -46.35879135131836
Agent1_Eval_StdReturn : 29.416399002075195
Agent1_Eval_MaxReturn : -10.429815292358398
Agent1_Eval_MinReturn : -93.25064086914062
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -22.902263641357422
Agent1_Train_StdReturn : 21.239179611206055
Agent1_Train_MaxReturn : 24.56534194946289
Agent1_Train_MinReturn : -56.51329040527344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 195000
Agent1_TimeSinceStart : 680.8345510959625
Agent1_Critic_Loss : 1.7643258571624756
Agent1_Actor_Loss : -4.383423328399658
Agent1_Alpha_Loss : 4.827702522277832
Agent1_Temperature : 0.48097594864228294
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -34.77933883666992
Agent0_Eval_StdReturn : 30.18395233154297
Agent0_Eval_MaxReturn : -0.4381479024887085
Agent0_Eval_MinReturn : -101.60063171386719
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -53.18617630004883
Agent0_Train_StdReturn : 21.347131729125977
Agent0_Train_MaxReturn : -24.02076530456543
Agent0_Train_MinReturn : -93.07177734375
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 196500
Agent0_TimeSinceStart : 683.726203918457
Agent0_Critic_Loss : 1.781274437904358
Agent0_Actor_Loss : -4.2487592697143555
Agent0_Alpha_Loss : 4.797377586364746
Agent0_Temperature : 0.48083448721058947
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -36.38129425048828
Agent1_Eval_StdReturn : 26.866653442382812
Agent1_Eval_MaxReturn : -0.030519485473632812
Agent1_Eval_MinReturn : -75.49723815917969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -47.09062576293945
Agent1_Train_StdReturn : 34.99156951904297
Agent1_Train_MaxReturn : 12.168996810913086
Agent1_Train_MinReturn : -102.8428955078125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 196500
Agent1_TimeSinceStart : 686.6015870571136
Agent1_Critic_Loss : 1.5560016632080078
Agent1_Actor_Loss : -4.301673889160156
Agent1_Alpha_Loss : 4.8177170753479
Agent1_Temperature : 0.4808337894640573
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.480804443359375
Agent0_Eval_StdReturn : 22.898361206054688
Agent0_Eval_MaxReturn : -9.182628631591797
Agent0_Eval_MinReturn : -83.18231201171875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -52.465057373046875
Agent0_Train_StdReturn : 29.128232955932617
Agent0_Train_MaxReturn : -8.138216018676758
Agent0_Train_MinReturn : -105.68659973144531
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 198000
Agent0_TimeSinceStart : 689.4839987754822
Agent0_Critic_Loss : 1.4998325109481812
Agent0_Actor_Loss : -4.266501426696777
Agent0_Alpha_Loss : 4.802281379699707
Agent1_Temperature : 0.09649197326924869
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -17.843036651611328
Agent0_Eval_StdReturn : 11.68476676940918
Agent0_Eval_MaxReturn : 3.4561452865600586
Agent0_Eval_MinReturn : -32.65001678466797
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -25.988296508789062
Agent0_Train_StdReturn : 19.475154876708984
Agent0_Train_MaxReturn : 12.349791526794434
Agent0_Train_MinReturn : -69.72599029541016
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 184500
Agent0_TimeSinceStart : 650.3326075077057
Agent0_Critic_Loss : 0.3180795907974243
Agent0_Actor_Loss : -0.3300529718399048
Agent0_Alpha_Loss : 0.8155755996704102
Agent0_Temperature : 0.09647368857952468
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.710018157958984
Agent1_Eval_StdReturn : 13.973886489868164
Agent1_Eval_MaxReturn : -3.8272199630737305
Agent1_Eval_MinReturn : -47.86601638793945
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -14.891420364379883
Agent1_Train_StdReturn : 7.407113075256348
Agent1_Train_MaxReturn : 2.7733006477355957
Agent1_Train_MinReturn : -25.386293411254883
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 184500
Agent1_TimeSinceStart : 653.0724918842316
Agent1_Critic_Loss : 0.2995407283306122
Agent1_Actor_Loss : -0.5329626798629761
Agent1_Alpha_Loss : 0.8196921348571777
Agent1_Temperature : 0.09646638486398135
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -19.404199600219727
Agent0_Eval_StdReturn : 7.010708808898926
Agent0_Eval_MaxReturn : -9.04991626739502
Agent0_Eval_MinReturn : -32.934608459472656
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -18.26816749572754
Agent0_Train_StdReturn : 9.919441223144531
Agent0_Train_MaxReturn : 1.8932204246520996
Agent0_Train_MinReturn : -30.517292022705078
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 186000
Agent0_TimeSinceStart : 655.8289113044739
Agent0_Critic_Loss : 0.2793738543987274
Agent0_Actor_Loss : -0.3222479522228241
Agent0_Alpha_Loss : 0.7927098274230957
Agent0_Temperature : 0.09644862853391312
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -15.674962997436523
Agent1_Eval_StdReturn : 18.514015197753906
Agent1_Eval_MaxReturn : 18.041439056396484
Agent1_Eval_MinReturn : -54.50547790527344
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -23.95391273498535
Agent1_Train_StdReturn : 12.065469741821289
Agent1_Train_MaxReturn : -2.20963191986084
Agent1_Train_MinReturn : -52.53404235839844
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 186000
Agent1_TimeSinceStart : 658.5990016460419
Agent1_Critic_Loss : 0.28738948702812195
Agent1_Actor_Loss : -0.530052661895752
Agent1_Alpha_Loss : 0.8082102537155151
Agent1_Temperature : 0.09644085652656034
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -23.045156478881836
Agent0_Eval_StdReturn : 21.557844161987305
Agent0_Eval_MaxReturn : 23.316299438476562
Agent0_Eval_MinReturn : -59.30779266357422
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -19.49697494506836
Agent0_Train_StdReturn : 26.71527099609375
Agent0_Train_MaxReturn : 20.09103775024414
Agent0_Train_MinReturn : -76.48178100585938
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 187500
Agent0_TimeSinceStart : 661.3494665622711
Agent0_Critic_Loss : 0.2827101945877075
Agent0_Actor_Loss : -0.3153386116027832
Agent0_Alpha_Loss : 0.8103842735290527
Agent0_Temperature : 0.0964235603132318
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -36.955589294433594
Agent1_Eval_StdReturn : 22.1213321685791
Agent1_Eval_MaxReturn : -6.774964332580566
Agent1_Eval_MinReturn : -74.431884765625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -25.762792587280273
Agent1_Train_StdReturn : 12.076983451843262
Agent1_Train_MaxReturn : 0.7268276214599609
Agent1_Train_MinReturn : -43.257083892822266
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 187500
Agent1_TimeSinceStart : 664.2540996074677
Agent1_Critic_Loss : 0.2565898895263672
Agent1_Actor_Loss : -0.49875369668006897
Agent1_Alpha_Loss : 0.8381680250167847
Agent1_Temperature : 0.09641529459427714
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -14.595613479614258
Agent0_Eval_StdReturn : 19.649194717407227
Agent0_Eval_MaxReturn : 19.035858154296875
Agent0_Eval_MinReturn : -53.11669921875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -23.95199966430664
Agent0_Train_StdReturn : 15.550067901611328
Agent0_Train_MaxReturn : 10.438608169555664
Agent0_Train_MinReturn : -46.357643127441406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 189000
Agent0_TimeSinceStart : 667.1556913852692
Agent0_Critic_Loss : 0.32858604192733765
Agent0_Actor_Loss : -0.403328537940979
Agent0_Alpha_Loss : 0.8113445043563843
Agent0_Temperature : 0.09639848024845499
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -18.192407608032227
Agent1_Eval_StdReturn : 16.764223098754883
Agent1_Eval_MaxReturn : 24.96856689453125
Agent1_Eval_MinReturn : -34.26313781738281
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -17.491260528564453
Agent1_Train_StdReturn : 13.222405433654785
Agent1_Train_MaxReturn : 5.7276692390441895
Agent1_Train_MinReturn : -41.247314453125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 189000
Agent1_TimeSinceStart : 670.0428595542908
Agent1_Critic_Loss : 0.2746155261993408
Agent1_Actor_Loss : -0.4839022159576416
Agent1_Alpha_Loss : 0.8367208242416382
Agent1_Temperature : 0.0963897052610467
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -13.489919662475586
Agent0_Eval_StdReturn : 14.134404182434082
Agent0_Eval_MaxReturn : 18.478954315185547
Agent0_Eval_MinReturn : -31.440824508666992
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -30.162029266357422
Agent0_Train_StdReturn : 16.68223762512207
Agent0_Train_MaxReturn : -3.7959203720092773
Agent0_Train_MinReturn : -61.32156753540039
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 190500
Agent0_TimeSinceStart : 672.933753490448
Agent0_Critic_Loss : 0.2565564513206482
Agent0_Actor_Loss : -0.3794867992401123
Agent0_Alpha_Loss : 0.8190950155258179
Agent0_Temperature : 0.09637336546319623
Agent0_Temperature : 0.48069250714647366
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -19.47395133972168
Agent1_Eval_StdReturn : 27.433565139770508
Agent1_Eval_MaxReturn : 27.928789138793945
Agent1_Eval_MinReturn : -53.581138610839844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -18.998138427734375
Agent1_Train_StdReturn : 31.9516658782959
Agent1_Train_MaxReturn : 43.973358154296875
Agent1_Train_MinReturn : -68.81224822998047
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 198000
Agent1_TimeSinceStart : 692.3452701568604
Agent1_Critic_Loss : 1.54887056350708
Agent1_Actor_Loss : -4.322463035583496
Agent1_Alpha_Loss : 4.833337306976318
Agent1_Temperature : 0.4806916432502119
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -51.799537658691406
Agent0_Eval_StdReturn : 25.439414978027344
Agent0_Eval_MaxReturn : -5.720248222351074
Agent0_Eval_MinReturn : -91.97789764404297
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -50.42164993286133
Agent0_Train_StdReturn : 37.06306076049805
Agent0_Train_MaxReturn : -14.478403091430664
Agent0_Train_MinReturn : -133.75083923339844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 199500
Agent0_TimeSinceStart : 695.2007586956024
Agent0_Critic_Loss : 1.5381147861480713
Agent0_Actor_Loss : -4.214103698730469
Agent0_Alpha_Loss : 4.769874572753906
Agent0_Temperature : 0.48055069917670074
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.97251319885254
Agent1_Eval_StdReturn : 27.957904815673828
Agent1_Eval_MaxReturn : 21.747211456298828
Agent1_Eval_MinReturn : -78.7813720703125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -43.90883255004883
Agent1_Train_StdReturn : 17.85492515563965
Agent1_Train_MaxReturn : -7.933300495147705
Agent1_Train_MinReturn : -70.42538452148438
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 199500
Agent1_TimeSinceStart : 698.0560464859009
Agent1_Critic_Loss : 1.8773081302642822
Agent1_Actor_Loss : -4.38285493850708
Agent1_Alpha_Loss : 4.790072441101074
Agent1_Temperature : 0.4805496290056458
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -31.531005859375
Agent0_Eval_StdReturn : 36.534828186035156
Agent0_Eval_MaxReturn : 54.77436828613281
Agent0_Eval_MinReturn : -67.78878784179688
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -47.248451232910156
Agent0_Train_StdReturn : 32.17799758911133
Agent0_Train_MaxReturn : 1.9846973419189453
Agent0_Train_MinReturn : -89.62419128417969
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 201000
Agent0_TimeSinceStart : 700.909503698349
Agent0_Critic_Loss : 1.4063904285430908
Agent0_Actor_Loss : -4.321203231811523
Agent0_Alpha_Loss : 4.829385757446289
Agent0_Temperature : 0.48040888601543225
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -41.26472473144531
Agent1_Eval_StdReturn : 40.890995025634766
Agent1_Eval_MaxReturn : 25.436382293701172
Agent1_Eval_MinReturn : -112.21128845214844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -21.930408477783203
Agent1_Train_StdReturn : 30.15529441833496
Agent1_Train_MaxReturn : 26.83474349975586
Agent1_Train_MinReturn : -65.81507873535156
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 201000
Agent1_TimeSinceStart : 703.7740244865417
Agent1_Critic_Loss : 1.721333384513855
Agent1_Actor_Loss : -4.3296051025390625
Agent1_Alpha_Loss : 4.790339469909668
Agent1_Temperature : 0.4804077347941213
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.99028396606445
Agent0_Eval_StdReturn : 25.359615325927734
Agent0_Eval_MaxReturn : -1.9223392009735107
Agent0_Eval_MinReturn : -92.53663635253906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -32.439998626708984
Agent0_Train_StdReturn : 27.283403396606445
Agent0_Train_MaxReturn : 2.2824764251708984
Agent0_Train_MinReturn : -71.29653930664062
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 202500
Agent0_TimeSinceStart : 706.6371080875397
Agent0_Critic_Loss : 1.5879971981048584
Agent0_Actor_Loss : -4.360903739929199
Agent0_Alpha_Loss : 4.813774585723877
Agent0_Temperature : 0.48026711340848754
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -34.46408462524414
Agent1_Eval_StdReturn : 25.970163345336914
Agent1_Eval_MaxReturn : -4.1926164627075195
Agent1_Eval_MinReturn : -76.12637329101562
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -48.3524169921875
Agent1_Train_StdReturn : 22.85944366455078
Agent1_Train_MaxReturn : 3.4720516204833984
Agent1_Train_MinReturn : -80.59181213378906
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 202500
Agent1_TimeSinceStart : 709.4945478439331
Agent1_Critic_Loss : 1.9032820463180542
Agent1_Actor_Loss : -4.455956935882568
Agent1_Alpha_Loss : 4.805204391479492
Agent1_Temperature : 0.48026591023910054
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -38.77257537841797
Agent0_Eval_StdReturn : 22.762550354003906
Agent0_Eval_MaxReturn : -2.3758156299591064
Agent0_Eval_MinReturn : -79.15034484863281
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -55.5206298828125
Agent0_Train_StdReturn : 27.331151962280273
Agent0_Train_MaxReturn : -13.817174911499023
Agent0_Train_MinReturn : -86.0819320678711
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 204000
Agent0_TimeSinceStart : 712.3445837497711
Agent0_Critic_Loss : 1.7024943828582764
Agent0_Actor_Loss : -4.374022483825684
Agent0_Alpha_Loss : 4.814894676208496
Agent0_Temperature : 0.48012537677515227
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.407798767089844
Agent1_Eval_StdReturn : 32.269317626953125
Agent1_Eval_MaxReturn : 7.189520835876465
Agent1_Eval_MinReturn : -91.788818359375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -48.60988998413086
Agent1_Train_StdReturn : 25.803802490234375
Agent1_Train_MaxReturn : -11.922918319702148
Agent1_Train_MinReturn : -84.02413940429688
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 204000
Agent1_TimeSinceStart : 715.1964933872223
Agent1_Critic_Loss : 2.1226844787597656
Agent1_Actor_Loss : -4.425215721130371
Agent1_Alpha_Loss : 4.816746234893799
Agent1_Temperature : 0.4801241193623618
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.5115966796875
Agent1_Eval_StdReturn : 17.854772567749023
Agent1_Eval_MaxReturn : 5.040260314941406
Agent1_Eval_MinReturn : -57.53917694091797
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -26.476537704467773
Agent1_Train_StdReturn : 14.335918426513672
Agent1_Train_MaxReturn : 1.631089210510254
Agent1_Train_MinReturn : -46.95075225830078
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 190500
Agent1_TimeSinceStart : 675.8150475025177
Agent1_Critic_Loss : 0.3075826168060303
Agent1_Actor_Loss : -0.4811796545982361
Agent1_Alpha_Loss : 0.8277421593666077
Agent1_Temperature : 0.09636411573076727
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -22.973594665527344
Agent0_Eval_StdReturn : 15.390305519104004
Agent0_Eval_MaxReturn : 0.6544361114501953
Agent0_Eval_MinReturn : -59.91438293457031
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -21.046661376953125
Agent0_Train_StdReturn : 21.245325088500977
Agent0_Train_MaxReturn : 1.5475702285766602
Agent0_Train_MinReturn : -61.27727127075195
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 192000
Agent0_TimeSinceStart : 678.7015552520752
Agent0_Critic_Loss : 0.2601712942123413
Agent0_Actor_Loss : -0.41879329085350037
Agent0_Alpha_Loss : 0.8280042409896851
Agent0_Temperature : 0.09634819215160753
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.29214859008789
Agent1_Eval_StdReturn : 11.968153953552246
Agent1_Eval_MaxReturn : -9.734415054321289
Agent1_Eval_MinReturn : -48.960693359375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -22.40981101989746
Agent1_Train_StdReturn : 10.951517105102539
Agent1_Train_MaxReturn : 0.05490344762802124
Agent1_Train_MinReturn : -43.48713684082031
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 192000
Agent1_TimeSinceStart : 681.5742452144623
Agent1_Critic_Loss : 0.30333924293518066
Agent1_Actor_Loss : -0.4786439538002014
Agent1_Alpha_Loss : 0.8217523097991943
Agent1_Temperature : 0.09633854183157696
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -21.656124114990234
Agent0_Eval_StdReturn : 11.81795883178711
Agent0_Eval_MaxReturn : -1.176870346069336
Agent0_Eval_MinReturn : -43.428741455078125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -18.363473892211914
Agent0_Train_StdReturn : 14.262819290161133
Agent0_Train_MaxReturn : -0.22874271869659424
Agent0_Train_MinReturn : -40.09243392944336
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 193500
Agent0_TimeSinceStart : 684.4522230625153
Agent0_Critic_Loss : 0.3089245557785034
Agent0_Actor_Loss : -0.3901751637458801
Agent0_Alpha_Loss : 0.837202787399292
Agent0_Temperature : 0.09632293821198785
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.69390869140625
Agent1_Eval_StdReturn : 16.195777893066406
Agent1_Eval_MaxReturn : -0.29727935791015625
Agent1_Eval_MinReturn : -45.53099822998047
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -26.1810245513916
Agent1_Train_StdReturn : 20.77939224243164
Agent1_Train_MaxReturn : 19.95290756225586
Agent1_Train_MinReturn : -63.20869064331055
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 193500
Agent1_TimeSinceStart : 687.3269879817963
Agent1_Critic_Loss : 0.26250535249710083
Agent1_Actor_Loss : -0.47255393862724304
Agent1_Alpha_Loss : 0.8303960561752319
Agent1_Temperature : 0.0963129556421836
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -15.482009887695312
Agent0_Eval_StdReturn : 20.347829818725586
Agent0_Eval_MaxReturn : 32.084747314453125
Agent0_Eval_MinReturn : -44.861900329589844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -14.265810012817383
Agent0_Train_StdReturn : 7.36346435546875
Agent0_Train_MaxReturn : 3.340207815170288
Agent0_Train_MinReturn : -24.128005981445312
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 195000
Agent0_TimeSinceStart : 690.211053609848
Agent0_Critic_Loss : 0.305527001619339
Agent0_Actor_Loss : -0.35809826850891113
Agent0_Alpha_Loss : 0.8378644585609436
Agent0_Temperature : 0.09629760859138974
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -20.8620548248291
Agent1_Eval_StdReturn : 13.651087760925293
Agent1_Eval_MaxReturn : 11.14372730255127
Agent1_Eval_MinReturn : -41.395782470703125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -20.30049705505371
Agent1_Train_StdReturn : 22.521907806396484
Agent1_Train_MaxReturn : 38.01268768310547
Agent1_Train_MinReturn : -47.95359802246094
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 195000
Agent1_TimeSinceStart : 693.0811879634857
Agent1_Critic_Loss : 0.28634023666381836
Agent1_Actor_Loss : -0.52199786901474
Agent1_Alpha_Loss : 0.8453607559204102
Agent1_Temperature : 0.0962873139919067
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -19.548580169677734
Agent0_Eval_StdReturn : 16.521154403686523
Agent0_Eval_MaxReturn : 8.752176284790039
Agent0_Eval_MinReturn : -45.0246467590332
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -14.681108474731445
Agent0_Train_StdReturn : 21.475635528564453
Agent0_Train_MaxReturn : 14.543732643127441
Agent0_Train_MinReturn : -54.472713470458984
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 196500
Agent0_TimeSinceStart : 695.9508166313171
Agent0_Critic_Loss : 0.2882238030433655
Agent0_Actor_Loss : -0.4336532950401306
Agent0_Alpha_Loss : 0.8276512026786804
Agent0_Temperature : 0.09627223924839395
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -13.513689994812012
Agent1_Eval_StdReturn : 14.429132461547852
Agent1_Eval_MaxReturn : 19.297100067138672
Agent1_Eval_MinReturn : -29.686973571777344
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -17.945987701416016
Agent1_Train_StdReturn : 15.033917427062988
Agent1_Train_MaxReturn : 12.048033714294434
Agent1_Train_MinReturn : -41.376922607421875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 196500
Agent1_TimeSinceStart : 698.8154211044312
Agent1_Critic_Loss : 0.25845566391944885
Agent1_Actor_Loss : -0.5235400199890137
Agent1_Alpha_Loss : 0.8266632556915283
Agent1_Temperature : 0.09626167522465418
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -32.607818603515625
Agent0_Eval_StdReturn : 40.715179443359375
Agent0_Eval_MaxReturn : 25.59674835205078
Agent0_Eval_MinReturn : -112.24781799316406
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.93069076538086
Agent0_Train_StdReturn : 16.857460021972656
Agent0_Train_MaxReturn : -10.859838485717773
Agent0_Train_MinReturn : -64.44310760498047
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 205500
Agent0_TimeSinceStart : 718.0390005111694
Agent0_Critic_Loss : 1.4922362565994263
Agent0_Actor_Loss : -4.433584213256836
Agent0_Alpha_Loss : 4.798458099365234
Agent0_Temperature : 0.4799837197691983
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -38.79961395263672
Agent1_Eval_StdReturn : 22.00108528137207
Agent1_Eval_MaxReturn : -10.101036071777344
Agent1_Eval_MinReturn : -78.71246337890625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -50.82025909423828
Agent1_Train_StdReturn : 23.1297550201416
Agent1_Train_MaxReturn : -8.794736862182617
Agent1_Train_MinReturn : -78.35916137695312
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 205500
Agent1_TimeSinceStart : 720.8961329460144
Agent1_Critic_Loss : 2.0289011001586914
Agent1_Actor_Loss : -4.568107604980469
Agent1_Alpha_Loss : 4.78115177154541
Agent1_Temperature : 0.47998245821826685
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -32.618385314941406
Agent0_Eval_StdReturn : 25.775630950927734
Agent0_Eval_MaxReturn : 8.157289505004883
Agent0_Eval_MinReturn : -83.42332458496094
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.68141555786133
Agent0_Train_StdReturn : 15.755617141723633
Agent0_Train_MaxReturn : -18.27347183227539
Agent0_Train_MinReturn : -65.56166076660156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 207000
Agent0_TimeSinceStart : 723.7475950717926
Agent0_Critic_Loss : 1.3887379169464111
Agent0_Actor_Loss : -4.3258280754089355
Agent0_Alpha_Loss : 4.7809977531433105
Agent0_Temperature : 0.4798421841794477
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.90105056762695
Agent1_Eval_StdReturn : 34.637550354003906
Agent1_Eval_MaxReturn : 8.75080394744873
Agent1_Eval_MinReturn : -130.19064331054688
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -51.865333557128906
Agent1_Train_StdReturn : 43.684139251708984
Agent1_Train_MaxReturn : 3.864459991455078
Agent1_Train_MinReturn : -126.70646667480469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 207000
Agent1_TimeSinceStart : 726.6126883029938
Agent1_Critic_Loss : 2.0648038387298584
Agent1_Actor_Loss : -4.55509614944458
Agent1_Alpha_Loss : 4.776948928833008
Agent1_Temperature : 0.479840927188978
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.015098571777344
Agent0_Eval_StdReturn : 34.963748931884766
Agent0_Eval_MaxReturn : 0.6138906478881836
Agent0_Eval_MinReturn : -124.22920227050781
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -31.8691349029541
Agent0_Train_StdReturn : 33.12152099609375
Agent0_Train_MaxReturn : 18.849206924438477
Agent0_Train_MinReturn : -87.393798828125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 208500
Agent0_TimeSinceStart : 729.3830296993256
Agent0_Critic_Loss : 1.5449440479278564
Agent0_Actor_Loss : -4.282195568084717
Agent0_Alpha_Loss : 4.798357009887695
Agent0_Temperature : 0.47970071253399843
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -37.53815460205078
Agent1_Eval_StdReturn : 42.444515228271484
Agent1_Eval_MaxReturn : 21.02092742919922
Agent1_Eval_MinReturn : -121.74607849121094
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -50.18920135498047
Agent1_Train_StdReturn : 27.679121017456055
Agent1_Train_MaxReturn : -16.941425323486328
Agent1_Train_MinReturn : -113.59210205078125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 208500
Agent1_TimeSinceStart : 732.1423518657684
Agent1_Critic_Loss : 1.843790054321289
Agent1_Actor_Loss : -4.554879188537598
Agent1_Alpha_Loss : 4.801575183868408
Agent1_Temperature : 0.47969944807462667
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -35.13689422607422
Agent0_Eval_StdReturn : 40.31186294555664
Agent0_Eval_MaxReturn : 21.169567108154297
Agent0_Eval_MinReturn : -83.2406005859375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -42.55868911743164
Agent0_Train_StdReturn : 36.96821975708008
Agent0_Train_MaxReturn : 5.396984100341797
Agent0_Train_MinReturn : -101.69276428222656
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 210000
Agent0_TimeSinceStart : 734.9203450679779
Agent0_Critic_Loss : 1.2605798244476318
Agent0_Actor_Loss : -4.354146480560303
Agent0_Alpha_Loss : 4.7818098068237305
Agent0_Temperature : 0.4795593457956958
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.31649398803711
Agent1_Eval_StdReturn : 18.82262420654297
Agent1_Eval_MaxReturn : -5.273223876953125
Agent1_Eval_MinReturn : -56.612152099609375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -37.29444122314453
Agent1_Train_StdReturn : 24.340309143066406
Agent1_Train_MaxReturn : -0.8170633316040039
Agent1_Train_MinReturn : -70.4952163696289
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 210000
Agent1_TimeSinceStart : 737.677948474884
Agent1_Critic_Loss : 1.9050896167755127
Agent1_Actor_Loss : -4.598366737365723
Agent1_Alpha_Loss : 4.813288688659668
Agent1_Temperature : 0.4795579861776279
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -38.71485137939453
Agent0_Eval_StdReturn : 34.45193862915039
Agent0_Eval_MaxReturn : 12.652770042419434
Agent0_Eval_MinReturn : -88.12979125976562
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -56.573402404785156
Agent0_Train_StdReturn : 31.223651885986328
Agent0_Train_MaxReturn : -11.660040855407715
Agent0_Train_MinReturn : -99.13880920410156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 211500
Agent0_TimeSinceStart : 740.4492280483246
Agent0_Critic_Loss : 1.4979320764541626
Agent0_Actor_Loss : -4.325590133666992
Agent0_Alpha_Loss : 4.795744895935059
Agent0_Temperature : 0.47941803750144457
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -16.017826080322266
Agent0_Eval_StdReturn : 20.6133975982666
Agent0_Eval_MaxReturn : 20.121849060058594
Agent0_Eval_MinReturn : -47.511329650878906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -27.6192569732666
Agent0_Train_StdReturn : 19.77245330810547
Agent0_Train_MaxReturn : 5.236865997314453
Agent0_Train_MinReturn : -63.21864700317383
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 198000
Agent0_TimeSinceStart : 701.6788680553436
Agent0_Critic_Loss : 0.2572318911552429
Agent0_Actor_Loss : -0.4202706813812256
Agent0_Alpha_Loss : 0.8447039723396301
Agent0_Temperature : 0.09624678348426911
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.891489028930664
Agent1_Eval_StdReturn : 11.039901733398438
Agent1_Eval_MaxReturn : 4.842364311218262
Agent1_Eval_MinReturn : -32.87271499633789
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -16.90005874633789
Agent1_Train_StdReturn : 17.496976852416992
Agent1_Train_MaxReturn : 9.483234405517578
Agent1_Train_MinReturn : -59.22483825683594
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 198000
Agent1_TimeSinceStart : 704.5477948188782
Agent1_Critic_Loss : 0.25713294744491577
Agent1_Actor_Loss : -0.5684340000152588
Agent1_Alpha_Loss : 0.8456302285194397
Agent1_Temperature : 0.09623598301968098
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -11.821462631225586
Agent0_Eval_StdReturn : 24.4910831451416
Agent0_Eval_MaxReturn : 44.46881866455078
Agent0_Eval_MinReturn : -58.539520263671875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -19.48299789428711
Agent0_Train_StdReturn : 9.905291557312012
Agent0_Train_MaxReturn : -5.541529655456543
Agent0_Train_MinReturn : -34.130489349365234
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 199500
Agent0_TimeSinceStart : 707.4325575828552
Agent0_Critic_Loss : 0.3226168155670166
Agent0_Actor_Loss : -0.4348565340042114
Agent0_Alpha_Loss : 0.841160774230957
Agent0_Temperature : 0.09622125920254095
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -27.271106719970703
Agent1_Eval_StdReturn : 19.673173904418945
Agent1_Eval_MaxReturn : 11.97885513305664
Agent1_Eval_MinReturn : -48.36806869506836
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.273326873779297
Agent1_Train_StdReturn : 20.080181121826172
Agent1_Train_MaxReturn : 0.5951137542724609
Agent1_Train_MinReturn : -63.87556457519531
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 199500
Agent1_TimeSinceStart : 710.2531683444977
Agent1_Critic_Loss : 0.2943943440914154
Agent1_Actor_Loss : -0.4727104902267456
Agent1_Alpha_Loss : 0.8121732473373413
Agent1_Temperature : 0.09621033833475079
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -18.31704330444336
Agent0_Eval_StdReturn : 19.885665893554688
Agent0_Eval_MaxReturn : 26.214500427246094
Agent0_Eval_MinReturn : -39.263431549072266
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.27993392944336
Agent0_Train_StdReturn : 13.645651817321777
Agent0_Train_MaxReturn : -0.7619476318359375
Agent0_Train_MinReturn : -44.50175476074219
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 201000
Agent0_TimeSinceStart : 713.0430099964142
Agent0_Critic_Loss : 0.29746735095977783
Agent0_Actor_Loss : -0.4768563508987427
Agent0_Alpha_Loss : 0.8549855351448059
Agent0_Temperature : 0.09619563218515213
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -21.004274368286133
Agent1_Eval_StdReturn : 10.953231811523438
Agent1_Eval_MaxReturn : -1.8036831617355347
Agent1_Eval_MinReturn : -38.10890197753906
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.763376235961914
Agent1_Train_StdReturn : 10.423677444458008
Agent1_Train_MaxReturn : -17.397653579711914
Agent1_Train_MinReturn : -55.49074935913086
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 201000
Agent1_TimeSinceStart : 715.8343482017517
Agent1_Critic_Loss : 0.2686557173728943
Agent1_Actor_Loss : -0.46486636996269226
Agent1_Alpha_Loss : 0.8305138349533081
Agent1_Temperature : 0.09618468186294672
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -17.6853084564209
Agent0_Eval_StdReturn : 12.31652545928955
Agent0_Eval_MaxReturn : 9.62490463256836
Agent0_Eval_MinReturn : -38.38873291015625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -12.214973449707031
Agent0_Train_StdReturn : 10.485218048095703
Agent0_Train_MaxReturn : 4.121384143829346
Agent0_Train_MinReturn : -28.8984317779541
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 202500
Agent0_TimeSinceStart : 718.6185824871063
Agent0_Critic_Loss : 0.2714454233646393
Agent0_Actor_Loss : -0.4756847321987152
Agent0_Alpha_Loss : 0.8684081435203552
Agent0_Temperature : 0.09616987309109329
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -12.269424438476562
Agent1_Eval_StdReturn : 12.395829200744629
Agent1_Eval_MaxReturn : 6.4082746505737305
Agent1_Eval_MinReturn : -36.50853729248047
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -19.94119644165039
Agent1_Train_StdReturn : 13.794037818908691
Agent1_Train_MaxReturn : -1.0749387741088867
Agent1_Train_MinReturn : -52.917606353759766
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 202500
Agent1_TimeSinceStart : 721.3986189365387
Agent1_Critic_Loss : 0.35003307461738586
Agent1_Actor_Loss : -0.5147737264633179
Agent1_Alpha_Loss : 0.8418033123016357
Agent1_Temperature : 0.09615898092329644
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -18.542001724243164
Agent0_Eval_StdReturn : 26.776630401611328
Agent0_Eval_MaxReturn : 24.425323486328125
Agent0_Eval_MinReturn : -57.99394989013672
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -13.99751091003418
Agent0_Train_StdReturn : 13.864826202392578
Agent0_Train_MaxReturn : 4.449061870574951
Agent0_Train_MinReturn : -47.55695343017578
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 204000
Agent0_TimeSinceStart : 724.1937654018402
Agent0_Critic_Loss : 0.2569098174571991
Agent0_Actor_Loss : -0.4715542495250702
Agent0_Alpha_Loss : 0.8655716776847839
Agent0_Temperature : 0.09614400283283019
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -43.194786071777344
Agent1_Eval_StdReturn : 30.2349796295166
Agent1_Eval_MaxReturn : 5.194127082824707
Agent1_Eval_MinReturn : -94.33047485351562
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -49.19234085083008
Agent1_Train_StdReturn : 34.13121032714844
Agent1_Train_MaxReturn : 4.119711875915527
Agent1_Train_MinReturn : -98.08682250976562
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 211500
Agent1_TimeSinceStart : 743.2260596752167
Agent1_Critic_Loss : 1.8561333417892456
Agent1_Actor_Loss : -4.483602046966553
Agent1_Alpha_Loss : 4.798239231109619
Agent1_Temperature : 0.47941658341126964
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.805728912353516
Agent0_Eval_StdReturn : 37.05281066894531
Agent0_Eval_MaxReturn : 43.717063903808594
Agent0_Eval_MinReturn : -105.48930358886719
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -42.572669982910156
Agent0_Train_StdReturn : 19.798992156982422
Agent0_Train_MaxReturn : 1.5272893905639648
Agent0_Train_MinReturn : -61.44010925292969
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 213000
Agent0_TimeSinceStart : 746.0033984184265
Agent0_Critic_Loss : 1.449970006942749
Agent0_Actor_Loss : -4.2358317375183105
Agent0_Alpha_Loss : 4.768670082092285
Agent0_Temperature : 0.47927685790087193
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -54.658935546875
Agent1_Eval_StdReturn : 37.636959075927734
Agent1_Eval_MaxReturn : -4.509279251098633
Agent1_Eval_MinReturn : -148.77769470214844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -58.50688934326172
Agent1_Train_StdReturn : 25.684083938598633
Agent1_Train_MaxReturn : 1.6122052669525146
Agent1_Train_MinReturn : -97.62440490722656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 213000
Agent1_TimeSinceStart : 748.7760078907013
Agent1_Critic_Loss : 1.5908669233322144
Agent1_Actor_Loss : -4.427486419677734
Agent1_Alpha_Loss : 4.808664798736572
Agent1_Temperature : 0.47927520781885785
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -19.007949829101562
Agent0_Eval_StdReturn : 24.753366470336914
Agent0_Eval_MaxReturn : 8.88650131225586
Agent0_Eval_MinReturn : -86.56341552734375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.136322021484375
Agent0_Train_StdReturn : 15.73786735534668
Agent0_Train_MaxReturn : -4.617149829864502
Agent0_Train_MinReturn : -64.57611083984375
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 214500
Agent0_TimeSinceStart : 751.5577714443207
Agent0_Critic_Loss : 1.4348087310791016
Agent0_Actor_Loss : -4.316384315490723
Agent0_Alpha_Loss : 4.803586959838867
Agent0_Temperature : 0.479135700709566
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -44.74701690673828
Agent1_Eval_StdReturn : 29.76837921142578
Agent1_Eval_MaxReturn : 5.813009738922119
Agent1_Eval_MinReturn : -89.24710083007812
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.53950500488281
Agent1_Train_StdReturn : 32.363975524902344
Agent1_Train_MaxReturn : 15.651763916015625
Agent1_Train_MinReturn : -104.75048828125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 214500
Agent1_TimeSinceStart : 754.3366765975952
Agent1_Critic_Loss : 1.657682180404663
Agent1_Actor_Loss : -4.320990085601807
Agent1_Alpha_Loss : 4.809326648712158
Agent1_Temperature : 0.4791338574732393
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -26.983266830444336
Agent0_Eval_StdReturn : 20.2626895904541
Agent0_Eval_MaxReturn : 2.1814775466918945
Agent0_Eval_MinReturn : -61.97650146484375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -38.36823272705078
Agent0_Train_StdReturn : 23.959312438964844
Agent0_Train_MaxReturn : -3.0738296508789062
Agent0_Train_MinReturn : -74.97103118896484
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 216000
Agent0_TimeSinceStart : 757.126455783844
Agent0_Critic_Loss : 1.7568129301071167
Agent0_Actor_Loss : -4.278748989105225
Agent0_Alpha_Loss : 4.757648468017578
Agent0_Temperature : 0.47899469149918117
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.14185333251953
Agent1_Eval_StdReturn : 22.756732940673828
Agent1_Eval_MaxReturn : 2.0841479301452637
Agent1_Eval_MinReturn : -67.405517578125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -31.256046295166016
Agent1_Train_StdReturn : 23.28278160095215
Agent1_Train_MaxReturn : 4.291173934936523
Agent1_Train_MinReturn : -73.81210327148438
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 216000
Agent1_TimeSinceStart : 759.9113237857819
Agent1_Critic_Loss : 2.006741762161255
Agent1_Actor_Loss : -4.26975154876709
Agent1_Alpha_Loss : 4.788369178771973
Agent1_Temperature : 0.47899258962181895
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -16.02981185913086
Agent0_Eval_StdReturn : 21.913599014282227
Agent0_Eval_MaxReturn : 17.368345260620117
Agent0_Eval_MinReturn : -51.611961364746094
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -38.80084991455078
Agent0_Train_StdReturn : 24.737138748168945
Agent0_Train_MaxReturn : 0.39264607429504395
Agent0_Train_MinReturn : -67.95346069335938
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 217500
Agent0_TimeSinceStart : 762.7086751461029
Agent0_Critic_Loss : 1.3613028526306152
Agent0_Actor_Loss : -4.370452880859375
Agent0_Alpha_Loss : 4.7785797119140625
Agent0_Temperature : 0.4788537599698461
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -36.90097427368164
Agent1_Eval_StdReturn : 35.152366638183594
Agent1_Eval_MaxReturn : 17.131052017211914
Agent1_Eval_MinReturn : -83.94119262695312
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.80614471435547
Agent1_Train_StdReturn : 30.37351417541504
Agent1_Train_MaxReturn : -10.414875030517578
Agent1_Train_MinReturn : -97.51737976074219
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 217500
Agent1_TimeSinceStart : 765.4964642524719
Agent1_Critic_Loss : 1.5096256732940674
Agent1_Actor_Loss : -4.267125129699707
Agent1_Alpha_Loss : 4.763510704040527
Agent1_Temperature : 0.47885146608432466
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -12.20997142791748
Agent1_Eval_StdReturn : 12.749239921569824
Agent1_Eval_MaxReturn : 11.573217391967773
Agent1_Eval_MinReturn : -38.28505325317383
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -21.891502380371094
Agent1_Train_StdReturn : 20.091629028320312
Agent1_Train_MaxReturn : 2.3991808891296387
Agent1_Train_MinReturn : -67.75496673583984
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 204000
Agent1_TimeSinceStart : 726.9888098239899
Agent1_Critic_Loss : 0.23768377304077148
Agent1_Actor_Loss : -0.5111121535301208
Agent1_Alpha_Loss : 0.8492550849914551
Agent1_Temperature : 0.09613321740247219
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -20.152463912963867
Agent0_Eval_StdReturn : 18.225570678710938
Agent0_Eval_MaxReturn : 16.97743797302246
Agent0_Eval_MinReturn : -46.865234375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -17.68227767944336
Agent0_Train_StdReturn : 17.344661712646484
Agent0_Train_MaxReturn : 24.36815643310547
Agent0_Train_MinReturn : -41.747833251953125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 205500
Agent0_TimeSinceStart : 729.7900817394257
Agent0_Critic_Loss : 0.2713887095451355
Agent0_Actor_Loss : -0.47003090381622314
Agent0_Alpha_Loss : 0.8559716939926147
Agent0_Temperature : 0.096118059797486
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -20.66809844970703
Agent1_Eval_StdReturn : 11.206656455993652
Agent1_Eval_MaxReturn : 1.5619020462036133
Agent1_Eval_MinReturn : -40.48893356323242
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -18.644149780273438
Agent1_Train_StdReturn : 15.783166885375977
Agent1_Train_MaxReturn : 11.953961372375488
Agent1_Train_MinReturn : -50.11715316772461
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 205500
Agent1_TimeSinceStart : 732.5957555770874
Agent1_Critic_Loss : 0.32767438888549805
Agent1_Actor_Loss : -0.5937545895576477
Agent1_Alpha_Loss : 0.846523642539978
Agent1_Temperature : 0.09610740459576496
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -21.712080001831055
Agent0_Eval_StdReturn : 20.373310089111328
Agent0_Eval_MaxReturn : 10.722197532653809
Agent0_Eval_MinReturn : -70.01739501953125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -23.638063430786133
Agent0_Train_StdReturn : 18.15805435180664
Agent0_Train_MaxReturn : 0.5003148913383484
Agent0_Train_MinReturn : -56.53963851928711
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 207000
Agent0_TimeSinceStart : 735.4056432247162
Agent0_Critic_Loss : 0.3304769992828369
Agent0_Actor_Loss : -0.452342689037323
Agent0_Alpha_Loss : 0.8559137582778931
Agent0_Temperature : 0.09609205074401785
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -22.283580780029297
Agent1_Eval_StdReturn : 27.05545997619629
Agent1_Eval_MaxReturn : 15.005817413330078
Agent1_Eval_MinReturn : -81.39119720458984
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -12.773545265197754
Agent1_Train_StdReturn : 13.464168548583984
Agent1_Train_MaxReturn : 4.872687339782715
Agent1_Train_MinReturn : -39.95602798461914
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 207000
Agent1_TimeSinceStart : 738.2041964530945
Agent1_Critic_Loss : 0.2741965651512146
Agent1_Actor_Loss : -0.5634713768959045
Agent1_Alpha_Loss : 0.8540805578231812
Agent1_Temperature : 0.09608152466881933
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -16.45098114013672
Agent0_Eval_StdReturn : 10.025078773498535
Agent0_Eval_MaxReturn : 1.6802722215652466
Agent0_Eval_MinReturn : -32.52706527709961
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -14.595087051391602
Agent0_Train_StdReturn : 17.07761573791504
Agent0_Train_MaxReturn : 9.50552749633789
Agent0_Train_MinReturn : -49.1820182800293
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 208500
Agent0_TimeSinceStart : 741.0201687812805
Agent0_Critic_Loss : 0.28092139959335327
Agent0_Actor_Loss : -0.5031967163085938
Agent0_Alpha_Loss : 0.868428111076355
Agent0_Temperature : 0.09606594532472416
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -19.647891998291016
Agent1_Eval_StdReturn : 11.196081161499023
Agent1_Eval_MaxReturn : -7.904208660125732
Agent1_Eval_MinReturn : -48.25743865966797
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -20.388912200927734
Agent1_Train_StdReturn : 16.725801467895508
Agent1_Train_MaxReturn : 3.8834762573242188
Agent1_Train_MinReturn : -57.67252731323242
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 208500
Agent1_TimeSinceStart : 743.8421297073364
Agent1_Critic_Loss : 0.2741447985172272
Agent1_Actor_Loss : -0.5616731643676758
Agent1_Alpha_Loss : 0.8455625772476196
Agent1_Temperature : 0.09605560826475701
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.719680786132812
Agent0_Eval_StdReturn : 30.356260299682617
Agent0_Eval_MaxReturn : 0.7532796859741211
Agent0_Eval_MinReturn : -108.94206237792969
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.163827896118164
Agent0_Train_StdReturn : 29.59488296508789
Agent0_Train_MaxReturn : 47.25880432128906
Agent0_Train_MinReturn : -66.70474243164062
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 210000
Agent0_TimeSinceStart : 746.6474678516388
Agent0_Critic_Loss : 0.2987876236438751
Agent0_Actor_Loss : -0.4989856779575348
Agent0_Alpha_Loss : 0.8583508729934692
Agent0_Temperature : 0.09603978197767259
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -30.14813804626465
Agent1_Eval_StdReturn : 17.880949020385742
Agent1_Eval_MaxReturn : -0.22568130493164062
Agent1_Eval_MinReturn : -59.81194305419922
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -17.544769287109375
Agent1_Train_StdReturn : 17.8920955657959
Agent1_Train_MaxReturn : 3.701140880584717
Agent1_Train_MinReturn : -52.15861511230469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 210000
Agent1_TimeSinceStart : 749.4582471847534
Agent1_Critic_Loss : 0.24554580450057983
Agent1_Actor_Loss : -0.5821715593338013
Agent1_Alpha_Loss : 0.8596985340118408
Agent1_Temperature : 0.09602961720815689
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -45.364044189453125
Agent0_Eval_StdReturn : 20.55620765686035
Agent0_Eval_MaxReturn : -15.112842559814453
Agent0_Eval_MinReturn : -76.80813598632812
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.95378494262695
Agent0_Train_StdReturn : 33.003902435302734
Agent0_Train_MaxReturn : 20.948135375976562
Agent0_Train_MinReturn : -108.34881591796875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 219000
Agent0_TimeSinceStart : 768.2930750846863
Agent0_Critic_Loss : 1.7054059505462646
Agent0_Actor_Loss : -4.39424991607666
Agent0_Alpha_Loss : 4.762393951416016
Agent0_Temperature : 0.47871294460981384
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -49.885765075683594
Agent1_Eval_StdReturn : 34.66646194458008
Agent1_Eval_MaxReturn : 3.3220858573913574
Agent1_Eval_MinReturn : -109.28041076660156
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.6367073059082
Agent1_Train_StdReturn : 35.081138610839844
Agent1_Train_MaxReturn : 16.91728973388672
Agent1_Train_MinReturn : -91.80281066894531
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 219000
Agent1_TimeSinceStart : 771.0946788787842
Agent1_Critic_Loss : 1.8787890672683716
Agent1_Actor_Loss : -4.410662651062012
Agent1_Alpha_Loss : 4.808080673217773
Agent1_Temperature : 0.47871035254866595
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -27.1107120513916
Agent0_Eval_StdReturn : 41.1620979309082
Agent0_Eval_MaxReturn : 45.192420959472656
Agent0_Eval_MinReturn : -98.92146301269531
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -35.25425338745117
Agent0_Train_StdReturn : 29.651836395263672
Agent0_Train_MaxReturn : 1.9166074991226196
Agent0_Train_MinReturn : -82.13301086425781
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 220500
Agent0_TimeSinceStart : 773.905246257782
Agent0_Critic_Loss : 1.390492558479309
Agent0_Actor_Loss : -4.382156848907471
Agent0_Alpha_Loss : 4.788689613342285
Agent0_Temperature : 0.4785721637613479
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -38.37521743774414
Agent1_Eval_StdReturn : 35.417232513427734
Agent1_Eval_MaxReturn : 2.9628677368164062
Agent1_Eval_MinReturn : -109.07778930664062
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.896873474121094
Agent1_Train_StdReturn : 26.680925369262695
Agent1_Train_MaxReturn : 11.719425201416016
Agent1_Train_MinReturn : -80.9822769165039
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 220500
Agent1_TimeSinceStart : 776.7188363075256
Agent1_Critic_Loss : 1.9107637405395508
Agent1_Actor_Loss : -4.319145679473877
Agent1_Alpha_Loss : 4.785266399383545
Agent1_Temperature : 0.4785693129635746
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.01494598388672
Agent0_Eval_StdReturn : 26.335660934448242
Agent0_Eval_MaxReturn : -12.92898941040039
Agent0_Eval_MinReturn : -90.1170654296875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -35.46944046020508
Agent0_Train_StdReturn : 30.370010375976562
Agent0_Train_MaxReturn : 25.944766998291016
Agent0_Train_MinReturn : -94.45330047607422
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 222000
Agent0_TimeSinceStart : 779.537237405777
Agent0_Critic_Loss : 1.4739599227905273
Agent0_Actor_Loss : -4.401142120361328
Agent0_Alpha_Loss : 4.769755840301514
Agent0_Temperature : 0.47843146801355574
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -51.801780700683594
Agent1_Eval_StdReturn : 36.459800720214844
Agent1_Eval_MaxReturn : 19.003955841064453
Agent1_Eval_MinReturn : -97.87550354003906
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -60.5180778503418
Agent1_Train_StdReturn : 28.51708984375
Agent1_Train_MaxReturn : -23.93341064453125
Agent1_Train_MinReturn : -126.2651138305664
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 222000
Agent1_TimeSinceStart : 782.3575417995453
Agent1_Critic_Loss : 1.8195488452911377
Agent1_Actor_Loss : -4.3810296058654785
Agent1_Alpha_Loss : 4.820033073425293
Agent1_Temperature : 0.4784282472526142
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -48.38272476196289
Agent0_Eval_StdReturn : 34.040679931640625
Agent0_Eval_MaxReturn : -2.5731167793273926
Agent0_Eval_MinReturn : -120.50334930419922
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -26.674320220947266
Agent0_Train_StdReturn : 25.29315757751465
Agent0_Train_MaxReturn : 15.270278930664062
Agent0_Train_MinReturn : -66.26960754394531
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 223500
Agent0_TimeSinceStart : 785.1715705394745
Agent0_Critic_Loss : 1.748992681503296
Agent0_Actor_Loss : -4.4239726066589355
Agent0_Alpha_Loss : 4.80665397644043
Agent0_Temperature : 0.4782907500070116
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.2957763671875
Agent1_Eval_StdReturn : 29.31307029724121
Agent1_Eval_MaxReturn : 5.315281867980957
Agent1_Eval_MinReturn : -97.20278930664062
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.348087310791016
Agent1_Train_StdReturn : 25.26439666748047
Agent1_Train_MaxReturn : 5.91274356842041
Agent1_Train_MinReturn : -76.13998413085938
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 223500
Agent1_TimeSinceStart : 787.985021352768
Agent1_Critic_Loss : 1.8001834154129028
Agent1_Actor_Loss : -4.4238481521606445
Agent1_Alpha_Loss : 4.750028610229492
Agent1_Temperature : 0.47828735222052576
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -40.81391143798828
Agent0_Eval_StdReturn : 34.00578689575195
Agent0_Eval_MaxReturn : 16.157272338867188
Agent0_Eval_MinReturn : -82.12504577636719
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -49.351829528808594
Agent0_Train_StdReturn : 32.745758056640625
Agent0_Train_MaxReturn : 2.433279037475586
Agent0_Train_MinReturn : -93.8825454711914
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 225000
Agent0_TimeSinceStart : 790.7937972545624
Agent0_Critic_Loss : 1.5750700235366821
Agent0_Actor_Loss : -4.360112190246582
Agent0_Alpha_Loss : 4.770202159881592
Agent0_Temperature : 0.47815011430562565
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -23.521076202392578
Agent0_Eval_StdReturn : 20.620439529418945
Agent0_Eval_MaxReturn : 23.854145050048828
Agent0_Eval_MinReturn : -46.26346206665039
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -27.378530502319336
Agent0_Train_StdReturn : 22.891353607177734
Agent0_Train_MaxReturn : 1.4827884435653687
Agent0_Train_MinReturn : -71.04722595214844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 211500
Agent0_TimeSinceStart : 752.2816541194916
Agent0_Critic_Loss : 0.3092814087867737
Agent0_Actor_Loss : -0.5168218016624451
Agent0_Alpha_Loss : 0.8627331256866455
Agent0_Temperature : 0.09601355319018205
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -16.667125701904297
Agent1_Eval_StdReturn : 21.49416732788086
Agent1_Eval_MaxReturn : 8.090740203857422
Agent1_Eval_MinReturn : -58.66529846191406
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.671871185302734
Agent1_Train_StdReturn : 26.3367919921875
Agent1_Train_MaxReturn : -3.081867218017578
Agent1_Train_MinReturn : -84.52081298828125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 211500
Agent1_TimeSinceStart : 755.1051936149597
Agent1_Critic_Loss : 0.23053142428398132
Agent1_Actor_Loss : -0.5097908973693848
Agent1_Alpha_Loss : 0.8593388795852661
Agent1_Temperature : 0.09600355940242951
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -16.301694869995117
Agent0_Eval_StdReturn : 25.185161590576172
Agent0_Eval_MaxReturn : 21.357656478881836
Agent0_Eval_MinReturn : -57.45802307128906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.114479064941406
Agent0_Train_StdReturn : 14.413267135620117
Agent0_Train_MaxReturn : 10.768579483032227
Agent0_Train_MinReturn : -46.2814826965332
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 213000
Agent0_TimeSinceStart : 757.934877872467
Agent0_Critic_Loss : 0.3212403655052185
Agent0_Actor_Loss : -0.5093957185745239
Agent0_Alpha_Loss : 0.8596521019935608
Agent0_Temperature : 0.09598727396155593
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -15.399205207824707
Agent1_Eval_StdReturn : 17.287689208984375
Agent1_Eval_MaxReturn : 12.730422019958496
Agent1_Eval_MinReturn : -48.03178405761719
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -32.039817810058594
Agent1_Train_StdReturn : 20.946691513061523
Agent1_Train_MaxReturn : -1.81514310836792
Agent1_Train_MinReturn : -70.76768493652344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 213000
Agent1_TimeSinceStart : 760.7645418643951
Agent1_Critic_Loss : 0.2538823187351227
Agent1_Actor_Loss : -0.5299078226089478
Agent1_Alpha_Loss : 0.8505231142044067
Agent1_Temperature : 0.09597746649833912
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -22.246326446533203
Agent0_Eval_StdReturn : 22.404142379760742
Agent0_Eval_MaxReturn : 10.441638946533203
Agent0_Eval_MinReturn : -56.40254211425781
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -13.546574592590332
Agent0_Train_StdReturn : 17.107954025268555
Agent0_Train_MaxReturn : 8.462904930114746
Agent0_Train_MinReturn : -48.7617301940918
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 214500
Agent0_TimeSinceStart : 763.6030912399292
Agent0_Critic_Loss : 0.3118361532688141
Agent0_Actor_Loss : -0.5174684524536133
Agent0_Alpha_Loss : 0.877367377281189
Agent0_Temperature : 0.09596089737710246
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -22.539993286132812
Agent1_Eval_StdReturn : 20.566415786743164
Agent1_Eval_MaxReturn : -1.0689868927001953
Agent1_Eval_MinReturn : -60.428016662597656
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -32.698699951171875
Agent1_Train_StdReturn : 25.226736068725586
Agent1_Train_MaxReturn : -3.4439940452575684
Agent1_Train_MinReturn : -93.89303588867188
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 214500
Agent1_TimeSinceStart : 766.4270310401917
Agent1_Critic_Loss : 0.2662982940673828
Agent1_Actor_Loss : -0.5206781625747681
Agent1_Alpha_Loss : 0.8678702116012573
Agent1_Temperature : 0.09595129096418135
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -7.054228782653809
Agent0_Eval_StdReturn : 22.063735961914062
Agent0_Eval_MaxReturn : 48.550559997558594
Agent0_Eval_MinReturn : -26.358882904052734
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.586576461791992
Agent0_Train_StdReturn : 22.30888557434082
Agent0_Train_MaxReturn : 22.53780174255371
Agent0_Train_MinReturn : -46.513267517089844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 216000
Agent0_TimeSinceStart : 769.251002073288
Agent0_Critic_Loss : 0.3316907286643982
Agent0_Actor_Loss : -0.4455810785293579
Agent0_Alpha_Loss : 0.848090648651123
Agent0_Temperature : 0.0959345180510932
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -21.30317497253418
Agent1_Eval_StdReturn : 18.444089889526367
Agent1_Eval_MaxReturn : 5.950817108154297
Agent1_Eval_MinReturn : -66.55136108398438
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -15.246068954467773
Agent1_Train_StdReturn : 13.49968147277832
Agent1_Train_MaxReturn : 7.3274030685424805
Agent1_Train_MinReturn : -40.955162048339844
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 216000
Agent1_TimeSinceStart : 772.0767233371735
Agent1_Critic_Loss : 0.27066272497177124
Agent1_Actor_Loss : -0.5774301290512085
Agent1_Alpha_Loss : 0.857854962348938
Agent1_Temperature : 0.09592506971313122
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -24.039037704467773
Agent0_Eval_StdReturn : 18.243682861328125
Agent0_Eval_MaxReturn : 1.5674110651016235
Agent0_Eval_MinReturn : -49.64857482910156
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -4.984360694885254
Agent0_Train_StdReturn : 15.490629196166992
Agent0_Train_MaxReturn : 28.01804542541504
Agent0_Train_MinReturn : -25.973432540893555
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 217500
Agent0_TimeSinceStart : 774.8888227939606
Agent0_Critic_Loss : 0.33786287903785706
Agent0_Actor_Loss : -0.39537566900253296
Agent0_Alpha_Loss : 0.8620790243148804
Agent0_Temperature : 0.0959080948689667
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...
Agent1_Eval_AverageReturn : -35.53550338745117
Agent1_Eval_StdReturn : 34.6468391418457
Agent1_Eval_MaxReturn : 19.605525970458984
Agent1_Eval_MinReturn : -88.73316955566406
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -44.418212890625
Agent1_Train_StdReturn : 28.03585433959961
Agent1_Train_MaxReturn : 0.6090283393859863
Agent1_Train_MinReturn : -89.11708068847656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 225000
Agent1_TimeSinceStart : 793.5874428749084
Agent1_Critic_Loss : 1.6874749660491943
Agent1_Actor_Loss : -4.457895755767822
Agent1_Alpha_Loss : 4.782262802124023
Agent1_Temperature : 0.47814652429417265
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -40.6850471496582
Agent0_Eval_StdReturn : 26.819416046142578
Agent0_Eval_MaxReturn : 26.115386962890625
Agent0_Eval_MinReturn : -69.23948669433594
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -43.41853332519531
Agent0_Train_StdReturn : 18.484769821166992
Agent0_Train_MaxReturn : 6.0101470947265625
Agent0_Train_MinReturn : -64.63094329833984
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 226500
Agent0_TimeSinceStart : 796.3930637836456
Agent0_Critic_Loss : 1.530665397644043
Agent0_Actor_Loss : -4.186531066894531
Agent0_Alpha_Loss : 4.7787628173828125
Agent0_Temperature : 0.4780095312956364
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.53098678588867
Agent1_Eval_StdReturn : 31.340299606323242
Agent1_Eval_MaxReturn : 28.29999351501465
Agent1_Eval_MinReturn : -79.72664642333984
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.137454986572266
Agent1_Train_StdReturn : 22.71620750427246
Agent1_Train_MaxReturn : 5.9427266120910645
Agent1_Train_MinReturn : -73.3817138671875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 226500
Agent1_TimeSinceStart : 799.206337928772
Agent1_Critic_Loss : 1.771939992904663
Agent1_Actor_Loss : -4.528729438781738
Agent1_Alpha_Loss : 4.763276100158691
Agent1_Temperature : 0.47800581093757694
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -41.02192306518555
Agent0_Eval_StdReturn : 28.942371368408203
Agent0_Eval_MaxReturn : 11.208625793457031
Agent0_Eval_MinReturn : -75.35743713378906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.339683532714844
Agent0_Train_StdReturn : 31.34355926513672
Agent0_Train_MaxReturn : 0.983637809753418
Agent0_Train_MinReturn : -98.81959533691406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 228000
Agent0_TimeSinceStart : 802.0162281990051
Agent0_Critic_Loss : 1.770744800567627
Agent0_Actor_Loss : -4.187101364135742
Agent0_Alpha_Loss : 4.742993354797363
Agent0_Temperature : 0.477869095795762
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -41.12578582763672
Agent1_Eval_StdReturn : 22.712642669677734
Agent1_Eval_MaxReturn : 1.2344260215759277
Agent1_Eval_MinReturn : -82.3506088256836
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -28.992000579833984
Agent1_Train_StdReturn : 29.235454559326172
Agent1_Train_MaxReturn : 24.392166137695312
Agent1_Train_MinReturn : -76.6537094116211
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 228000
Agent1_TimeSinceStart : 804.8180177211761
Agent1_Critic_Loss : 1.740430474281311
Agent1_Actor_Loss : -4.512513160705566
Agent1_Alpha_Loss : 4.763077735900879
Agent1_Temperature : 0.47786520315486425
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.81435775756836
Agent0_Eval_StdReturn : 28.265485763549805
Agent0_Eval_MaxReturn : 6.848785877227783
Agent0_Eval_MinReturn : -84.72370910644531
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -47.82605743408203
Agent0_Train_StdReturn : 24.65384292602539
Agent0_Train_MaxReturn : 10.701608657836914
Agent0_Train_MinReturn : -73.9889907836914
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 229500
Agent0_TimeSinceStart : 807.6369361877441
Agent0_Critic_Loss : 1.5547351837158203
Agent0_Actor_Loss : -4.201815128326416
Agent0_Alpha_Loss : 4.759592056274414
Agent0_Temperature : 0.4777287491863355
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -46.371177673339844
Agent1_Eval_StdReturn : 32.9102783203125
Agent1_Eval_MaxReturn : 7.072018623352051
Agent1_Eval_MinReturn : -83.2020492553711
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -53.589698791503906
Agent1_Train_StdReturn : 31.846698760986328
Agent1_Train_MaxReturn : 23.685972213745117
Agent1_Train_MinReturn : -88.28101348876953
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 229500
Agent1_TimeSinceStart : 810.4582991600037
Agent1_Critic_Loss : 1.6117620468139648
Agent1_Actor_Loss : -4.488824844360352
Agent1_Alpha_Loss : 4.753139495849609
Agent1_Temperature : 0.4777247195354843
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -41.93781661987305
Agent0_Eval_StdReturn : 23.40128517150879
Agent0_Eval_MaxReturn : 1.8471145629882812
Agent0_Eval_MinReturn : -79.27845764160156
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -41.06560516357422
Agent0_Train_StdReturn : 27.216371536254883
Agent0_Train_MaxReturn : 1.5409023761749268
Agent0_Train_MinReturn : -85.58407592773438
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 231000
Agent0_TimeSinceStart : 813.2724297046661
Agent0_Critic_Loss : 1.414827823638916
Agent0_Actor_Loss : -4.251580715179443
Agent0_Alpha_Loss : 4.743756294250488
Agent0_Temperature : 0.4775885278550202
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -29.077983856201172
Agent1_Eval_StdReturn : 40.314178466796875
Agent1_Eval_MaxReturn : 45.390052795410156
Agent1_Eval_MinReturn : -88.51812744140625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -38.02828598022461
Agent1_Train_StdReturn : 31.278244018554688
Agent1_Train_MaxReturn : 12.800725936889648
Agent1_Train_MinReturn : -84.13746643066406
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 231000
Agent1_TimeSinceStart : 816.093113899231
Agent1_Critic_Loss : 1.7610656023025513
Agent1_Actor_Loss : -4.428768157958984
Agent1_Alpha_Loss : 4.7791948318481445
Agent1_Temperature : 0.4775842781192289
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -67.0013656616211
Agent0_Eval_StdReturn : 24.639461517333984

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -21.194229125976562
Agent1_Eval_StdReturn : 20.4606990814209
Agent1_Eval_MaxReturn : 9.878690719604492
Agent1_Eval_MinReturn : -57.22650909423828
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -19.326810836791992
Agent1_Train_StdReturn : 23.398454666137695
Agent1_Train_MaxReturn : 21.967193603515625
Agent1_Train_MinReturn : -54.762569427490234
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 217500
Agent1_TimeSinceStart : 777.7172629833221
Agent1_Critic_Loss : 0.30937519669532776
Agent1_Actor_Loss : -0.5282439589500427
Agent1_Alpha_Loss : 0.8498133420944214
Agent1_Temperature : 0.09589883011533758
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -23.907085418701172
Agent0_Eval_StdReturn : 18.305213928222656
Agent0_Eval_MaxReturn : 19.420875549316406
Agent0_Eval_MinReturn : -49.6037712097168
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -23.323030471801758
Agent0_Train_StdReturn : 18.14167022705078
Agent0_Train_MaxReturn : -5.071351051330566
Agent0_Train_MinReturn : -64.39677429199219
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 219000
Agent0_TimeSinceStart : 780.5411972999573
Agent0_Critic_Loss : 0.38145411014556885
Agent0_Actor_Loss : -0.4262610077857971
Agent0_Alpha_Loss : 0.8596342206001282
Agent0_Temperature : 0.09588163889301479
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -24.281177520751953
Agent1_Eval_StdReturn : 21.468429565429688
Agent1_Eval_MaxReturn : 14.611363410949707
Agent1_Eval_MinReturn : -62.5262451171875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -18.767162322998047
Agent1_Train_StdReturn : 22.362478256225586
Agent1_Train_MaxReturn : 3.4635848999023438
Agent1_Train_MinReturn : -66.03103637695312
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 219000
Agent1_TimeSinceStart : 783.364673614502
Agent1_Critic_Loss : 0.3302076458930969
Agent1_Actor_Loss : -0.6716347932815552
Agent1_Alpha_Loss : 0.8666852116584778
Agent1_Temperature : 0.09587252430306122
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -24.435016632080078
Agent0_Eval_StdReturn : 26.0965633392334
Agent0_Eval_MaxReturn : 2.560227394104004
Agent0_Eval_MinReturn : -78.23432922363281
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -28.87240982055664
Agent0_Train_StdReturn : 30.985700607299805
Agent0_Train_MaxReturn : 11.844884872436523
Agent0_Train_MinReturn : -79.09762573242188
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 220500
Agent0_TimeSinceStart : 786.1924240589142
Agent0_Critic_Loss : 0.3160083293914795
Agent0_Actor_Loss : -0.45264294743537903
Agent0_Alpha_Loss : 0.8694295883178711
Agent0_Temperature : 0.09585512443358048
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.38357162475586
Agent1_Eval_StdReturn : 27.778478622436523
Agent1_Eval_MaxReturn : 19.662521362304688
Agent1_Eval_MinReturn : -77.30006408691406
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -16.94846534729004
Agent1_Train_StdReturn : 19.16848373413086
Agent1_Train_MaxReturn : 19.133594512939453
Agent1_Train_MinReturn : -51.848236083984375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 220500
Agent1_TimeSinceStart : 789.026664018631
Agent1_Critic_Loss : 0.2593552768230438
Agent1_Actor_Loss : -0.6245023012161255
Agent1_Alpha_Loss : 0.849624514579773
Agent1_Temperature : 0.09584620811594217
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -23.037677764892578
Agent0_Eval_StdReturn : 17.159839630126953
Agent0_Eval_MaxReturn : 2.880910873413086
Agent0_Eval_MinReturn : -54.31514358520508
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -20.83527946472168
Agent0_Train_StdReturn : 12.80105209350586
Agent0_Train_MaxReturn : 4.236606121063232
Agent0_Train_MinReturn : -37.08596420288086
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 222000
Agent0_TimeSinceStart : 791.8597428798676
Agent0_Critic_Loss : 0.30746936798095703
Agent0_Actor_Loss : -0.45542287826538086
Agent0_Alpha_Loss : 0.8588927984237671
Agent0_Temperature : 0.09582858773983638
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -24.020414352416992
Agent1_Eval_StdReturn : 23.046630859375
Agent1_Eval_MaxReturn : 10.646195411682129
Agent1_Eval_MinReturn : -58.13996505737305
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -18.442676544189453
Agent1_Train_StdReturn : 18.14376449584961
Agent1_Train_MaxReturn : 16.66692352294922
Agent1_Train_MinReturn : -45.989376068115234
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 222000
Agent1_TimeSinceStart : 794.687474489212
Agent1_Critic_Loss : 0.24498017132282257
Agent1_Actor_Loss : -0.5814318656921387
Agent1_Alpha_Loss : 0.8776366114616394
Agent1_Temperature : 0.09581980050336723
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -19.083736419677734
Agent0_Eval_StdReturn : 31.32893180847168
Agent0_Eval_MaxReturn : 26.7793025970459
Agent0_Eval_MinReturn : -67.74198913574219
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -12.482145309448242
Agent0_Train_StdReturn : 11.657384872436523
Agent0_Train_MaxReturn : 5.382328033447266
Agent0_Train_MinReturn : -34.654624938964844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 223500
Agent0_TimeSinceStart : 797.5305550098419
Agent0_Critic_Loss : 0.26250290870666504
Agent0_Actor_Loss : -0.5031150579452515
Agent0_Alpha_Loss : 0.8674148917198181
Agent0_Temperature : 0.09580200576257589
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -18.78097152709961
Agent1_Eval_StdReturn : 27.95699691772461
Agent1_Eval_MaxReturn : 13.935672760009766
Agent1_Eval_MinReturn : -82.57787322998047
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -21.06938362121582
Agent1_Train_StdReturn : 10.03409481048584
Agent1_Train_MaxReturn : -2.9289255142211914
Agent1_Train_MinReturn : -41.45930099487305
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 223500
Agent1_TimeSinceStart : 800.363068819046
Agent1_Critic_Loss : 0.28973084688186646
Agent1_Actor_Loss : -0.55806565284729
Agent1_Alpha_Loss : 0.8593896627426147
Agent1_Temperature : 0.09579336350187548
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...
Agent0_Eval_MaxReturn : -27.9827880859375
Agent0_Eval_MinReturn : -111.18798065185547
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -42.731300354003906
Agent0_Train_StdReturn : 19.288801193237305
Agent0_Train_MaxReturn : -4.044442176818848
Agent0_Train_MinReturn : -68.13246154785156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 232500
Agent0_TimeSinceStart : 818.9038670063019
Agent0_Critic_Loss : 1.2486404180526733
Agent0_Actor_Loss : -4.373774528503418
Agent0_Alpha_Loss : 4.749436378479004
Agent0_Temperature : 0.47744840535580757
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -42.0990104675293
Agent1_Eval_StdReturn : 32.35475540161133
Agent1_Eval_MaxReturn : 2.0241546630859375
Agent1_Eval_MinReturn : -89.39523315429688
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -43.018890380859375
Agent1_Train_StdReturn : 28.839637756347656
Agent1_Train_MaxReturn : 13.47694206237793
Agent1_Train_MinReturn : -78.14327239990234
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 232500
Agent1_TimeSinceStart : 821.7515778541565
Agent1_Critic_Loss : 1.5132477283477783
Agent1_Actor_Loss : -4.368317604064941
Agent1_Alpha_Loss : 4.763107776641846
Agent1_Temperature : 0.4774439210351051
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -51.00669479370117
Agent0_Eval_StdReturn : 34.732627868652344
Agent0_Eval_MaxReturn : -6.417657852172852
Agent0_Eval_MinReturn : -123.00157165527344
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.23050880432129
Agent0_Train_StdReturn : 28.946992874145508
Agent0_Train_MaxReturn : 8.096253395080566
Agent0_Train_MinReturn : -81.47016143798828
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 234000
Agent0_TimeSinceStart : 824.5759003162384
Agent0_Critic_Loss : 1.435995101928711
Agent0_Actor_Loss : -4.352079391479492
Agent0_Alpha_Loss : 4.73036003112793
Agent0_Temperature : 0.4773084258697327
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -27.26007080078125
Agent1_Eval_StdReturn : 33.76350784301758
Agent1_Eval_MaxReturn : 26.707847595214844
Agent1_Eval_MinReturn : -100.70603942871094
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -38.00971984863281
Agent1_Train_StdReturn : 35.79648971557617
Agent1_Train_MaxReturn : 38.096900939941406
Agent1_Train_MinReturn : -92.70909881591797
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 234000
Agent1_TimeSinceStart : 827.3785221576691
Agent1_Critic_Loss : 1.5196647644042969
Agent1_Actor_Loss : -4.384627819061279
Agent1_Alpha_Loss : 4.785737037658691
Agent1_Temperature : 0.4773035798925075
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -39.216468811035156
Agent0_Eval_StdReturn : 27.7293758392334
Agent0_Eval_MaxReturn : 2.522515296936035
Agent0_Eval_MinReturn : -89.36274719238281
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -39.00878143310547
Agent0_Train_StdReturn : 30.234403610229492
Agent0_Train_MaxReturn : 0.7210693359375
Agent0_Train_MinReturn : -77.03002166748047
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 235500
Agent0_TimeSinceStart : 830.1956722736359
Agent0_Critic_Loss : 1.7419335842132568
Agent0_Actor_Loss : -4.507709980010986
Agent0_Alpha_Loss : 4.776251316070557
Agent0_Temperature : 0.47716845075537934
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -33.33002471923828
Agent1_Eval_StdReturn : 27.796283721923828
Agent1_Eval_MaxReturn : 17.97801399230957
Agent1_Eval_MinReturn : -79.33570861816406
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -37.26982879638672
Agent1_Train_StdReturn : 43.675540924072266
Agent1_Train_MaxReturn : 15.07677936553955
Agent1_Train_MinReturn : -93.12113952636719
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 235500
Agent1_TimeSinceStart : 833.0077958106995
Agent1_Critic_Loss : 1.5639433860778809
Agent1_Actor_Loss : -4.348110198974609
Agent1_Alpha_Loss : 4.769301414489746
Agent1_Temperature : 0.4771633005858413
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -52.95105743408203
Agent0_Eval_StdReturn : 19.536685943603516
Agent0_Eval_MaxReturn : -13.1243896484375
Agent0_Eval_MinReturn : -83.34502410888672
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -54.58197021484375
Agent0_Train_StdReturn : 34.89081573486328
Agent0_Train_MaxReturn : -5.944140434265137
Agent0_Train_MinReturn : -122.88616180419922
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 237000
Agent0_TimeSinceStart : 835.8097381591797
Agent0_Critic_Loss : 1.380760908126831
Agent0_Actor_Loss : -4.482115745544434
Agent0_Alpha_Loss : 4.757006645202637
Agent0_Temperature : 0.4770285346575922
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -42.42204284667969
Agent1_Eval_StdReturn : 23.2338924407959
Agent1_Eval_MaxReturn : -16.487632751464844
Agent1_Eval_MinReturn : -97.34819030761719
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.684412002563477
Agent1_Train_StdReturn : 18.217193603515625
Agent1_Train_MaxReturn : 11.300786972045898
Agent1_Train_MinReturn : -50.340728759765625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 237000
Agent1_TimeSinceStart : 838.6176409721375
Agent1_Critic_Loss : 1.6172517538070679
Agent1_Actor_Loss : -4.443892478942871
Agent1_Alpha_Loss : 4.788769721984863
Agent1_Temperature : 0.4770230257716779
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -33.34855651855469
Agent0_Eval_StdReturn : 31.60883903503418
Agent0_Eval_MaxReturn : 5.877412796020508
Agent0_Eval_MinReturn : -84.5810546875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -38.37670135498047
Agent0_Train_StdReturn : 38.092586517333984
Agent0_Train_MaxReturn : 21.273483276367188
Agent0_Train_MinReturn : -122.20539093017578
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 238500
Agent0_TimeSinceStart : 841.4285306930542
Agent0_Critic_Loss : 1.494107723236084
Agent0_Actor_Loss : -4.49477481842041
Agent0_Alpha_Loss : 4.754749298095703
Agent0_Temperature : 0.47688867986007466
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -39.47550582885742
Agent1_Eval_StdReturn : 25.191303253173828
Agent1_Eval_MaxReturn : -6.896805763244629
Agent1_Eval_MinReturn : -85.07000732421875

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.302730560302734
Agent0_Eval_StdReturn : 23.44198989868164
Agent0_Eval_MaxReturn : -2.4195175170898438
Agent0_Eval_MinReturn : -86.68566131591797
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.915191650390625
Agent0_Train_StdReturn : 11.07414436340332
Agent0_Train_MaxReturn : -4.829449653625488
Agent0_Train_MinReturn : -45.71164321899414
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 225000
Agent0_TimeSinceStart : 803.2141470909119
Agent0_Critic_Loss : 0.3523031175136566
Agent0_Actor_Loss : -0.4497089982032776
Agent0_Alpha_Loss : 0.852356493473053
Agent0_Temperature : 0.0957754266465159
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -25.165348052978516
Agent1_Eval_StdReturn : 14.848860740661621
Agent1_Eval_MaxReturn : -1.9999759197235107
Agent1_Eval_MinReturn : -50.38339614868164
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -27.318599700927734
Agent1_Train_StdReturn : 12.467625617980957
Agent1_Train_MaxReturn : -9.414140701293945
Agent1_Train_MinReturn : -45.43958282470703
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 225000
Agent1_TimeSinceStart : 806.0555539131165
Agent1_Critic_Loss : 0.30549508333206177
Agent1_Actor_Loss : -0.4654506742954254
Agent1_Alpha_Loss : 0.8753007650375366
Agent1_Temperature : 0.09576685331467119
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -14.325955390930176
Agent0_Eval_StdReturn : 21.524871826171875
Agent0_Eval_MaxReturn : 22.254430770874023
Agent0_Eval_MinReturn : -53.62425994873047
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -24.67791748046875
Agent0_Train_StdReturn : 19.530059814453125
Agent0_Train_MaxReturn : 18.11823844909668
Agent0_Train_MinReturn : -49.941192626953125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 226500
Agent0_TimeSinceStart : 808.8706951141357
Agent0_Critic_Loss : 0.2987099885940552
Agent0_Actor_Loss : -0.44491538405418396
Agent0_Alpha_Loss : 0.8505823612213135
Agent0_Temperature : 0.09574885475941661
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.8280143737793
Agent1_Eval_StdReturn : 31.621122360229492
Agent1_Eval_MaxReturn : 23.65765953063965
Agent1_Eval_MinReturn : -96.2089614868164
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -17.88263702392578
Agent1_Train_StdReturn : 14.635814666748047
Agent1_Train_MaxReturn : 7.511260032653809
Agent1_Train_MinReturn : -44.604652404785156
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 226500
Agent1_TimeSinceStart : 811.6984272003174
Agent1_Critic_Loss : 0.35432055592536926
Agent1_Actor_Loss : -0.5266045331954956
Agent1_Alpha_Loss : 0.8556926250457764
Agent1_Temperature : 0.0957403341571689
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -16.03372573852539
Agent0_Eval_StdReturn : 30.39152717590332
Agent0_Eval_MaxReturn : 35.84992218017578
Agent0_Eval_MinReturn : -82.32571411132812
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.78561782836914
Agent0_Train_StdReturn : 20.195459365844727
Agent0_Train_MaxReturn : 19.585636138916016
Agent0_Train_MinReturn : -50.38702392578125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 228000
Agent0_TimeSinceStart : 814.5241456031799
Agent0_Critic_Loss : 0.2973341643810272
Agent0_Actor_Loss : -0.4666948616504669
Agent0_Alpha_Loss : 0.8513469696044922
Agent0_Temperature : 0.09572228660672615
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.55234146118164
Agent1_Eval_StdReturn : 27.988147735595703
Agent1_Eval_MaxReturn : 22.916141510009766
Agent1_Eval_MinReturn : -81.44622802734375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -16.785341262817383
Agent1_Train_StdReturn : 11.971729278564453
Agent1_Train_MaxReturn : 6.787696361541748
Agent1_Train_MinReturn : -27.631479263305664
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 228000
Agent1_TimeSinceStart : 817.3541989326477
Agent1_Critic_Loss : 0.3480384349822998
Agent1_Actor_Loss : -0.5620368123054504
Agent1_Alpha_Loss : 0.8639237284660339
Agent1_Temperature : 0.09571378246019134
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -26.43295669555664
Agent0_Eval_StdReturn : 6.105301856994629
Agent0_Eval_MaxReturn : -13.798460006713867
Agent0_Eval_MinReturn : -34.35853576660156
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -7.908782958984375
Agent0_Train_StdReturn : 25.259780883789062
Agent0_Train_MaxReturn : 23.060977935791016
Agent0_Train_MinReturn : -50.024803161621094
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 229500
Agent0_TimeSinceStart : 820.1857044696808
Agent0_Critic_Loss : 0.342303991317749
Agent0_Actor_Loss : -0.5494744777679443
Agent0_Alpha_Loss : 0.8486299514770508
Agent0_Temperature : 0.09569572924500001
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -22.666461944580078
Agent1_Eval_StdReturn : 22.395132064819336
Agent1_Eval_MaxReturn : 6.18588924407959
Agent1_Eval_MinReturn : -62.2142333984375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -16.601966857910156
Agent1_Train_StdReturn : 19.28215789794922
Agent1_Train_MaxReturn : 14.738680839538574
Agent1_Train_MinReturn : -47.862430572509766
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 229500
Agent1_TimeSinceStart : 823.0131711959839
Agent1_Critic_Loss : 0.3543277680873871
Agent1_Actor_Loss : -0.5822229385375977
Agent1_Alpha_Loss : 0.8371348977088928
Agent1_Temperature : 0.09568727928008405
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -28.93743324279785
Agent0_Eval_StdReturn : 16.275550842285156
Agent0_Eval_MaxReturn : -3.8799149990081787
Agent0_Eval_MinReturn : -65.00157928466797
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -16.509265899658203
Agent0_Train_StdReturn : 21.607864379882812
Agent0_Train_MaxReturn : 17.074005126953125
Agent0_Train_MinReturn : -62.277503967285156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 231000
Agent0_TimeSinceStart : 825.8509850502014
Agent0_Critic_Loss : 0.34434884786605835
Agent0_Actor_Loss : -0.5006337761878967
Agent0_Alpha_Loss : 0.8439524173736572
Agent0_Temperature : 0.09566919473970203
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -35.27030563354492
Agent1_Train_StdReturn : 21.54656410217285
Agent1_Train_MaxReturn : 5.21164608001709
Agent1_Train_MinReturn : -68.05844116210938
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 238500
Agent1_TimeSinceStart : 844.2469563484192
Agent1_Critic_Loss : 1.633805751800537
Agent1_Actor_Loss : -4.490854263305664
Agent1_Alpha_Loss : 4.754044532775879
Agent1_Temperature : 0.4768828527423399
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -24.172948837280273
Agent0_Eval_StdReturn : 21.882404327392578
Agent0_Eval_MaxReturn : 6.546388626098633
Agent0_Eval_MinReturn : -53.5433464050293
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -31.138769149780273
Agent0_Train_StdReturn : 28.860374450683594
Agent0_Train_MaxReturn : 12.75579833984375
Agent0_Train_MinReturn : -80.45368957519531
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 240000
Agent0_TimeSinceStart : 847.0635325908661
Agent0_Critic_Loss : 1.420236349105835
Agent0_Actor_Loss : -4.386627197265625
Agent0_Alpha_Loss : 4.755297660827637
Agent0_Temperature : 0.47674888070737015
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -44.75930404663086
Agent1_Eval_StdReturn : 30.33030128479004
Agent1_Eval_MaxReturn : 16.53896713256836
Agent1_Eval_MinReturn : -108.38522338867188
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -25.451324462890625
Agent1_Train_StdReturn : 27.379112243652344
Agent1_Train_MaxReturn : 13.635780334472656
Agent1_Train_MinReturn : -65.90152740478516
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 240000
Agent1_TimeSinceStart : 849.8944733142853
Agent1_Critic_Loss : 1.4765057563781738
Agent1_Actor_Loss : -4.391880035400391
Agent1_Alpha_Loss : 4.736978054046631
Agent1_Temperature : 0.47674282007440705
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.827362060546875
Agent0_Eval_StdReturn : 27.459064483642578
Agent0_Eval_MaxReturn : 25.02104949951172
Agent0_Eval_MinReturn : -61.241703033447266
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -31.388519287109375
Agent0_Train_StdReturn : 24.85824203491211
Agent0_Train_MaxReturn : 26.419309616088867
Agent0_Train_MinReturn : -68.62364196777344
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 241500
Agent0_TimeSinceStart : 852.7214608192444
Agent0_Critic_Loss : 1.5822968482971191
Agent0_Actor_Loss : -4.3409223556518555
Agent0_Alpha_Loss : 4.751738548278809
Agent0_Temperature : 0.476609143413948
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -34.92291259765625
Agent1_Eval_StdReturn : 32.313499450683594
Agent1_Eval_MaxReturn : 15.943380355834961
Agent1_Eval_MinReturn : -91.61302185058594
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -38.3770751953125
Agent1_Train_StdReturn : 34.38764953613281
Agent1_Train_MaxReturn : 45.79875946044922
Agent1_Train_MinReturn : -74.86233520507812
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 241500
Agent1_TimeSinceStart : 855.5357213020325
Agent1_Critic_Loss : 1.5334558486938477
Agent1_Actor_Loss : -4.551942825317383
Agent1_Alpha_Loss : 4.752976894378662
Agent1_Temperature : 0.4766028715024247
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -51.28185272216797
Agent0_Eval_StdReturn : 27.786256790161133
Agent0_Eval_MaxReturn : 7.937265396118164
Agent0_Eval_MinReturn : -94.9580307006836
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -46.322715759277344
Agent0_Train_StdReturn : 31.33588218688965
Agent0_Train_MaxReturn : -1.5509796142578125
Agent0_Train_MinReturn : -88.19740295410156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 243000
Agent0_TimeSinceStart : 858.3559017181396
Agent0_Critic_Loss : 1.387924313545227
Agent0_Actor_Loss : -4.287981986999512
Agent0_Alpha_Loss : 4.729541778564453
Agent0_Temperature : 0.4764695247058982
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -50.238136291503906
Agent1_Eval_StdReturn : 26.390729904174805
Agent1_Eval_MaxReturn : -13.103185653686523
Agent1_Eval_MinReturn : -100.09239196777344
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -27.277721405029297
Agent1_Train_StdReturn : 28.231964111328125
Agent1_Train_MaxReturn : 18.57988739013672
Agent1_Train_MinReturn : -84.82626342773438
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 243000
Agent1_TimeSinceStart : 861.1807794570923
Agent1_Critic_Loss : 1.6562557220458984
Agent1_Actor_Loss : -4.6044464111328125
Agent1_Alpha_Loss : 4.780583381652832
Agent1_Temperature : 0.4764629248557774
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.989288330078125
Agent0_Eval_StdReturn : 37.653934478759766
Agent0_Eval_MaxReturn : 26.44657325744629
Agent0_Eval_MinReturn : -87.11271667480469
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -55.92521286010742
Agent0_Train_StdReturn : 21.915241241455078
Agent0_Train_MaxReturn : -6.517898082733154
Agent0_Train_MinReturn : -84.40655517578125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 244500
Agent0_TimeSinceStart : 864.0081496238708
Agent0_Critic_Loss : 1.4110010862350464
Agent0_Actor_Loss : -4.14631986618042
Agent0_Alpha_Loss : 4.739630699157715
Agent0_Temperature : 0.47632998661799386
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -28.48415184020996
Agent1_Eval_StdReturn : 27.008378982543945
Agent1_Eval_MaxReturn : 11.378602981567383
Agent1_Eval_MinReturn : -92.9465560913086
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -41.59557342529297
Agent1_Train_StdReturn : 29.65787124633789
Agent1_Train_MaxReturn : 1.2435798645019531
Agent1_Train_MinReturn : -96.28738403320312
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 244500
Agent1_TimeSinceStart : 866.8364818096161
Agent1_Critic_Loss : 1.6156812906265259
Agent1_Actor_Loss : -4.582322120666504
Agent1_Alpha_Loss : 4.778602123260498
Agent1_Temperature : 0.4763229878793953
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.27151870727539
Agent0_Eval_StdReturn : 24.587560653686523
Agent0_Eval_MaxReturn : -8.208765029907227
Agent0_Eval_MinReturn : -73.5544662475586
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.20352554321289
Agent1_Eval_AverageReturn : -32.41179656982422
Agent1_Eval_StdReturn : 21.18373680114746
Agent1_Eval_MaxReturn : 2.4480581283569336
Agent1_Eval_MinReturn : -74.3843765258789
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -27.537710189819336
Agent1_Train_StdReturn : 17.797801971435547
Agent1_Train_MaxReturn : 14.877427101135254
Agent1_Train_MinReturn : -51.65608215332031
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 231000
Agent1_TimeSinceStart : 828.6834721565247
Agent1_Critic_Loss : 0.32312607765197754
Agent1_Actor_Loss : -0.5809172987937927
Agent1_Alpha_Loss : 0.8627078533172607
Agent1_Temperature : 0.09566074436588304
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -30.039541244506836
Agent0_Eval_StdReturn : 21.386762619018555
Agent0_Eval_MaxReturn : 9.235265731811523
Agent0_Eval_MinReturn : -69.06135559082031
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -21.355731964111328
Agent0_Train_StdReturn : 17.828067779541016
Agent0_Train_MaxReturn : -2.649118423461914
Agent0_Train_MinReturn : -59.03020095825195
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 232500
Agent0_TimeSinceStart : 831.5316262245178
Agent0_Critic_Loss : 0.27255165576934814
Agent0_Actor_Loss : -0.49972522258758545
Agent0_Alpha_Loss : 0.8374916911125183
Agent0_Temperature : 0.0956426991311686
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -16.599830627441406
Agent1_Eval_StdReturn : 11.524429321289062
Agent1_Eval_MaxReturn : 7.564499855041504
Agent1_Eval_MinReturn : -34.423667907714844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -19.23866844177246
Agent1_Train_StdReturn : 17.94594955444336
Agent1_Train_MaxReturn : 11.450050354003906
Agent1_Train_MinReturn : -58.03130340576172
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 232500
Agent1_TimeSinceStart : 834.3728704452515
Agent1_Critic_Loss : 0.3260606825351715
Agent1_Actor_Loss : -0.5572832822799683
Agent1_Alpha_Loss : 0.8570600152015686
Agent1_Temperature : 0.09563419704175684
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -23.579387664794922
Agent0_Eval_StdReturn : 19.416595458984375
Agent0_Eval_MaxReturn : 9.381689071655273
Agent0_Eval_MinReturn : -59.805442810058594
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.719274520874023
Agent0_Train_StdReturn : 18.083311080932617
Agent0_Train_MaxReturn : -1.19989013671875
Agent0_Train_MinReturn : -66.10829162597656
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 234000
Agent0_TimeSinceStart : 837.2088630199432
Agent0_Critic_Loss : 0.2784753441810608
Agent0_Actor_Loss : -0.5003286004066467
Agent0_Alpha_Loss : 0.8488594889640808
Agent0_Temperature : 0.09561620452953774
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -25.559667587280273
Agent1_Eval_StdReturn : 14.981328964233398
Agent1_Eval_MaxReturn : 3.9928159713745117
Agent1_Eval_MinReturn : -45.199058532714844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -15.754582405090332
Agent1_Train_StdReturn : 8.522726058959961
Agent1_Train_MaxReturn : -1.8837957382202148
Agent1_Train_MinReturn : -28.04290199279785
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 234000
Agent1_TimeSinceStart : 840.0611629486084
Agent1_Critic_Loss : 0.26216578483581543
Agent1_Actor_Loss : -0.5499019622802734
Agent1_Alpha_Loss : 0.8488746881484985
Agent1_Temperature : 0.09560766205030431
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -25.697895050048828
Agent0_Eval_StdReturn : 32.46489334106445
Agent0_Eval_MaxReturn : 16.508893966674805
Agent0_Eval_MinReturn : -90.23499298095703
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -20.147216796875
Agent0_Train_StdReturn : 18.90652847290039
Agent0_Train_MaxReturn : 30.080102920532227
Agent0_Train_MinReturn : -40.74940872192383
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 235500
Agent0_TimeSinceStart : 842.9105551242828
Agent0_Critic_Loss : 0.37957173585891724
Agent0_Actor_Loss : -0.5032374858856201
Agent0_Alpha_Loss : 0.8654927015304565
Agent0_Temperature : 0.09558966161924778
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -18.60430908203125
Agent1_Eval_StdReturn : 19.402677536010742
Agent1_Eval_MaxReturn : -0.4603615999221802
Agent1_Eval_MinReturn : -71.20367431640625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -21.6583251953125
Agent1_Train_StdReturn : 20.154340744018555
Agent1_Train_MaxReturn : -1.0851554870605469
Agent1_Train_MinReturn : -75.22918701171875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 235500
Agent1_TimeSinceStart : 845.760986328125
Agent1_Critic_Loss : 0.23226472735404968
Agent1_Actor_Loss : -0.5660662651062012
Agent1_Alpha_Loss : 0.8355515003204346
Agent1_Temperature : 0.09558117661196211
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -25.082731246948242
Agent0_Eval_StdReturn : 16.592527389526367
Agent0_Eval_MaxReturn : -7.299361228942871
Agent0_Eval_MinReturn : -56.110782623291016
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -25.47100830078125
Agent0_Train_StdReturn : 27.09816551208496
Agent0_Train_MaxReturn : 14.790878295898438
Agent0_Train_MinReturn : -85.69217681884766
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 237000
Agent0_TimeSinceStart : 848.6283121109009
Agent0_Critic_Loss : 0.31191933155059814
Agent0_Actor_Loss : -0.4838114082813263
Agent0_Alpha_Loss : 0.8594134449958801
Agent0_Temperature : 0.09556309282604011
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -18.58761978149414
Agent1_Eval_StdReturn : 19.98621940612793
Agent1_Eval_MaxReturn : 4.762022972106934
Agent1_Eval_MinReturn : -63.73398208618164
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -17.449268341064453
Agent1_Train_StdReturn : 14.95743465423584
Agent1_Train_MaxReturn : 0.3078641891479492
Agent1_Train_MinReturn : -56.96544647216797
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 237000
Agent1_TimeSinceStart : 851.4838943481445
Agent1_Critic_Loss : 0.29282325506210327
Agent1_Actor_Loss : -0.5853321552276611
Agent1_Alpha_Loss : 0.8704447746276855
Agent1_Temperature : 0.09555463298929955
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -18.733776092529297
Agent0_Train_StdReturn : 38.41889953613281
Agent0_Train_MaxReturn : 21.458410263061523
Agent0_Train_MinReturn : -113.68365478515625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 246000
Agent0_TimeSinceStart : 869.672465801239
Agent0_Critic_Loss : 1.256685733795166
Agent0_Actor_Loss : -4.214154243469238
Agent0_Alpha_Loss : 4.749309539794922
Agent0_Temperature : 0.47619049631260973
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -38.774234771728516
Agent1_Eval_StdReturn : 26.871688842773438
Agent1_Eval_MaxReturn : 15.22520923614502
Agent1_Eval_MinReturn : -86.70271301269531
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -38.02354431152344
Agent1_Train_StdReturn : 43.15815734863281
Agent1_Train_MaxReturn : 29.104698181152344
Agent1_Train_MinReturn : -89.0107650756836
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 246000
Agent1_TimeSinceStart : 872.494042634964
Agent1_Critic_Loss : 1.5965816974639893
Agent1_Actor_Loss : -4.537989616394043
Agent1_Alpha_Loss : 4.780290126800537
Agent1_Temperature : 0.476183057456298
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -42.525543212890625
Agent0_Eval_StdReturn : 27.14666748046875
Agent0_Eval_MaxReturn : 2.936155319213867
Agent0_Eval_MinReturn : -77.67256927490234
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -34.12786102294922
Agent0_Train_StdReturn : 38.67269515991211
Agent0_Train_MaxReturn : 13.823911666870117
Agent0_Train_MinReturn : -112.58478546142578
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 247500
Agent0_TimeSinceStart : 875.3088481426239
Agent0_Critic_Loss : 1.7011091709136963
Agent0_Actor_Loss : -4.2314863204956055
Agent0_Alpha_Loss : 4.7332329750061035
Agent0_Temperature : 0.4760510951895878
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -27.608652114868164
Agent1_Eval_StdReturn : 28.534652709960938
Agent1_Eval_MaxReturn : 9.791698455810547
Agent1_Eval_MinReturn : -90.30985260009766
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.91864585876465
Agent1_Train_StdReturn : 30.057941436767578
Agent1_Train_MaxReturn : 10.181989669799805
Agent1_Train_MinReturn : -79.386474609375
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 247500
Agent1_TimeSinceStart : 878.1268422603607
Agent1_Critic_Loss : 1.484978437423706
Agent1_Actor_Loss : -4.52425479888916
Agent1_Alpha_Loss : 4.735652923583984
Agent1_Temperature : 0.4760432580183879
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -44.78722381591797
Agent0_Eval_StdReturn : 34.90200424194336
Agent0_Eval_MaxReturn : 1.341604232788086
Agent0_Eval_MinReturn : -112.0748062133789
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -48.61848831176758
Agent0_Train_StdReturn : 38.699745178222656
Agent0_Train_MaxReturn : 14.35681438446045
Agent0_Train_MinReturn : -106.42777252197266
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 249000
Agent0_TimeSinceStart : 880.962723493576
Agent0_Critic_Loss : 1.631030797958374
Agent0_Actor_Loss : -4.348008155822754
Agent0_Alpha_Loss : 4.770761489868164
Agent0_Temperature : 0.47591167295228354
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -47.506103515625
Agent1_Eval_StdReturn : 34.66083908081055
Agent1_Eval_MaxReturn : 7.7332563400268555
Agent1_Eval_MinReturn : -117.60413360595703
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -31.662738800048828
Agent1_Train_StdReturn : 26.220829010009766
Agent1_Train_MaxReturn : -8.079187393188477
Agent1_Train_MinReturn : -89.80842590332031
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 249000
Agent1_TimeSinceStart : 883.8806900978088
Agent1_Critic_Loss : 1.5260645151138306
Agent1_Actor_Loss : -4.569269180297852
Agent1_Alpha_Loss : 4.7426347732543945
Agent1_Temperature : 0.4759035590458045
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.86689758300781
Agent0_Eval_StdReturn : 36.69198226928711
Agent0_Eval_MaxReturn : 3.203829765319824
Agent0_Eval_MinReturn : -106.25949096679688
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -45.769775390625
Agent0_Train_StdReturn : 30.311506271362305
Agent0_Train_MaxReturn : 3.6262340545654297
Agent0_Train_MinReturn : -88.63922119140625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 250500
Agent0_TimeSinceStart : 886.8128361701965
Agent0_Critic_Loss : 1.4898831844329834
Agent0_Actor_Loss : -4.493597030639648
Agent0_Alpha_Loss : 4.7373480796813965
Agent0_Temperature : 0.4757723259787531
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -35.295875549316406
Agent1_Eval_StdReturn : 26.042869567871094
Agent1_Eval_MaxReturn : 10.948158264160156
Agent1_Eval_MinReturn : -79.89122772216797
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.579524993896484
Agent1_Train_StdReturn : 24.274600982666016
Agent1_Train_MaxReturn : -9.10073471069336
Agent1_Train_MinReturn : -102.8703384399414
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 250500
Agent1_TimeSinceStart : 889.730895280838
Agent1_Critic_Loss : 1.6864768266677856
Agent1_Actor_Loss : -4.5409746170043945
Agent1_Alpha_Loss : 4.712953090667725
Agent1_Temperature : 0.47576403397845285
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -12.511186599731445
Agent0_Eval_StdReturn : 23.904705047607422
Agent0_Eval_MaxReturn : 16.793821334838867
Agent0_Eval_MinReturn : -56.41179656982422
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -38.721004486083984
Agent0_Train_StdReturn : 31.05068588256836
Agent0_Train_MaxReturn : 16.006046295166016
Agent0_Train_MinReturn : -95.88624572753906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 252000
Agent0_TimeSinceStart : 892.6525912284851
Agent0_Critic_Loss : 1.6176928281784058
Agent0_Actor_Loss : -4.4701738357543945
Agent0_Alpha_Loss : 4.71141242980957
Agent0_Temperature : 0.475633119936211
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -36.730464935302734
Agent1_Eval_StdReturn : 24.649890899658203
Agent1_Eval_MaxReturn : -13.226126670837402
Agent1_Eval_MinReturn : -91.85630798339844
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -15.960406303405762
Agent1_Train_StdReturn : 39.77964782714844
Agent1_Train_MaxReturn : 37.43801498413086
Agent0_Eval_StdReturn : 14.908087730407715
Agent0_Eval_MaxReturn : -1.4622679948806763
Agent0_Eval_MinReturn : -43.85247039794922
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -9.539794921875
Agent0_Train_StdReturn : 17.50929069519043
Agent0_Train_MaxReturn : 12.22372817993164
Agent0_Train_MinReturn : -41.16728973388672
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 238500
Agent0_TimeSinceStart : 854.3306288719177
Agent0_Critic_Loss : 0.3282390236854553
Agent0_Actor_Loss : -0.5021111965179443
Agent0_Alpha_Loss : 0.8439538478851318
Agent0_Temperature : 0.09553654577527433
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -28.929889678955078
Agent1_Eval_StdReturn : 30.85317039489746
Agent1_Eval_MaxReturn : 6.640381813049316
Agent1_Eval_MinReturn : -78.50790405273438
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -19.313940048217773
Agent1_Train_StdReturn : 14.345088958740234
Agent1_Train_MaxReturn : 3.9274301528930664
Agent1_Train_MinReturn : -42.059783935546875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 238500
Agent1_TimeSinceStart : 857.1714494228363
Agent1_Critic_Loss : 0.27373751997947693
Agent1_Actor_Loss : -0.5921797752380371
Agent1_Alpha_Loss : 0.8391010761260986
Agent1_Temperature : 0.09552812855704217
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -21.838180541992188
Agent0_Eval_StdReturn : 12.927840232849121
Agent0_Eval_MaxReturn : -0.44644927978515625
Agent0_Eval_MinReturn : -36.1618766784668
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -25.908884048461914
Agent0_Train_StdReturn : 23.42647933959961
Agent0_Train_MaxReturn : 3.8873941898345947
Agent0_Train_MinReturn : -76.27718353271484
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 240000
Agent0_TimeSinceStart : 860.01340675354
Agent0_Critic_Loss : 0.3873841166496277
Agent0_Actor_Loss : -0.40022343397140503
Agent0_Alpha_Loss : 0.8582311868667603
Agent0_Temperature : 0.09550997586292843
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -15.470819473266602
Agent1_Eval_StdReturn : 15.665258407592773
Agent1_Eval_MaxReturn : 7.079885959625244
Agent1_Eval_MinReturn : -38.84166717529297
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -25.834590911865234
Agent1_Train_StdReturn : 17.784608840942383
Agent1_Train_MaxReturn : 9.732404708862305
Agent1_Train_MinReturn : -52.16992950439453
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 240000
Agent1_TimeSinceStart : 862.8690893650055
Agent1_Critic_Loss : 0.2729226350784302
Agent1_Actor_Loss : -0.6355482339859009
Agent1_Alpha_Loss : 0.8629525303840637
Agent1_Temperature : 0.09550158891187019
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -15.522297859191895
Agent0_Eval_StdReturn : 15.986647605895996
Agent0_Eval_MaxReturn : 4.896818161010742
Agent0_Eval_MinReturn : -43.299068450927734
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -21.344802856445312
Agent0_Train_StdReturn : 28.217731475830078
Agent0_Train_MaxReturn : 6.767124652862549
Agent0_Train_MinReturn : -88.13501739501953
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 241500
Agent0_TimeSinceStart : 865.8003671169281
Agent0_Critic_Loss : 0.3781590461730957
Agent0_Actor_Loss : -0.43539661169052124
Agent0_Alpha_Loss : 0.8649840354919434
Agent0_Temperature : 0.09548336527389238
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.95278549194336
Agent1_Eval_StdReturn : 16.347854614257812
Agent1_Eval_MaxReturn : 10.545025825500488
Agent1_Eval_MinReturn : -43.66126251220703
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -17.711214065551758
Agent1_Train_StdReturn : 14.597676277160645
Agent1_Train_MaxReturn : 3.029137134552002
Agent1_Train_MinReturn : -45.241695404052734
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 241500
Agent1_TimeSinceStart : 868.7502958774567
Agent1_Critic_Loss : 0.26566803455352783
Agent1_Actor_Loss : -0.5392134189605713
Agent1_Alpha_Loss : 0.8387526273727417
Agent1_Temperature : 0.09547508822642457
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -25.116710662841797
Agent0_Eval_StdReturn : 21.628454208374023
Agent0_Eval_MaxReturn : 4.723609924316406
Agent0_Eval_MinReturn : -60.99651336669922
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -10.506460189819336
Agent0_Train_StdReturn : 21.90461540222168
Agent0_Train_MaxReturn : 28.834341049194336
Agent0_Train_MinReturn : -60.721778869628906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 243000
Agent0_TimeSinceStart : 871.6880474090576
Agent0_Critic_Loss : 0.4105670750141144
Agent0_Actor_Loss : -0.42319759726524353
Agent0_Alpha_Loss : 0.8491812348365784
Agent0_Temperature : 0.09545676429137523
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -18.00345802307129
Agent1_Eval_StdReturn : 23.984251022338867
Agent1_Eval_MaxReturn : 13.419527053833008
Agent1_Eval_MinReturn : -82.75593566894531
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -28.17697525024414
Agent1_Train_StdReturn : 24.789756774902344
Agent1_Train_MaxReturn : 6.1284332275390625
Agent1_Train_MinReturn : -82.07508850097656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 243000
Agent1_TimeSinceStart : 874.6284708976746
Agent1_Critic_Loss : 0.24840721487998962
Agent1_Actor_Loss : -0.5749790668487549
Agent1_Alpha_Loss : 0.8581650257110596
Agent1_Temperature : 0.09544856507038467
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -29.669076919555664
Agent0_Eval_StdReturn : 25.50718116760254
Agent0_Eval_MaxReturn : 2.9558088779449463
Agent0_Eval_MinReturn : -88.05538940429688
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -15.799013137817383
Agent0_Train_StdReturn : 14.086813926696777
Agent0_Train_MaxReturn : 13.202301025390625
Agent0_Train_MinReturn : -35.51336669921875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 244500
Agent0_TimeSinceStart : 877.5626680850983
Agent0_Critic_Loss : 0.3351554870605469
Agent0_Actor_Loss : -0.44714176654815674
Agent0_Alpha_Loss : 0.8539511561393738
Agent0_Temperature : 0.09543015752513943
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -21.933521270751953
Agent1_Eval_StdReturn : 18.8351993560791
Agent1_Train_MinReturn : -84.36094665527344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 252000
Agent1_TimeSinceStart : 895.5622866153717
Agent1_Critic_Loss : 1.3505678176879883
Agent1_Actor_Loss : -4.420360565185547
Agent1_Alpha_Loss : 4.72423791885376
Agent1_Temperature : 0.47562463581979936
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -50.23358154296875
Agent0_Eval_StdReturn : 20.593721389770508
Agent0_Eval_MaxReturn : -19.02620506286621
Agent0_Eval_MinReturn : -78.4856948852539
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -36.779624938964844
Agent0_Train_StdReturn : 23.470178604125977
Agent0_Train_MaxReturn : 2.835188388824463
Agent0_Train_MinReturn : -66.80416870117188
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 253500
Agent0_TimeSinceStart : 898.4771203994751
Agent0_Critic_Loss : 1.610774040222168
Agent0_Actor_Loss : -4.528306007385254
Agent0_Alpha_Loss : 4.728968620300293
Agent0_Temperature : 0.47549399387684804
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -31.41556739807129
Agent1_Eval_StdReturn : 34.62794494628906
Agent1_Eval_MaxReturn : 16.515207290649414
Agent1_Eval_MinReturn : -96.46170043945312
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -34.16669464111328
Agent1_Train_StdReturn : 18.50137710571289
Agent1_Train_MaxReturn : 12.06668472290039
Agent1_Train_MinReturn : -60.13597869873047
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 253500
Agent1_TimeSinceStart : 901.388605594635
Agent1_Critic_Loss : 1.648453712463379
Agent1_Actor_Loss : -4.53770637512207
Agent1_Alpha_Loss : 4.7433671951293945
Agent1_Temperature : 0.47548530092128016
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.63209915161133
Agent0_Eval_StdReturn : 29.032663345336914
Agent0_Eval_MaxReturn : -16.147554397583008
Agent0_Eval_MinReturn : -113.25699615478516
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -57.79426193237305
Agent0_Train_StdReturn : 32.5797119140625
Agent0_Train_MaxReturn : -14.69091510772705
Agent0_Train_MinReturn : -126.23121643066406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 255000
Agent0_TimeSinceStart : 904.2957901954651
Agent0_Critic_Loss : 1.5456352233886719
Agent0_Actor_Loss : -4.4986724853515625
Agent0_Alpha_Loss : 4.731666564941406
Agent0_Temperature : 0.4753549341533622
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -27.288991928100586
Agent1_Eval_StdReturn : 30.86688804626465
Agent1_Eval_MaxReturn : 26.284608840942383
Agent1_Eval_MinReturn : -62.29949188232422
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.12464141845703
Agent1_Train_StdReturn : 20.858901977539062
Agent1_Train_MaxReturn : -9.666072845458984
Agent1_Train_MinReturn : -78.80369567871094
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 255000
Agent1_TimeSinceStart : 907.2066311836243
Agent1_Critic_Loss : 1.712411642074585
Agent1_Actor_Loss : -4.479488372802734
Agent1_Alpha_Loss : 4.72233772277832
Agent1_Temperature : 0.4753460827938941
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -19.160287857055664
Agent0_Eval_StdReturn : 22.05213737487793
Agent0_Eval_MaxReturn : 22.06127166748047
Agent0_Eval_MinReturn : -54.06526565551758
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -37.724830627441406
Agent0_Train_StdReturn : 27.334856033325195
Agent0_Train_MaxReturn : -4.929955959320068
Agent0_Train_MinReturn : -94.52151489257812
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 256500
Agent0_TimeSinceStart : 910.1114444732666
Agent0_Critic_Loss : 1.005096197128296
Agent0_Actor_Loss : -4.311100482940674
Agent0_Alpha_Loss : 4.708813667297363
Agent0_Temperature : 0.4752159988818042
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -43.54633331298828
Agent1_Eval_StdReturn : 38.87662887573242
Agent1_Eval_MaxReturn : 23.58274269104004
Agent1_Eval_MinReturn : -106.06134796142578
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -45.53382873535156
Agent1_Train_StdReturn : 28.3851318359375
Agent1_Train_MaxReturn : 1.168508529663086
Agent1_Train_MinReturn : -86.58966827392578
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 256500
Agent1_TimeSinceStart : 913.0199592113495
Agent1_Critic_Loss : 1.5032336711883545
Agent1_Actor_Loss : -4.48898458480835
Agent1_Alpha_Loss : 4.742363929748535
Agent1_Temperature : 0.4752069163290733
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -50.24851608276367
Agent0_Eval_StdReturn : 50.803653717041016
Agent0_Eval_MaxReturn : 6.795853137969971
Agent0_Eval_MinReturn : -158.52532958984375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -54.28571319580078
Agent0_Train_StdReturn : 27.802513122558594
Agent0_Train_MaxReturn : 13.472742080688477
Agent0_Train_MinReturn : -86.33175659179688
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 258000
Agent0_TimeSinceStart : 915.9288551807404
Agent0_Critic_Loss : 1.2205803394317627
Agent0_Actor_Loss : -4.402237892150879
Agent0_Alpha_Loss : 4.7500810623168945
Agent0_Temperature : 0.4750770634920728
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.26599884033203
Agent1_Eval_StdReturn : 27.68927574157715
Agent1_Eval_MaxReturn : 16.11884307861328
Agent1_Eval_MinReturn : -65.74227905273438
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -32.568878173828125
Agent1_Train_StdReturn : 43.042903900146484
Agent1_Train_MaxReturn : 27.443025588989258
Agent1_Train_MinReturn : -138.96377563476562
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 258000
Agent1_TimeSinceStart : 918.8376202583313
Agent1_Critic_Loss : 1.6565382480621338
Agent1_Actor_Loss : -4.4817705154418945
Agent1_Alpha_Loss : 4.735558032989502
Agent1_Temperature : 0.47506781711499113
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -26.531179428100586
Agent0_Eval_StdReturn : 18.255069732666016
Agent0_Eval_MaxReturn : 18.165504455566406
Agent0_Eval_MinReturn : -46.135475158691406
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -31.834957122802734
Agent0_Train_StdReturn : 36.169715881347656
Agent0_Train_MaxReturn : 13.752971649169922
Agent0_Train_MinReturn : -109.36009216308594
Agent0_Train_AverageEpLen : 150.0
Agent1_Eval_MaxReturn : -0.5107030868530273
Agent1_Eval_MinReturn : -70.99071502685547
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -19.146080017089844
Agent1_Train_StdReturn : 17.484342575073242
Agent1_Train_MaxReturn : 18.118927001953125
Agent1_Train_MinReturn : -41.22566223144531
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 244500
Agent1_TimeSinceStart : 880.5019419193268
Agent1_Critic_Loss : 0.27604055404663086
Agent1_Actor_Loss : -0.6120032668113708
Agent1_Alpha_Loss : 0.8427579402923584
Agent1_Temperature : 0.09542206657785597
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -24.92923927307129
Agent0_Eval_StdReturn : 26.505624771118164
Agent0_Eval_MaxReturn : 19.338851928710938
Agent0_Eval_MinReturn : -63.30729293823242
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -12.012189865112305
Agent0_Train_StdReturn : 14.964753150939941
Agent0_Train_MaxReturn : 7.365784645080566
Agent0_Train_MinReturn : -39.60939407348633
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 246000
Agent0_TimeSinceStart : 883.4355165958405
Agent0_Critic_Loss : 0.34606555104255676
Agent0_Actor_Loss : -0.4365270137786865
Agent0_Alpha_Loss : 0.8607059717178345
Agent0_Temperature : 0.0954035253798614
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -14.812501907348633
Agent1_Eval_StdReturn : 21.542003631591797
Agent1_Eval_MaxReturn : 40.116722106933594
Agent1_Eval_MinReturn : -48.85131072998047
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -10.5410795211792
Agent1_Train_StdReturn : 17.417430877685547
Agent1_Train_MaxReturn : 23.37264060974121
Agent1_Train_MinReturn : -38.87956237792969
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 246000
Agent1_TimeSinceStart : 886.3687512874603
Agent1_Critic_Loss : 0.29932984709739685
Agent1_Actor_Loss : -0.5826860666275024
Agent1_Alpha_Loss : 0.8484511375427246
Agent1_Temperature : 0.09539557304003948
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -21.220020294189453
Agent0_Eval_StdReturn : 23.681835174560547
Agent0_Eval_MaxReturn : 22.985401153564453
Agent0_Eval_MinReturn : -60.612857818603516
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -27.586772918701172
Agent0_Train_StdReturn : 30.73847770690918
Agent0_Train_MaxReturn : 8.677005767822266
Agent0_Train_MinReturn : -87.92730712890625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 247500
Agent0_TimeSinceStart : 889.295595407486
Agent0_Critic_Loss : 0.3967922329902649
Agent0_Actor_Loss : -0.5137589573860168
Agent0_Alpha_Loss : 0.8375185132026672
Agent0_Temperature : 0.09537693838471724
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.689807891845703
Agent1_Eval_StdReturn : 19.890745162963867
Agent1_Eval_MaxReturn : 9.297651290893555
Agent1_Eval_MinReturn : -52.90635299682617
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -18.972509384155273
Agent1_Train_StdReturn : 10.55068302154541
Agent1_Train_MaxReturn : -5.693397521972656
Agent1_Train_MinReturn : -33.709136962890625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 247500
Agent1_TimeSinceStart : 892.2222921848297
Agent1_Critic_Loss : 0.29036200046539307
Agent1_Actor_Loss : -0.5452382564544678
Agent1_Alpha_Loss : 0.8223952651023865
Agent1_Temperature : 0.09536916013299329
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -21.91594886779785
Agent0_Eval_StdReturn : 10.586039543151855
Agent0_Eval_MaxReturn : -4.668973445892334
Agent0_Eval_MinReturn : -37.2770881652832
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -18.827892303466797
Agent0_Train_StdReturn : 15.801766395568848
Agent0_Train_MaxReturn : -0.3362407684326172
Agent0_Train_MinReturn : -51.46050262451172
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 249000
Agent0_TimeSinceStart : 895.1537296772003
Agent0_Critic_Loss : 0.34131526947021484
Agent0_Actor_Loss : -0.4832710027694702
Agent0_Alpha_Loss : 0.8525305986404419
Agent0_Temperature : 0.0953503472760512
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.97599220275879
Agent1_Eval_StdReturn : 18.095069885253906
Agent1_Eval_MaxReturn : -6.000646114349365
Agent1_Eval_MinReturn : -64.71634674072266
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -36.94164276123047
Agent1_Train_StdReturn : 26.508262634277344
Agent1_Train_MaxReturn : 17.680683135986328
Agent1_Train_MinReturn : -71.34545135498047
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 249000
Agent1_TimeSinceStart : 898.090398311615
Agent1_Critic_Loss : 0.2859498858451843
Agent1_Actor_Loss : -0.5271913409233093
Agent1_Alpha_Loss : 0.8265050053596497
Agent1_Temperature : 0.09534280690570211
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -29.080181121826172
Agent0_Eval_StdReturn : 36.97113037109375
Agent0_Eval_MaxReturn : 1.1394844055175781
Agent0_Eval_MinReturn : -128.53363037109375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -13.758733749389648
Agent0_Train_StdReturn : 23.79916763305664
Agent0_Train_MaxReturn : 46.925392150878906
Agent0_Train_MinReturn : -45.41029357910156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 250500
Agent0_TimeSinceStart : 901.0229678153992
Agent0_Critic_Loss : 0.31113913655281067
Agent0_Actor_Loss : -0.4523523151874542
Agent0_Alpha_Loss : 0.8291833400726318
Agent0_Temperature : 0.0953238209391423
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.787227630615234
Agent1_Eval_StdReturn : 5.768559455871582
Agent1_Eval_MaxReturn : -12.625747680664062
Agent1_Eval_MinReturn : -31.280508041381836
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -16.888145446777344
Agent1_Train_StdReturn : 30.44525718688965
Agent1_Train_MaxReturn : 33.70942687988281
Agent1_Train_MinReturn : -50.16470718383789
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 250500
Agent1_TimeSinceStart : 903.9549283981323
Agent1_Critic_Loss : 0.24347856640815735
Agent1_Actor_Loss : -0.5183984041213989
Agent1_Alpha_Loss : 0.8255208730697632
Agent1_Temperature : 0.09531650957075716
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -8.212182998657227
Agent0_Eval_StdReturn : 16.91664695739746
Agent0_Eval_MaxReturn : 25.093233108520508
Agent0_Train_EnvstepsSoFar : 259500
Agent0_TimeSinceStart : 921.7467408180237
Agent0_Critic_Loss : 1.1477714776992798
Agent0_Actor_Loss : -4.377885818481445
Agent0_Alpha_Loss : 4.717386245727539
Agent0_Temperature : 0.4749382202221092
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -31.198354721069336
Agent1_Eval_StdReturn : 36.499263763427734
Agent1_Eval_MaxReturn : 3.8103408813476562
Agent1_Eval_MinReturn : -121.39108276367188
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -43.014793395996094
Agent1_Train_StdReturn : 45.56850814819336
Agent1_Train_MaxReturn : 23.586599349975586
Agent1_Train_MinReturn : -158.03897094726562
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 259500
Agent1_TimeSinceStart : 924.6625831127167
Agent1_Critic_Loss : 1.2648136615753174
Agent1_Actor_Loss : -4.602034568786621
Agent1_Alpha_Loss : 4.739263534545898
Agent1_Temperature : 0.4749287701625934
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -31.73089599609375
Agent0_Eval_StdReturn : 26.956789016723633
Agent0_Eval_MaxReturn : 10.630998611450195
Agent0_Eval_MinReturn : -82.8219223022461
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -51.431884765625
Agent0_Train_StdReturn : 36.79628372192383
Agent0_Train_MaxReturn : -0.16391944885253906
Agent0_Train_MinReturn : -114.37904357910156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 261000
Agent0_TimeSinceStart : 927.5751323699951
Agent0_Critic_Loss : 1.521820306777954
Agent0_Actor_Loss : -4.222982406616211
Agent0_Alpha_Loss : 4.73835563659668
Agent0_Temperature : 0.4747994037842095
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -36.43959045410156
Agent1_Eval_StdReturn : 22.303573608398438
Agent1_Eval_MaxReturn : -7.741784572601318
Agent1_Eval_MinReturn : -69.54265594482422
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -37.67803192138672
Agent1_Train_StdReturn : 26.966100692749023
Agent1_Train_MaxReturn : 17.84577178955078
Agent1_Train_MinReturn : -84.93891906738281
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 261000
Agent1_TimeSinceStart : 930.5021886825562
Agent1_Critic_Loss : 1.421118974685669
Agent1_Actor_Loss : -4.651246070861816
Agent1_Alpha_Loss : 4.756369590759277
Agent1_Temperature : 0.47478972518771795
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -45.98539352416992
Agent0_Eval_StdReturn : 30.855443954467773
Agent0_Eval_MaxReturn : 1.6783027648925781
Agent0_Eval_MinReturn : -94.64972686767578
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -35.391170501708984
Agent0_Train_StdReturn : 16.9228458404541
Agent0_Train_MaxReturn : -7.036410331726074
Agent0_Train_MinReturn : -67.5283432006836
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 262500
Agent0_TimeSinceStart : 933.4047949314117
Agent0_Critic_Loss : 1.2722804546356201
Agent0_Actor_Loss : -4.316985130310059
Agent0_Alpha_Loss : 4.686776638031006
Agent0_Temperature : 0.4746607556624881
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -22.882640838623047
Agent1_Eval_StdReturn : 37.85337829589844
Agent1_Eval_MaxReturn : 44.42829513549805
Agent1_Eval_MinReturn : -95.100341796875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -37.92890548706055
Agent1_Train_StdReturn : 19.930212020874023
Agent1_Train_MaxReturn : -4.957598686218262
Agent1_Train_MinReturn : -66.6666030883789
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 262500
Agent1_TimeSinceStart : 936.3112976551056
Agent1_Critic_Loss : 1.340625524520874
Agent1_Actor_Loss : -4.642281532287598
Agent1_Alpha_Loss : 4.697170257568359
Agent1_Temperature : 0.4746508473322209
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -28.713794708251953
Agent0_Eval_StdReturn : 34.67548370361328
Agent0_Eval_MaxReturn : 11.850950241088867
Agent0_Eval_MinReturn : -94.86630249023438
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -51.9009895324707
Agent0_Train_StdReturn : 26.355825424194336
Agent0_Train_MaxReturn : -5.519233703613281
Agent0_Train_MinReturn : -94.43673706054688
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 264000
Agent0_TimeSinceStart : 939.2271747589111
Agent0_Critic_Loss : 1.3574800491333008
Agent0_Actor_Loss : -4.413634300231934
Agent0_Alpha_Loss : 4.700400352478027
Agent0_Temperature : 0.4745222226901411
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -43.098175048828125
Agent1_Eval_StdReturn : 28.638446807861328
Agent1_Eval_MaxReturn : 3.3986968994140625
Agent1_Eval_MinReturn : -99.14218139648438
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -46.76500701904297
Agent1_Train_StdReturn : 31.673412322998047
Agent1_Train_MaxReturn : 16.417896270751953
Agent1_Train_MinReturn : -97.59493255615234
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 264000
Agent1_TimeSinceStart : 942.1379165649414
Agent1_Critic_Loss : 1.3340184688568115
Agent1_Actor_Loss : -4.656454086303711
Agent1_Alpha_Loss : 4.6929402351379395
Agent1_Temperature : 0.4745121329075194
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -49.244850158691406
Agent0_Eval_StdReturn : 42.76605987548828
Agent0_Eval_MaxReturn : 17.97132110595703
Agent0_Eval_MinReturn : -121.58293151855469
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -24.828380584716797
Agent0_Train_StdReturn : 24.747976303100586
Agent0_Train_MaxReturn : 20.704572677612305
Agent0_Train_MinReturn : -57.97970199584961
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 265500
Agent0_TimeSinceStart : 945.0509948730469
Agent0_Critic_Loss : 1.2728219032287598
Agent0_Actor_Loss : -4.356504440307617
Agent0_Alpha_Loss : 4.658347129821777
Agent0_Temperature : 0.4743839108144531
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -45.78926467895508
Agent1_Eval_StdReturn : 37.611602783203125
Agent1_Eval_MaxReturn : -1.3900175094604492
Agent1_Eval_MinReturn : -110.42486572265625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -32.730934143066406
Agent1_Train_StdReturn : 25.226320266723633
Agent1_Train_MaxReturn : 0.05208015441894531
Agent1_Train_MinReturn : -72.04345703125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 265500
Agent1_TimeSinceStart : 947.9588046073914
Agent0_Eval_MinReturn : -30.986621856689453
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -23.043109893798828
Agent0_Train_StdReturn : 18.637439727783203
Agent0_Train_MaxReturn : 3.1473913192749023
Agent0_Train_MinReturn : -53.07292938232422
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 252000
Agent0_TimeSinceStart : 906.8827941417694
Agent0_Critic_Loss : 0.3507995009422302
Agent0_Actor_Loss : -0.500685453414917
Agent0_Alpha_Loss : 0.8286517858505249
Agent0_Temperature : 0.09529735381950895
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -21.889339447021484
Agent1_Eval_StdReturn : 17.76420783996582
Agent1_Eval_MaxReturn : 2.327535629272461
Agent1_Eval_MinReturn : -48.44317626953125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -19.248001098632812
Agent1_Train_StdReturn : 17.725873947143555
Agent1_Train_MaxReturn : 2.1930646896362305
Agent1_Train_MinReturn : -60.59852600097656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 252000
Agent1_TimeSinceStart : 909.8278985023499
Agent1_Critic_Loss : 0.3024636507034302
Agent1_Actor_Loss : -0.536284863948822
Agent1_Alpha_Loss : 0.8218584060668945
Agent1_Temperature : 0.09529027261987305
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -28.12563133239746
Agent0_Eval_StdReturn : 16.264793395996094
Agent0_Eval_MaxReturn : 12.690896034240723
Agent0_Eval_MinReturn : -51.102760314941406
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -19.8775634765625
Agent0_Train_StdReturn : 15.866988182067871
Agent0_Train_MaxReturn : 1.3739280700683594
Agent0_Train_MinReturn : -46.281494140625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 253500
Agent0_TimeSinceStart : 912.7784202098846
Agent0_Critic_Loss : 0.3208848536014557
Agent0_Actor_Loss : -0.4152536392211914
Agent0_Alpha_Loss : 0.8263821601867676
Agent0_Temperature : 0.0952709460577543
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -24.220006942749023
Agent1_Eval_StdReturn : 10.624905586242676
Agent1_Eval_MaxReturn : -9.255959510803223
Agent1_Eval_MinReturn : -45.096923828125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -13.132234573364258
Agent1_Train_StdReturn : 17.983963012695312
Agent1_Train_MaxReturn : 24.529220581054688
Agent1_Train_MinReturn : -42.10490036010742
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 253500
Agent1_TimeSinceStart : 915.6959960460663
Agent1_Critic_Loss : 0.25064605474472046
Agent1_Actor_Loss : -0.6222524046897888
Agent1_Alpha_Loss : 0.83997642993927
Agent1_Temperature : 0.0952640358138767
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -22.213531494140625
Agent0_Eval_StdReturn : 16.253774642944336
Agent0_Eval_MaxReturn : 4.5605363845825195
Agent0_Eval_MinReturn : -51.07900619506836
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -28.699016571044922
Agent0_Train_StdReturn : 18.270368576049805
Agent0_Train_MaxReturn : 10.527976036071777
Agent0_Train_MinReturn : -65.96029663085938
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 255000
Agent0_TimeSinceStart : 918.6258294582367
Agent0_Critic_Loss : 0.3139653205871582
Agent0_Actor_Loss : -0.4434345066547394
Agent0_Alpha_Loss : 0.8334264755249023
Agent0_Temperature : 0.09524457024077504
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -20.426958084106445
Agent1_Eval_StdReturn : 24.86896514892578
Agent1_Eval_MaxReturn : 23.79389762878418
Agent1_Eval_MinReturn : -64.76481628417969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -25.222354888916016
Agent1_Train_StdReturn : 17.570716857910156
Agent1_Train_MaxReturn : 1.4673374891281128
Agent1_Train_MinReturn : -50.53426742553711
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 255000
Agent1_TimeSinceStart : 921.5581724643707
Agent1_Critic_Loss : 0.34434065222740173
Agent1_Actor_Loss : -0.6465983390808105
Agent1_Alpha_Loss : 0.8364390134811401
Agent1_Temperature : 0.09523780907725947
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -27.63323402404785
Agent0_Eval_StdReturn : 16.50409507751465
Agent0_Eval_MaxReturn : -11.951001167297363
Agent0_Eval_MinReturn : -65.99227905273438
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -17.131946563720703
Agent0_Train_StdReturn : 16.297752380371094
Agent0_Train_MaxReturn : 5.056848526000977
Agent0_Train_MinReturn : -48.97040939331055
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 256500
Agent0_TimeSinceStart : 924.4879577159882
Agent0_Critic_Loss : 0.2792432904243469
Agent0_Actor_Loss : -0.41598278284072876
Agent0_Alpha_Loss : 0.8136704564094543
Agent0_Temperature : 0.09521828106671212
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.64767837524414
Agent1_Eval_StdReturn : 11.483853340148926
Agent1_Eval_MaxReturn : -7.878091812133789
Agent1_Eval_MinReturn : -44.73047637939453
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -19.91208267211914
Agent1_Train_StdReturn : 18.11113166809082
Agent1_Train_MaxReturn : 4.138636589050293
Agent1_Train_MinReturn : -57.248268127441406
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 256500
Agent1_TimeSinceStart : 927.4291892051697
Agent1_Critic_Loss : 0.2798721194267273
Agent1_Actor_Loss : -0.602073073387146
Agent1_Alpha_Loss : 0.8441407084465027
Agent1_Temperature : 0.09521156814489272
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -18.420635223388672
Agent0_Eval_StdReturn : 20.749065399169922
Agent0_Eval_MaxReturn : 13.863554000854492
Agent0_Eval_MinReturn : -58.27471923828125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -20.614139556884766
Agent0_Train_StdReturn : 24.30925178527832
Agent0_Train_MaxReturn : 32.724037170410156
Agent0_Train_MinReturn : -50.4897575378418
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 258000
Agent0_TimeSinceStart : 930.365136384964
Agent0_Critic_Loss : 0.29961711168289185
Agent0_Actor_Loss : -0.4871995449066162
Agent0_Alpha_Loss : 0.8305785655975342
Agent0_Temperature : 0.09519201900391297
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -19.421566009521484
Agent1_Eval_StdReturn : 18.451419830322266
Agent1_Eval_MaxReturn : 11.952705383300781
Agent1_Eval_MinReturn : -51.041725158691406
Agent1_Critic_Loss : 1.7223505973815918
Agent1_Actor_Loss : -4.656205177307129
Agent1_Alpha_Loss : 4.729008197784424
Agent1_Temperature : 0.47437346750011966
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -34.585540771484375
Agent0_Eval_StdReturn : 33.32227325439453
Agent0_Eval_MaxReturn : 26.740196228027344
Agent0_Eval_MinReturn : -74.89736938476562
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -46.98847961425781
Agent0_Train_StdReturn : 32.155174255371094
Agent0_Train_MaxReturn : 4.267124176025391
Agent0_Train_MinReturn : -104.395751953125
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 267000
Agent0_TimeSinceStart : 950.8840551376343
Agent0_Critic_Loss : 1.2109732627868652
Agent0_Actor_Loss : -4.350327014923096
Agent0_Alpha_Loss : 4.673139572143555
Agent0_Temperature : 0.47424575793401114
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -55.3259391784668
Agent1_Eval_StdReturn : 31.233139038085938
Agent1_Eval_MaxReturn : 5.266162872314453
Agent1_Eval_MinReturn : -112.86882019042969
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -49.076515197753906
Agent1_Train_StdReturn : 33.75312805175781
Agent1_Train_MaxReturn : 31.933696746826172
Agent1_Train_MinReturn : -87.30506134033203
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 267000
Agent1_TimeSinceStart : 953.7952833175659
Agent1_Critic_Loss : 1.4207816123962402
Agent1_Actor_Loss : -4.576274871826172
Agent1_Alpha_Loss : 4.718355178833008
Agent1_Temperature : 0.47423487750806503
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -56.26715850830078
Agent0_Eval_StdReturn : 18.932430267333984
Agent0_Eval_MaxReturn : -22.918846130371094
Agent0_Eval_MinReturn : -85.30237579345703
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -40.88767623901367
Agent0_Train_StdReturn : 26.141469955444336
Agent0_Train_MaxReturn : 8.21768856048584
Agent0_Train_MinReturn : -76.00309753417969
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 268500
Agent0_TimeSinceStart : 956.7021586894989
Agent0_Critic_Loss : 1.4517621994018555
Agent0_Actor_Loss : -4.525697708129883
Agent0_Alpha_Loss : 4.715540409088135
Agent0_Temperature : 0.47410763224846253
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -49.56001663208008
Agent1_Eval_StdReturn : 26.506620407104492
Agent1_Eval_MaxReturn : -5.518434524536133
Agent1_Eval_MinReturn : -93.84539794921875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.121253967285156
Agent1_Train_StdReturn : 26.81146812438965
Agent1_Train_MaxReturn : 0.5798931121826172
Agent1_Train_MinReturn : -76.83859252929688
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 268500
Agent1_TimeSinceStart : 959.609213590622
Agent1_Critic_Loss : 2.0166571140289307
Agent1_Actor_Loss : -4.506407737731934
Agent1_Alpha_Loss : 4.696440696716309
Agent1_Temperature : 0.474096417597843
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -46.47489547729492
Agent0_Eval_StdReturn : 32.56034469604492
Agent0_Eval_MaxReturn : 1.4417858123779297
Agent0_Eval_MinReturn : -95.257080078125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -53.31642532348633
Agent0_Train_StdReturn : 14.880593299865723
Agent0_Train_MaxReturn : -33.4295539855957
Agent0_Train_MinReturn : -82.8409194946289
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 270000
Agent0_TimeSinceStart : 962.5301420688629
Agent0_Critic_Loss : 1.3265535831451416
Agent0_Actor_Loss : -4.516325950622559
Agent0_Alpha_Loss : 4.693714141845703
Agent0_Temperature : 0.4739695930723301
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -34.86870574951172
Agent1_Eval_StdReturn : 30.0416316986084
Agent1_Eval_MaxReturn : 13.9688138961792
Agent1_Eval_MinReturn : -88.7759017944336
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -50.652320861816406
Agent1_Train_StdReturn : 34.55990982055664
Agent1_Train_MaxReturn : 2.617992877960205
Agent1_Train_MinReturn : -109.01663970947266
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 270000
Agent1_TimeSinceStart : 965.4474639892578
Agent1_Critic_Loss : 1.510441541671753
Agent1_Actor_Loss : -4.423745155334473
Agent1_Alpha_Loss : 4.706552982330322
Agent1_Temperature : 0.47395804833003435
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -31.09609031677246
Agent0_Eval_StdReturn : 21.836332321166992
Agent0_Eval_MaxReturn : -1.274282693862915
Agent0_Eval_MinReturn : -76.69889068603516
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -30.371410369873047
Agent0_Train_StdReturn : 27.62382698059082
Agent0_Train_MaxReturn : 11.446476936340332
Agent0_Train_MinReturn : -78.12699127197266
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 271500
Agent0_TimeSinceStart : 968.3579421043396
Agent0_Critic_Loss : 1.2006042003631592
Agent0_Actor_Loss : -4.34617805480957
Agent0_Alpha_Loss : 4.681051731109619
Agent0_Temperature : 0.47383166818537314
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -52.18950271606445
Agent1_Eval_StdReturn : 38.55329895019531
Agent1_Eval_MaxReturn : -2.5617284774780273
Agent1_Eval_MinReturn : -116.97064971923828
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -45.91557693481445
Agent1_Train_StdReturn : 33.18534469604492
Agent1_Train_MaxReturn : 4.098134994506836
Agent1_Train_MinReturn : -99.89788055419922
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 271500
Agent1_TimeSinceStart : 971.2692110538483
Agent1_Critic_Loss : 1.3798824548721313
Agent1_Actor_Loss : -4.319576263427734
Agent1_Alpha_Loss : 4.6967597007751465
Agent1_Temperature : 0.47381978930104146
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -35.14276885986328
Agent0_Eval_StdReturn : 33.02573013305664
Agent0_Eval_MaxReturn : 2.218256950378418
Agent0_Eval_MinReturn : -108.48716735839844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -48.6402473449707
Agent0_Train_StdReturn : 20.629955291748047
Agent0_Train_MaxReturn : -1.9847450256347656
Agent0_Train_MinReturn : -80.23765563964844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 273000
Agent0_TimeSinceStart : 974.1924986839294
Agent0_Critic_Loss : 1.2527072429656982
Agent0_Actor_Loss : -4.308600425720215
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -14.527132034301758
Agent1_Train_StdReturn : 14.679280281066895
Agent1_Train_MaxReturn : 14.70614242553711
Agent1_Train_MinReturn : -32.55760955810547
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 258000
Agent1_TimeSinceStart : 933.2980353832245
Agent1_Critic_Loss : 0.3246362805366516
Agent1_Actor_Loss : -0.5133891105651855
Agent1_Alpha_Loss : 0.8500247001647949
Agent1_Temperature : 0.09518529666798983
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -31.546390533447266
Agent0_Eval_StdReturn : 25.886621475219727
Agent0_Eval_MaxReturn : 15.908835411071777
Agent0_Eval_MinReturn : -73.07006072998047
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -25.732135772705078
Agent0_Train_StdReturn : 16.698392868041992
Agent0_Train_MaxReturn : -3.48134708404541
Agent0_Train_MinReturn : -63.246826171875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 259500
Agent0_TimeSinceStart : 936.2280218601227
Agent0_Critic_Loss : 0.3713064193725586
Agent0_Actor_Loss : -0.47489380836486816
Agent0_Alpha_Loss : 0.8068335652351379
Agent0_Temperature : 0.09516585114129393
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.99559211730957
Agent1_Eval_StdReturn : 27.145214080810547
Agent1_Eval_MaxReturn : 26.40770721435547
Agent1_Eval_MinReturn : -78.02961730957031
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -26.15829086303711
Agent1_Train_StdReturn : 15.311239242553711
Agent1_Train_MaxReturn : -7.630087852478027
Agent1_Train_MinReturn : -46.3226432800293
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 259500
Agent1_TimeSinceStart : 939.1458547115326
Agent1_Critic_Loss : 0.2505742311477661
Agent1_Actor_Loss : -0.48523351550102234
Agent1_Alpha_Loss : 0.8330438733100891
Agent1_Temperature : 0.09515904753211063
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -23.007314682006836
Agent0_Eval_StdReturn : 18.890491485595703
Agent0_Eval_MaxReturn : 8.92021369934082
Agent0_Eval_MinReturn : -59.886802673339844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -23.071426391601562
Agent0_Train_StdReturn : 18.498306274414062
Agent0_Train_MaxReturn : 12.258899688720703
Agent0_Train_MinReturn : -54.854549407958984
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 261000
Agent0_TimeSinceStart : 942.071727514267
Agent0_Critic_Loss : 0.3119255304336548
Agent0_Actor_Loss : -0.40386080741882324
Agent0_Alpha_Loss : 0.8317736387252808
Agent0_Temperature : 0.09513969322879047
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.79995346069336
Agent1_Eval_StdReturn : 15.850346565246582
Agent1_Eval_MaxReturn : 10.231342315673828
Agent1_Eval_MinReturn : -43.448280334472656
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -18.343215942382812
Agent1_Train_StdReturn : 27.012271881103516
Agent1_Train_MaxReturn : 13.452394485473633
Agent1_Train_MinReturn : -76.37648010253906
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 261000
Agent1_TimeSinceStart : 945.0088798999786
Agent1_Critic_Loss : 0.27382373809814453
Agent1_Actor_Loss : -0.4530453383922577
Agent1_Alpha_Loss : 0.832138180732727
Agent1_Temperature : 0.09513282063632762
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -22.74420928955078
Agent0_Eval_StdReturn : 15.509589195251465
Agent0_Eval_MaxReturn : 2.216794490814209
Agent0_Eval_MinReturn : -55.1341438293457
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.45502281188965
Agent0_Train_StdReturn : 14.236262321472168
Agent0_Train_MaxReturn : 3.9647254943847656
Agent0_Train_MinReturn : -43.958824157714844
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 262500
Agent0_TimeSinceStart : 947.9440424442291
Agent0_Critic_Loss : 0.3242836594581604
Agent0_Actor_Loss : -0.4064755141735077
Agent0_Alpha_Loss : 0.7890176177024841
Agent0_Temperature : 0.09511367070151196
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -25.31411361694336
Agent1_Eval_StdReturn : 21.058942794799805
Agent1_Eval_MaxReturn : 6.96299934387207
Agent1_Eval_MinReturn : -56.7198486328125
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -22.332759857177734
Agent1_Train_StdReturn : 16.64152717590332
Agent1_Train_MaxReturn : 15.524153709411621
Agent1_Train_MinReturn : -45.01968765258789
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 262500
Agent1_TimeSinceStart : 950.8781805038452
Agent1_Critic_Loss : 0.2713109850883484
Agent1_Actor_Loss : -0.48947739601135254
Agent1_Alpha_Loss : 0.8081884980201721
Agent1_Temperature : 0.09510668412290531
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -15.875284194946289
Agent0_Eval_StdReturn : 15.170669555664062
Agent0_Eval_MaxReturn : 6.3105082511901855
Agent0_Eval_MinReturn : -36.1923828125
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.486331939697266
Agent0_Train_StdReturn : 11.420889854431152
Agent0_Train_MaxReturn : 6.600667953491211
Agent0_Train_MinReturn : -33.32594680786133
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 264000
Agent0_TimeSinceStart : 953.8187057971954
Agent0_Critic_Loss : 0.3108844757080078
Agent0_Actor_Loss : -0.4120849668979645
Agent0_Alpha_Loss : 0.8128831386566162
Agent0_Temperature : 0.095087697995168
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -24.347515106201172
Agent1_Eval_StdReturn : 18.082679748535156
Agent1_Eval_MaxReturn : 4.598541259765625
Agent1_Eval_MinReturn : -60.456886291503906
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -29.009380340576172
Agent1_Train_StdReturn : 23.340303421020508
Agent1_Train_MaxReturn : 18.618778228759766
Agent1_Train_MinReturn : -61.63795471191406
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 264000
Agent1_TimeSinceStart : 956.7540078163147
Agent1_Critic_Loss : 0.26411688327789307
Agent1_Actor_Loss : -0.5473835468292236
Agent1_Alpha_Loss : 0.8005508780479431
Agent1_Temperature : 0.09508065077268905
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -15.872865676879883
Agent0_Eval_StdReturn : 7.451403617858887
Agent0_Eval_MaxReturn : 0.3148345947265625
Agent0_Eval_MinReturn : -24.742368698120117
Agent0_Eval_AverageEpLen : 150.0
Agent0_Alpha_Loss : 4.670141220092773
Agent0_Temperature : 0.4736938775912969
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -61.70001983642578
Agent1_Eval_StdReturn : 31.0026798248291
Agent1_Eval_MaxReturn : -14.331443786621094
Agent1_Eval_MinReturn : -112.2208023071289
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.95488929748535
Agent1_Train_StdReturn : 33.71857833862305
Agent1_Train_MaxReturn : 14.38931655883789
Agent1_Train_MinReturn : -114.77364349365234
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 273000
Agent1_TimeSinceStart : 977.1068387031555
Agent1_Critic_Loss : 1.6072384119033813
Agent1_Actor_Loss : -4.267202854156494
Agent1_Alpha_Loss : 4.70020866394043
Agent1_Temperature : 0.47368162148602266
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -31.166988372802734
Agent0_Eval_StdReturn : 22.096492767333984
Agent0_Eval_MaxReturn : 7.771580219268799
Agent0_Eval_MinReturn : -70.64126586914062
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -25.477331161499023
Agent0_Train_StdReturn : 25.086076736450195
Agent0_Train_MaxReturn : 17.425180435180664
Agent0_Train_MinReturn : -71.7782974243164
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 274500
Agent0_TimeSinceStart : 980.0265429019928
Agent0_Critic_Loss : 1.403996467590332
Agent0_Actor_Loss : -4.2412800788879395
Agent0_Alpha_Loss : 4.689357280731201
Agent0_Temperature : 0.47355615593750044
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -35.25817108154297
Agent1_Eval_StdReturn : 32.66249465942383
Agent1_Eval_MaxReturn : 20.694927215576172
Agent1_Eval_MinReturn : -93.990234375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -32.45130157470703
Agent1_Train_StdReturn : 28.711341857910156
Agent1_Train_MaxReturn : 12.051252365112305
Agent1_Train_MinReturn : -97.5595703125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 274500
Agent1_TimeSinceStart : 982.9345421791077
Agent1_Critic_Loss : 1.3547567129135132
Agent1_Actor_Loss : -4.387267112731934
Agent1_Alpha_Loss : 4.714983940124512
Agent1_Temperature : 0.4735434965878376
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -35.704715728759766
Agent0_Eval_StdReturn : 29.00259017944336
Agent0_Eval_MaxReturn : 5.462742805480957
Agent0_Eval_MinReturn : -86.16448974609375
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -26.216876983642578
Agent0_Train_StdReturn : 22.44585418701172
Agent0_Train_MaxReturn : 16.548717498779297
Agent0_Train_MinReturn : -74.65679931640625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 276000
Agent0_TimeSinceStart : 985.877345085144
Agent0_Critic_Loss : 1.3605046272277832
Agent0_Actor_Loss : -4.315357208251953
Agent0_Alpha_Loss : 4.634275913238525
Agent0_Temperature : 0.4734186500401131
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -25.122907638549805
Agent1_Eval_StdReturn : 26.13454246520996
Agent1_Eval_MaxReturn : 22.737825393676758
Agent1_Eval_MinReturn : -69.50091552734375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -20.959985733032227
Agent1_Train_StdReturn : 27.88648796081543
Agent1_Train_MaxReturn : 24.222259521484375
Agent1_Train_MinReturn : -70.58827209472656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 276000
Agent1_TimeSinceStart : 988.7919545173645
Agent1_Critic_Loss : 1.6471039056777954
Agent1_Actor_Loss : -4.499267578125
Agent1_Alpha_Loss : 4.698063850402832
Agent1_Temperature : 0.47340545886943314
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -45.15050506591797
Agent0_Eval_StdReturn : 33.02553939819336
Agent0_Eval_MaxReturn : 10.336074829101562
Agent0_Eval_MinReturn : -117.08833312988281
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -41.42430877685547
Agent0_Train_StdReturn : 31.912471771240234
Agent0_Train_MaxReturn : 28.77132225036621
Agent0_Train_MinReturn : -99.12741088867188
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 277500
Agent0_TimeSinceStart : 991.7040011882782
Agent0_Critic_Loss : 1.3405910730361938
Agent0_Actor_Loss : -4.403781890869141
Agent0_Alpha_Loss : 4.704803466796875
Agent0_Temperature : 0.4732811440389418
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -40.58795166015625
Agent1_Eval_StdReturn : 29.756214141845703
Agent1_Eval_MaxReturn : -4.9314141273498535
Agent1_Eval_MinReturn : -83.92527770996094
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -49.41520309448242
Agent1_Train_StdReturn : 32.53319549560547
Agent1_Train_MaxReturn : 9.832687377929688
Agent1_Train_MinReturn : -93.16567993164062
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 277500
Agent1_TimeSinceStart : 994.6203663349152
Agent1_Critic_Loss : 1.3676469326019287
Agent1_Actor_Loss : -4.620002269744873
Agent1_Alpha_Loss : 4.701804161071777
Agent1_Temperature : 0.47326749089561165
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -51.141380310058594
Agent0_Eval_StdReturn : 37.13581466674805
Agent0_Eval_MaxReturn : 6.544113636016846
Agent0_Eval_MinReturn : -104.21734619140625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.010608673095703
Agent0_Train_StdReturn : 27.919090270996094
Agent0_Train_MaxReturn : 12.349184036254883
Agent0_Train_MinReturn : -62.20604705810547
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 279000
Agent0_TimeSinceStart : 997.5337429046631
Agent0_Critic_Loss : 1.3099480867385864
Agent0_Actor_Loss : -4.469059944152832
Agent0_Alpha_Loss : 4.677316665649414
Agent0_Temperature : 0.4731437156986635
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -48.612762451171875
Agent1_Eval_StdReturn : 33.95177459716797
Agent1_Eval_MaxReturn : 14.979804039001465
Agent1_Eval_MinReturn : -105.03707885742188
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -20.41937255859375
Agent1_Train_StdReturn : 30.86370849609375
Agent1_Train_MaxReturn : 20.552961349487305
Agent1_Train_MinReturn : -82.1170425415039
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 279000
Agent1_TimeSinceStart : 1000.4444944858551
Agent1_Critic_Loss : 1.2403939962387085
Agent1_Actor_Loss : -4.716239929199219
Agent1_Alpha_Loss : 4.704358100891113
Agent1_Temperature : 0.4731295803320088
Agent0_Train_AverageReturn : -19.355375289916992
Agent0_Train_StdReturn : 15.61993408203125
Agent0_Train_MaxReturn : 1.183847427368164
Agent0_Train_MinReturn : -46.853519439697266
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 265500
Agent0_TimeSinceStart : 959.6868517398834
Agent0_Critic_Loss : 0.3116663098335266
Agent0_Actor_Loss : -0.39790186285972595
Agent0_Alpha_Loss : 0.7890048623085022
Agent0_Temperature : 0.09506184044361785
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -19.088787078857422
Agent1_Eval_StdReturn : 15.598867416381836
Agent1_Eval_MaxReturn : -1.6689040660858154
Agent1_Eval_MinReturn : -48.263084411621094
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -15.451428413391113
Agent1_Train_StdReturn : 14.034442901611328
Agent1_Train_MaxReturn : 1.0093474388122559
Agent1_Train_MinReturn : -43.12343978881836
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 265500
Agent1_TimeSinceStart : 962.6256003379822
Agent1_Critic_Loss : 0.2558530569076538
Agent1_Actor_Loss : -0.6240909099578857
Agent1_Alpha_Loss : 0.8009318113327026
Agent1_Temperature : 0.09505470821926514
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -18.04636001586914
Agent0_Eval_StdReturn : 15.229569435119629
Agent0_Eval_MaxReturn : 7.61195707321167
Agent0_Eval_MinReturn : -50.755455017089844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -16.817554473876953
Agent0_Train_StdReturn : 16.142311096191406
Agent0_Train_MaxReturn : 17.49298858642578
Agent0_Train_MinReturn : -33.40279006958008
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 267000
Agent0_TimeSinceStart : 965.582866191864
Agent0_Critic_Loss : 0.27697867155075073
Agent0_Actor_Loss : -0.4013119637966156
Agent0_Alpha_Loss : 0.7978295087814331
Agent0_Temperature : 0.09503605920615396
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.46363830566406
Agent1_Eval_StdReturn : 19.135175704956055
Agent1_Eval_MaxReturn : -9.633061408996582
Agent1_Eval_MinReturn : -67.78974914550781
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -21.208049774169922
Agent1_Train_StdReturn : 11.49929428100586
Agent1_Train_MaxReturn : 2.4374303817749023
Agent1_Train_MinReturn : -38.02384948730469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 267000
Agent1_TimeSinceStart : 968.5207829475403
Agent1_Critic_Loss : 0.29663586616516113
Agent1_Actor_Loss : -0.5464695692062378
Agent1_Alpha_Loss : 0.8296974301338196
Agent1_Temperature : 0.09502876106494383
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -6.6046552658081055
Agent0_Eval_StdReturn : 19.3654842376709
Agent0_Eval_MaxReturn : 16.808216094970703
Agent0_Eval_MinReturn : -45.01776123046875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -19.231979370117188
Agent0_Train_StdReturn : 16.262142181396484
Agent0_Train_MaxReturn : 7.083060264587402
Agent0_Train_MinReturn : -48.05767822265625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 268500
Agent0_TimeSinceStart : 971.4678616523743
Agent0_Critic_Loss : 0.2844564616680145
Agent0_Actor_Loss : -0.47429290413856506
Agent0_Alpha_Loss : 0.812984824180603
Agent0_Temperature : 0.09501030056656705
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.578540802001953
Agent1_Eval_StdReturn : 23.46275520324707
Agent1_Eval_MaxReturn : 22.95786476135254
Agent1_Eval_MinReturn : -70.83007049560547
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -22.47952651977539
Agent1_Train_StdReturn : 17.121660232543945
Agent1_Train_MaxReturn : 3.559023857116699
Agent1_Train_MinReturn : -54.71444320678711
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 268500
Agent1_TimeSinceStart : 974.4010999202728
Agent1_Critic_Loss : 0.3293192386627197
Agent1_Actor_Loss : -0.5574888586997986
Agent1_Alpha_Loss : 0.8128446340560913
Agent1_Temperature : 0.0950028592786114
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -28.78731346130371
Agent0_Eval_StdReturn : 17.786155700683594
Agent0_Eval_MaxReturn : -0.3861980438232422
Agent0_Eval_MinReturn : -63.96485900878906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.850133895874023
Agent0_Train_StdReturn : 17.05860137939453
Agent0_Train_MaxReturn : 0.7608222961425781
Agent0_Train_MinReturn : -57.160125732421875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 270000
Agent0_TimeSinceStart : 977.3447501659393
Agent0_Critic_Loss : 0.3163207769393921
Agent0_Actor_Loss : -0.40730130672454834
Agent0_Alpha_Loss : 0.8032429218292236
Agent0_Temperature : 0.09498459059061237
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -15.871670722961426
Agent1_Eval_StdReturn : 10.655865669250488
Agent1_Eval_MaxReturn : 3.86873722076416
Agent1_Eval_MinReturn : -29.939149856567383
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -15.698469161987305
Agent1_Train_StdReturn : 14.85641098022461
Agent1_Train_MaxReturn : 13.113656044006348
Agent1_Train_MinReturn : -36.971866607666016
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 270000
Agent1_TimeSinceStart : 980.2842028141022
Agent1_Critic_Loss : 0.27975302934646606
Agent1_Actor_Loss : -0.5106884241104126
Agent1_Alpha_Loss : 0.8096815347671509
Agent1_Temperature : 0.09497700698834753
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -23.2138614654541
Agent0_Eval_StdReturn : 11.191688537597656
Agent0_Eval_MaxReturn : -1.9666309356689453
Agent0_Eval_MinReturn : -41.19568634033203
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.06344223022461
Agent0_Train_StdReturn : 18.083349227905273
Agent0_Train_MaxReturn : -2.8064002990722656
Agent0_Train_MinReturn : -65.9148178100586
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 271500
Agent0_TimeSinceStart : 983.2230980396271
Agent0_Critic_Loss : 0.29498785734176636
Agent0_Actor_Loss : -0.4008597135543823
Agent0_Alpha_Loss : 0.7884222865104675
Agent0_Temperature : 0.09495896784403773
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -14.62488079071045
Agent1_Eval_StdReturn : 16.242088317871094
Agent1_Eval_MaxReturn : 9.385151863098145
Agent1_Eval_MinReturn : -42.12882995605469
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -24.849987030029297
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -26.340457916259766
Agent0_Eval_StdReturn : 31.814348220825195
Agent0_Eval_MaxReturn : 27.753585815429688
Agent0_Eval_MinReturn : -69.31025695800781
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -38.98701095581055
Agent0_Train_StdReturn : 31.404863357543945
Agent0_Train_MaxReturn : 5.614513397216797
Agent0_Train_MinReturn : -102.32222747802734
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 280500
Agent0_TimeSinceStart : 1003.3567068576813
Agent0_Critic_Loss : 1.184671401977539
Agent0_Actor_Loss : -4.504343509674072
Agent0_Alpha_Loss : 4.64990234375
Agent0_Temperature : 0.4730064344405258
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -38.28403854370117
Agent1_Eval_StdReturn : 24.963623046875
Agent1_Eval_MaxReturn : 18.03904151916504
Agent1_Eval_MinReturn : -72.27118682861328
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -37.60683822631836
Agent1_Train_StdReturn : 21.002613067626953
Agent1_Train_MaxReturn : -2.445469856262207
Agent1_Train_MinReturn : -63.88179397583008
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 280500
Agent1_TimeSinceStart : 1006.264758348465
Agent1_Critic_Loss : 1.4268056154251099
Agent1_Actor_Loss : -4.778830528259277
Agent1_Alpha_Loss : 4.699251174926758
Agent1_Temperature : 0.4729917372956268
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -35.78020095825195
Agent0_Eval_StdReturn : 31.68980598449707
Agent0_Eval_MaxReturn : 10.991920471191406
Agent0_Eval_MinReturn : -101.68759155273438
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -33.273658752441406
Agent0_Train_StdReturn : 32.81878662109375
Agent0_Train_MaxReturn : 28.65203857421875
Agent0_Train_MinReturn : -75.90420532226562
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 282000
Agent0_TimeSinceStart : 1009.1707108020782
Agent0_Critic_Loss : 1.3695491552352905
Agent0_Actor_Loss : -4.568323135375977
Agent0_Alpha_Loss : 4.701929569244385
Agent0_Temperature : 0.4728691426576251
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -31.184295654296875
Agent1_Eval_StdReturn : 22.583505630493164
Agent1_Eval_MaxReturn : 7.988114356994629
Agent1_Eval_MinReturn : -77.89773559570312
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -29.921606063842773
Agent1_Train_StdReturn : 22.968957901000977
Agent1_Train_MaxReturn : -6.2714033126831055
Agent1_Train_MinReturn : -73.12052917480469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 282000
Agent1_TimeSinceStart : 1012.0842909812927
Agent1_Critic_Loss : 1.548995018005371
Agent1_Actor_Loss : -4.768579483032227
Agent1_Alpha_Loss : 4.710203170776367
Agent1_Temperature : 0.472853926485049
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -31.759754180908203
Agent0_Eval_StdReturn : 18.003402709960938
Agent0_Eval_MaxReturn : 1.2913919687271118
Agent0_Eval_MinReturn : -62.25370407104492
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -40.88561248779297
Agent0_Train_StdReturn : 18.728639602661133
Agent0_Train_MaxReturn : -17.236289978027344
Agent0_Train_MinReturn : -81.23455810546875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 283500
Agent0_TimeSinceStart : 1014.9934995174408
Agent0_Critic_Loss : 1.1049721240997314
Agent0_Actor_Loss : -4.500918865203857
Agent0_Alpha_Loss : 4.674487113952637
Agent0_Temperature : 0.47273191911083645
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -53.33304977416992
Agent1_Eval_StdReturn : 36.470523834228516
Agent1_Eval_MaxReturn : -4.521207809448242
Agent1_Eval_MinReturn : -115.41990661621094
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -28.392486572265625
Agent1_Train_StdReturn : 30.242286682128906
Agent1_Train_MaxReturn : 13.163829803466797
Agent1_Train_MinReturn : -82.16175842285156
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 283500
Agent1_TimeSinceStart : 1017.9078872203827
Agent1_Critic_Loss : 1.7243196964263916
Agent1_Actor_Loss : -4.621291160583496
Agent1_Alpha_Loss : 4.670924186706543
Agent1_Temperature : 0.472716255095448
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -22.952882766723633
Agent0_Eval_StdReturn : 23.499305725097656
Agent0_Eval_MaxReturn : 25.154666900634766
Agent0_Eval_MinReturn : -61.02867126464844
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -29.95259666442871
Agent0_Train_StdReturn : 29.978118896484375
Agent0_Train_MaxReturn : 26.87550163269043
Agent0_Train_MinReturn : -81.45460510253906
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 285000
Agent0_TimeSinceStart : 1020.8269503116608
Agent0_Critic_Loss : 1.390122890472412
Agent0_Actor_Loss : -4.456315040588379
Agent0_Alpha_Loss : 4.642953872680664
Agent0_Temperature : 0.47259484564518867
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -45.35569763183594
Agent1_Eval_StdReturn : 20.707149505615234
Agent1_Eval_MaxReturn : -14.986858367919922
Agent1_Eval_MinReturn : -84.14334106445312
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -37.45504379272461
Agent1_Train_StdReturn : 32.52722930908203
Agent1_Train_MaxReturn : 10.336315155029297
Agent1_Train_MinReturn : -92.7696533203125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 285000
Agent1_TimeSinceStart : 1023.7373716831207
Agent1_Critic_Loss : 1.4376081228256226
Agent1_Actor_Loss : -4.624111175537109
Agent1_Alpha_Loss : 4.708371162414551
Agent1_Temperature : 0.4725786070026311
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -20.945627212524414
Agent0_Eval_StdReturn : 43.148902893066406
Agent0_Eval_MaxReturn : 33.81968307495117
Agent0_Eval_MinReturn : -110.25919342041016
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -35.29914093017578
Agent0_Train_StdReturn : 26.636455535888672
Agent0_Train_MaxReturn : -8.373769760131836
Agent0_Train_MinReturn : -103.00106811523438
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 286500
Agent0_TimeSinceStart : 1026.6528580188751
Agent0_Critic_Loss : 1.2106389999389648
Agent0_Actor_Loss : -4.382104873657227
Agent0_Alpha_Loss : 4.653722286224365
Agent0_Temperature : 0.47245787840073544
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Agent1_Train_StdReturn : 19.311412811279297
Agent1_Train_MaxReturn : 10.814435958862305
Agent1_Train_MinReturn : -56.121337890625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 271500
Agent1_TimeSinceStart : 986.1530513763428
Agent1_Critic_Loss : 0.2763991951942444
Agent1_Actor_Loss : -0.45388635993003845
Agent1_Alpha_Loss : 0.8116782307624817
Agent1_Temperature : 0.09495119253502246
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -19.827194213867188
Agent0_Eval_StdReturn : 15.023053169250488
Agent0_Eval_MaxReturn : 2.2478959560394287
Agent0_Eval_MinReturn : -56.184326171875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -32.21392059326172
Agent0_Train_StdReturn : 17.660348892211914
Agent0_Train_MaxReturn : 1.3599424362182617
Agent0_Train_MinReturn : -66.92279052734375
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 273000
Agent0_TimeSinceStart : 989.0861287117004
Agent0_Critic_Loss : 0.2756347954273224
Agent0_Actor_Loss : -0.3476610481739044
Agent0_Alpha_Loss : 0.7888436913490295
Agent0_Temperature : 0.0949334213597314
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -20.627544403076172
Agent1_Eval_StdReturn : 11.924692153930664
Agent1_Eval_MaxReturn : -0.3945426940917969
Agent1_Eval_MinReturn : -38.863216400146484
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -25.819751739501953
Agent1_Train_StdReturn : 22.785337448120117
Agent1_Train_MaxReturn : 20.649871826171875
Agent1_Train_MinReturn : -62.144893646240234
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 273000
Agent1_TimeSinceStart : 992.0123672485352
Agent1_Critic_Loss : 0.23017975687980652
Agent1_Actor_Loss : -0.47669294476509094
Agent1_Alpha_Loss : 0.8166427612304688
Agent1_Temperature : 0.09492539662442538
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -31.63369369506836
Agent0_Eval_StdReturn : 17.332609176635742
Agent0_Eval_MaxReturn : -14.832752227783203
Agent0_Eval_MinReturn : -65.2871322631836
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -26.05121421813965
Agent0_Train_StdReturn : 11.700675010681152
Agent0_Train_MaxReturn : -3.1384027004241943
Agent0_Train_MinReturn : -47.5763053894043
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 274500
Agent0_TimeSinceStart : 994.9471461772919
Agent0_Critic_Loss : 0.34796398878097534
Agent0_Actor_Loss : -0.3308959901332855
Agent0_Alpha_Loss : 0.8059628009796143
Agent0_Temperature : 0.0949078913397628
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -25.18560791015625
Agent1_Eval_StdReturn : 23.440513610839844
Agent1_Eval_MaxReturn : 7.552042007446289
Agent1_Eval_MinReturn : -57.960594177246094
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -34.80240249633789
Agent1_Train_StdReturn : 27.869792938232422
Agent1_Train_MaxReturn : 1.5737266540527344
Agent1_Train_MinReturn : -87.94709777832031
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 274500
Agent1_TimeSinceStart : 997.8746821880341
Agent1_Critic_Loss : 0.2730794847011566
Agent1_Actor_Loss : -0.5785331130027771
Agent1_Alpha_Loss : 0.8347259759902954
Agent1_Temperature : 0.09489956293153488
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -23.920747756958008
Agent0_Eval_StdReturn : 15.623903274536133
Agent0_Eval_MaxReturn : 0.7930617332458496
Agent0_Eval_MinReturn : -46.94575500488281
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -19.7750244140625
Agent0_Train_StdReturn : 22.278078079223633
Agent0_Train_MaxReturn : 16.4288387298584
Agent0_Train_MinReturn : -57.912837982177734
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 276000
Agent0_TimeSinceStart : 1000.8123421669006
Agent0_Critic_Loss : 0.2972133755683899
Agent0_Actor_Loss : -0.34939414262771606
Agent0_Alpha_Loss : 0.7786540985107422
Agent0_Temperature : 0.09488245713144354
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -18.88848114013672
Agent1_Eval_StdReturn : 12.452346801757812
Agent1_Eval_MaxReturn : 2.2227330207824707
Agent1_Eval_MinReturn : -44.12382888793945
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -9.955697059631348
Agent1_Train_StdReturn : 15.40174388885498
Agent1_Train_MaxReturn : 21.454730987548828
Agent1_Train_MinReturn : -37.38099670410156
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 276000
Agent1_TimeSinceStart : 1003.7538537979126
Agent1_Critic_Loss : 0.27365046739578247
Agent1_Actor_Loss : -0.6200511455535889
Agent1_Alpha_Loss : 0.799369215965271
Agent1_Temperature : 0.0948738001500235
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -21.470775604248047
Agent0_Eval_StdReturn : 25.66312026977539
Agent0_Eval_MaxReturn : 26.40711212158203
Agent0_Eval_MinReturn : -77.9916000366211
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -19.808300018310547
Agent0_Train_StdReturn : 16.96094512939453
Agent0_Train_MaxReturn : 3.872406005859375
Agent0_Train_MinReturn : -60.90864562988281
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 277500
Agent0_TimeSinceStart : 1006.6922791004181
Agent0_Critic_Loss : 0.2552070617675781
Agent0_Actor_Loss : -0.33248573541641235
Agent0_Alpha_Loss : 0.8118627071380615
Agent0_Temperature : 0.09485700861697674
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -15.71552848815918
Agent1_Eval_StdReturn : 4.293295383453369
Agent1_Eval_MaxReturn : -9.421298027038574
Agent1_Eval_MinReturn : -23.358741760253906
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -19.28207015991211
Agent1_Train_StdReturn : 13.969122886657715
Agent1_Train_MaxReturn : 6.030696868896484
Agent1_Train_MinReturn : -40.00217819213867
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 277500
Agent1_TimeSinceStart : 1009.6286480426788
Agent1_Critic_Loss : 0.3076627552509308
Agent1_Actor_Loss : -0.5855553150177002
Agent1_Alpha_Loss : 0.8006184101104736
Agent1_Temperature : 0.09484809661886434
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -26.8756103515625
Agent0_Eval_StdReturn : 16.42934226989746
Agent0_Eval_MaxReturn : 11.165142059326172
Agent0_Eval_MinReturn : -44.01457595825195
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -13.120306015014648
Agent0_Train_StdReturn : 24.886383056640625
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.160079956054688
Agent1_Eval_StdReturn : 32.27444839477539
Agent1_Eval_MaxReturn : 30.168054580688477
Agent1_Eval_MinReturn : -96.92045593261719
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -62.3390998840332
Agent1_Train_StdReturn : 27.36329460144043
Agent1_Train_MaxReturn : -14.057774543762207
Agent1_Train_MinReturn : -102.6506118774414
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 286500
Agent1_TimeSinceStart : 1029.5714583396912
Agent1_Critic_Loss : 1.663762092590332
Agent1_Actor_Loss : -4.60156774520874
Agent1_Alpha_Loss : 4.712002277374268
Agent1_Temperature : 0.47244097175495287
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -37.34077072143555
Agent0_Eval_StdReturn : 40.168701171875
Agent0_Eval_MaxReturn : 9.30915641784668
Agent0_Eval_MinReturn : -117.85515594482422
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -44.15137481689453
Agent0_Train_StdReturn : 29.38926124572754
Agent0_Train_MaxReturn : 5.379613876342773
Agent0_Train_MinReturn : -73.96705627441406
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 288000
Agent0_TimeSinceStart : 1032.49360704422
Agent0_Critic_Loss : 1.2785598039627075
Agent0_Actor_Loss : -4.389951705932617
Agent0_Alpha_Loss : 4.662783622741699
Agent0_Temperature : 0.47232098281926505
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -23.89682388305664
Agent1_Eval_StdReturn : 25.553495407104492
Agent1_Eval_MaxReturn : 16.65373420715332
Agent1_Eval_MinReturn : -80.42575073242188
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -34.13976287841797
Agent1_Train_StdReturn : 18.905656814575195
Agent1_Train_MaxReturn : -5.435529708862305
Agent1_Train_MinReturn : -66.96731567382812
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 288000
Agent1_TimeSinceStart : 1035.4178836345673
Agent1_Critic_Loss : 1.267391562461853
Agent1_Actor_Loss : -4.49128532409668
Agent1_Alpha_Loss : 4.656023979187012
Agent1_Temperature : 0.47230350480903555
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -21.483295440673828
Agent0_Eval_StdReturn : 19.69146156311035
Agent0_Eval_MaxReturn : 3.555386543273926
Agent0_Eval_MinReturn : -53.40252685546875
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.987489700317383
Agent0_Train_StdReturn : 22.80726432800293
Agent0_Train_MaxReturn : 15.902692794799805
Agent0_Train_MinReturn : -64.4092788696289
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 289500
Agent0_TimeSinceStart : 1038.336259841919
Agent0_Critic_Loss : 1.3041572570800781
Agent0_Actor_Loss : -4.304954528808594
Agent0_Alpha_Loss : 4.650699615478516
Agent0_Temperature : 0.4721841865063276
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.39682388305664
Agent1_Eval_StdReturn : 35.75457000732422
Agent1_Eval_MaxReturn : 30.869115829467773
Agent1_Eval_MinReturn : -90.5570297241211
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -41.77257537841797
Agent1_Train_StdReturn : 22.738676071166992
Agent1_Train_MaxReturn : -8.315604209899902
Agent1_Train_MinReturn : -87.64752197265625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 289500
Agent1_TimeSinceStart : 1041.256234407425
Agent1_Critic_Loss : 1.5701158046722412
Agent1_Actor_Loss : -4.531160831451416
Agent1_Alpha_Loss : 4.698355674743652
Agent1_Temperature : 0.4721660734201808
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -16.280776977539062
Agent0_Eval_StdReturn : 21.400190353393555
Agent0_Eval_MaxReturn : 13.791202545166016
Agent0_Eval_MinReturn : -51.24333572387695
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -45.23187255859375
Agent0_Train_StdReturn : 22.54874038696289
Agent0_Train_MaxReturn : -18.059253692626953
Agent0_Train_MinReturn : -88.2928466796875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 291000
Agent0_TimeSinceStart : 1044.1760427951813
Agent0_Critic_Loss : 1.4739164113998413
Agent0_Actor_Loss : -4.315705299377441
Agent0_Alpha_Loss : 4.674385070800781
Agent0_Temperature : 0.4720474151003976
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.74546432495117
Agent1_Eval_StdReturn : 25.858367919921875
Agent1_Eval_MaxReturn : 11.873262405395508
Agent1_Eval_MinReturn : -58.96107864379883
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -35.729339599609375
Agent1_Train_StdReturn : 39.75406265258789
Agent1_Train_MaxReturn : 28.389644622802734
Agent1_Train_MinReturn : -88.48149108886719
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 291000
Agent1_TimeSinceStart : 1047.0839409828186
Agent1_Critic_Loss : 1.2584935426712036
Agent1_Actor_Loss : -4.51485538482666
Agent1_Alpha_Loss : 4.663280010223389
Agent1_Temperature : 0.47202877285994804
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -32.20198059082031
Agent0_Eval_StdReturn : 29.950740814208984
Agent0_Eval_MaxReturn : 1.8890173435211182
Agent0_Eval_MinReturn : -107.75674438476562
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -25.595905303955078
Agent0_Train_StdReturn : 23.174659729003906
Agent0_Train_MaxReturn : 7.306140899658203
Agent0_Train_MinReturn : -59.77033615112305
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 292500
Agent0_TimeSinceStart : 1050.0043132305145
Agent0_Critic_Loss : 1.2064213752746582
Agent0_Actor_Loss : -4.434602737426758
Agent0_Alpha_Loss : 4.65155029296875
Agent0_Temperature : 0.4719107308973372
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -32.531639099121094
Agent1_Eval_StdReturn : 28.177270889282227
Agent1_Eval_MaxReturn : 4.204827308654785
Agent1_Eval_MinReturn : -76.84630584716797
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -26.052722930908203
Agent1_Train_StdReturn : 29.43252182006836
Agent1_Train_MaxReturn : 24.939115524291992
Agent1_Train_MinReturn : -55.39572525024414
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 292500
Agent1_TimeSinceStart : 1052.911062002182
Agent1_Critic_Loss : 1.5571602582931519
Agent1_Actor_Loss : -4.553565979003906
Agent1_Alpha_Loss : 4.69000244140625
Agent1_Temperature : 0.47189151743384367
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...
Agent0_Train_MaxReturn : 40.03777313232422
Agent0_Train_MinReturn : -57.991661071777344
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 279000
Agent0_TimeSinceStart : 1012.5598273277283
Agent0_Critic_Loss : 0.3540113568305969
Agent0_Actor_Loss : -0.39479532837867737
Agent0_Alpha_Loss : 0.7889786958694458
Agent0_Temperature : 0.09483161512261631
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.488887786865234
Agent1_Eval_StdReturn : 13.232832908630371
Agent1_Eval_MaxReturn : 15.148099899291992
Agent1_Eval_MinReturn : -33.34159469604492
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -15.121002197265625
Agent1_Train_StdReturn : 12.51726245880127
Agent1_Train_MaxReturn : 8.567928314208984
Agent1_Train_MinReturn : -31.035350799560547
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 279000
Agent1_TimeSinceStart : 1015.5008780956268
Agent1_Critic_Loss : 0.34593749046325684
Agent1_Actor_Loss : -0.6190035343170166
Agent1_Alpha_Loss : 0.8170723915100098
Agent1_Temperature : 0.09482239648457129
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -20.72759246826172
Agent0_Eval_StdReturn : 9.25900650024414
Agent0_Eval_MaxReturn : 1.4947137832641602
Agent0_Eval_MinReturn : -32.83708953857422
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -19.715932846069336
Agent0_Train_StdReturn : 3.4971420764923096
Agent0_Train_MaxReturn : -14.187864303588867
Agent0_Train_MinReturn : -26.4251651763916
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 280500
Agent0_TimeSinceStart : 1018.4404118061066
Agent0_Critic_Loss : 0.24105653166770935
Agent0_Actor_Loss : -0.3921884298324585
Agent0_Alpha_Loss : 0.7959266901016235
Agent0_Temperature : 0.09480624938012336
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -19.9407958984375
Agent1_Eval_StdReturn : 13.745858192443848
Agent1_Eval_MaxReturn : 7.217792510986328
Agent1_Eval_MinReturn : -43.28450012207031
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.5827693939209
Agent1_Train_StdReturn : 16.703752517700195
Agent1_Train_MaxReturn : -8.334807395935059
Agent1_Train_MinReturn : -54.199371337890625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 280500
Agent1_TimeSinceStart : 1021.3773455619812
Agent1_Critic_Loss : 0.29982447624206543
Agent1_Actor_Loss : -0.5385121703147888
Agent1_Alpha_Loss : 0.7946698665618896
Agent1_Temperature : 0.0947967657140391
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -20.026620864868164
Agent0_Eval_StdReturn : 20.489328384399414
Agent0_Eval_MaxReturn : 10.661859512329102
Agent0_Eval_MinReturn : -54.13602828979492
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -10.482206344604492
Agent0_Train_StdReturn : 11.891227722167969
Agent0_Train_MaxReturn : 8.414552688598633
Agent0_Train_MinReturn : -28.054601669311523
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 282000
Agent0_TimeSinceStart : 1024.3171200752258
Agent0_Critic_Loss : 0.299824982881546
Agent0_Actor_Loss : -0.48216506838798523
Agent0_Alpha_Loss : 0.8100346326828003
Agent0_Temperature : 0.09478086546302548
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -24.90426254272461
Agent1_Eval_StdReturn : 14.26763916015625
Agent1_Eval_MaxReturn : 1.1780285835266113
Agent1_Eval_MinReturn : -42.48796844482422
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -20.049190521240234
Agent1_Train_StdReturn : 20.879865646362305
Agent1_Train_MaxReturn : 4.869964599609375
Agent1_Train_MinReturn : -70.96070861816406
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 282000
Agent1_TimeSinceStart : 1027.2560613155365
Agent1_Critic_Loss : 0.25226855278015137
Agent1_Actor_Loss : -0.5017688274383545
Agent1_Alpha_Loss : 0.803145706653595
Agent1_Temperature : 0.09477117116196937
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -20.35775375366211
Agent0_Eval_StdReturn : 21.2777042388916
Agent0_Eval_MaxReturn : 7.558816909790039
Agent0_Eval_MinReturn : -63.97892761230469
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -25.797527313232422
Agent0_Train_StdReturn : 16.699493408203125
Agent0_Train_MaxReturn : 10.789263725280762
Agent0_Train_MinReturn : -47.86668014526367
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 283500
Agent0_TimeSinceStart : 1030.196017742157
Agent0_Critic_Loss : 0.25132468342781067
Agent0_Actor_Loss : -0.4546996057033539
Agent0_Alpha_Loss : 0.7864103317260742
Agent0_Temperature : 0.09475553545573304
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -31.840627670288086
Agent1_Eval_StdReturn : 18.027557373046875
Agent1_Eval_MaxReturn : -10.773584365844727
Agent1_Eval_MinReturn : -56.97160339355469
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -19.76888656616211
Agent1_Train_StdReturn : 12.381868362426758
Agent1_Train_MaxReturn : 1.5598742961883545
Agent1_Train_MinReturn : -32.388084411621094
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 283500
Agent1_TimeSinceStart : 1033.1320359706879
Agent1_Critic_Loss : 0.2684252858161926
Agent1_Actor_Loss : -0.49199190735816956
Agent1_Alpha_Loss : 0.79513019323349
Agent1_Temperature : 0.09474563243590564
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -18.192398071289062
Agent0_Eval_StdReturn : 21.840404510498047
Agent0_Eval_MaxReturn : 23.194461822509766
Agent0_Eval_MinReturn : -49.576194763183594
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -23.324312210083008
Agent0_Train_StdReturn : 13.003643035888672
Agent0_Train_MaxReturn : -2.499762535095215
Agent0_Train_MinReturn : -46.56663513183594
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 285000
Agent0_TimeSinceStart : 1036.0751638412476
Agent0_Critic_Loss : 0.3090377151966095
Agent0_Actor_Loss : -0.3593538701534271
Agent0_Alpha_Loss : 0.7812204957008362
Agent0_Temperature : 0.09473026865883964
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -26.210886001586914
Agent1_Eval_StdReturn : 17.482648849487305
Agent1_Eval_MaxReturn : 3.354397773742676
Agent1_Eval_MinReturn : -65.29225158691406
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -30.92984390258789
Agent1_Train_StdReturn : 16.513376235961914
Agent1_Train_MaxReturn : -5.896378993988037

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -24.434680938720703
Agent0_Eval_StdReturn : 30.812549591064453
Agent0_Eval_MaxReturn : 13.592329025268555
Agent0_Eval_MinReturn : -94.03627014160156
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -26.633075714111328
Agent0_Train_StdReturn : 24.374141693115234
Agent0_Train_MaxReturn : 16.275739669799805
Agent0_Train_MinReturn : -58.79566192626953
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 294000
Agent0_TimeSinceStart : 1055.8389964103699
Agent0_Critic_Loss : 1.3221909999847412
Agent0_Actor_Loss : -4.439405918121338
Agent0_Alpha_Loss : 4.627223968505859
Agent0_Temperature : 0.4717741938082928
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -45.476863861083984
Agent1_Eval_StdReturn : 36.961830139160156
Agent1_Eval_MaxReturn : 0.7071533203125
Agent1_Eval_MinReturn : -121.87456512451172
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -25.732629776000977
Agent1_Train_StdReturn : 41.52137756347656
Agent1_Train_MaxReturn : 21.606842041015625
Agent1_Train_MinReturn : -104.55039978027344
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 294000
Agent1_TimeSinceStart : 1058.7564187049866
Agent1_Critic_Loss : 1.4587173461914062
Agent1_Actor_Loss : -4.477512359619141
Agent1_Alpha_Loss : 4.6579132080078125
Agent1_Temperature : 0.4717543931496344
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -45.86969757080078
Agent0_Eval_StdReturn : 31.06500816345215
Agent0_Eval_MaxReturn : -3.3429336547851562
Agent0_Eval_MinReturn : -119.45763397216797
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -25.933956146240234
Agent0_Train_StdReturn : 22.0711612701416
Agent0_Train_MaxReturn : 0.03948783874511719
Agent0_Train_MinReturn : -64.80998992919922
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 295500
Agent0_TimeSinceStart : 1061.6814205646515
Agent0_Critic_Loss : 1.3107445240020752
Agent0_Actor_Loss : -4.475765228271484
Agent0_Alpha_Loss : 4.61195707321167
Agent0_Temperature : 0.47163783233638307
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -44.39820098876953
Agent1_Eval_StdReturn : 33.766971588134766
Agent1_Eval_MaxReturn : 14.566376686096191
Agent1_Eval_MinReturn : -104.54103088378906
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -44.53042221069336
Agent1_Train_StdReturn : 33.15029525756836
Agent1_Train_MaxReturn : 12.040924072265625
Agent1_Train_MinReturn : -103.93675994873047
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 295500
Agent1_TimeSinceStart : 1064.5985176563263
Agent1_Critic_Loss : 1.4942659139633179
Agent1_Actor_Loss : -4.602436065673828
Agent1_Alpha_Loss : 4.6545820236206055
Agent1_Temperature : 0.47161739739436587
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -36.054046630859375
Agent0_Eval_StdReturn : 28.98272132873535
Agent0_Eval_MaxReturn : 16.255327224731445
Agent0_Eval_MinReturn : -88.2336654663086
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -21.306133270263672
Agent0_Train_StdReturn : 23.610034942626953
Agent0_Train_MaxReturn : 18.133193969726562
Agent0_Train_MinReturn : -74.01543426513672
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 297000
Agent0_TimeSinceStart : 1067.5281946659088
Agent0_Critic_Loss : 1.3222178220748901
Agent0_Actor_Loss : -4.511272430419922
Agent0_Alpha_Loss : 4.606067657470703
Agent0_Temperature : 0.4715016459543563
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -52.95048904418945
Agent1_Eval_StdReturn : 27.258094787597656
Agent1_Eval_MaxReturn : -23.419504165649414
Agent1_Eval_MinReturn : -123.50167846679688
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -42.40593719482422
Agent1_Train_StdReturn : 17.14290428161621
Agent1_Train_MaxReturn : -10.777874946594238
Agent1_Train_MinReturn : -61.63174057006836
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 297000
Agent1_TimeSinceStart : 1070.4558804035187
Agent1_Critic_Loss : 1.4014458656311035
Agent1_Actor_Loss : -4.628768444061279
Agent1_Alpha_Loss : 4.656240940093994
Agent1_Temperature : 0.47148051397567264
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -43.16765594482422
Agent0_Eval_StdReturn : 24.748964309692383
Agent0_Eval_MaxReturn : 13.252700805664062
Agent0_Eval_MinReturn : -94.70491027832031
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -33.1174430847168
Agent0_Train_StdReturn : 15.78887939453125
Agent0_Train_MaxReturn : -2.7520885467529297
Agent0_Train_MinReturn : -58.45330047607422
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 298500
Agent0_TimeSinceStart : 1073.3885929584503
Agent0_Critic_Loss : 1.2613872289657593
Agent0_Actor_Loss : -4.602792739868164
Agent0_Alpha_Loss : 4.64005184173584
Agent0_Temperature : 0.4713655235328836
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -36.76899337768555
Agent1_Eval_StdReturn : 47.18316650390625
Agent1_Eval_MaxReturn : 16.91554832458496
Agent1_Eval_MinReturn : -108.612060546875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -49.013999938964844
Agent1_Train_StdReturn : 24.435178756713867
Agent1_Train_MaxReturn : -2.2672533988952637
Agent1_Train_MinReturn : -76.58189392089844
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 298500
Agent1_TimeSinceStart : 1076.3042678833008
Agent1_Critic_Loss : 1.1777149438858032
Agent1_Actor_Loss : -4.72420597076416
Agent1_Alpha_Loss : 4.694785118103027
Agent1_Temperature : 0.4713436262596501
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -25.597631454467773
Agent0_Eval_StdReturn : 26.308195114135742
Agent0_Eval_MaxReturn : 8.626111030578613
Agent0_Eval_MinReturn : -87.81045532226562
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -24.892318725585938
Agent0_Train_StdReturn : 22.201881408691406
Agent0_Train_MaxReturn : 12.745023727416992
Agent0_Train_MinReturn : -59.991119384765625
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 300000
Agent0_TimeSinceStart : 1079.22860789299
Agent0_Critic_Loss : 1.200499176979065
Agent0_Actor_Loss : -4.638740062713623
Agent0_Alpha_Loss : 4.678182601928711
Agent0_Temperature : 0.47122935410682015
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer.../home/harvey/anaconda3/envs/285final/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/285final/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/285final/lib/python3.7/site-packages/gym/core.py:257: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.[0m
  "Function `env.seed(seed)` is marked as deprecated and will be removed in the future. "


Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -27.178791046142578
Agent1_Eval_StdReturn : 28.310150146484375
Agent1_Eval_MaxReturn : 5.089478492736816
Agent1_Eval_MinReturn : -73.83812713623047
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -41.364715576171875
Agent1_Train_StdReturn : 33.25611877441406
Agent1_Train_MaxReturn : 18.731225967407227
Agent1_Train_MinReturn : -89.06437683105469
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 300000
Agent1_TimeSinceStart : 1082.15323138237
Agent1_Critic_Loss : 1.5996675491333008
Agent1_Actor_Loss : -4.702746391296387
Agent1_Alpha_Loss : 4.673752307891846
Agent1_Temperature : 0.4712067948053244
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...


./peersac.sh: 7: --seed: not found

Agent1_Train_MinReturn : -59.86140441894531
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 285000
Agent1_TimeSinceStart : 1039.0138583183289
Agent1_Critic_Loss : 0.2897682189941406
Agent1_Actor_Loss : -0.4467925727367401
Agent1_Alpha_Loss : 0.8044582009315491
Agent1_Temperature : 0.09472011518842795
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -20.923036575317383
Agent0_Eval_StdReturn : 18.485824584960938
Agent0_Eval_MaxReturn : 18.16732406616211
Agent0_Eval_MinReturn : -54.85353469848633
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -19.124942779541016
Agent0_Train_StdReturn : 9.350639343261719
Agent0_Train_MaxReturn : -4.97014045715332
Agent0_Train_MinReturn : -36.17302703857422
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 286500
Agent0_TimeSinceStart : 1041.9647192955017
Agent0_Critic_Loss : 0.34365177154541016
Agent0_Actor_Loss : -0.3812803626060486
Agent0_Alpha_Loss : 0.804370105266571
Agent0_Temperature : 0.09470498817040565
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -11.336750984191895
Agent1_Eval_StdReturn : 7.808938503265381
Agent1_Eval_MaxReturn : 4.8235297203063965
Agent1_Eval_MinReturn : -24.574844360351562
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -18.051593780517578
Agent1_Train_StdReturn : 15.37503433227539
Agent1_Train_MaxReturn : 10.628449440002441
Agent1_Train_MinReturn : -50.479705810546875
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 286500
Agent1_TimeSinceStart : 1044.903728723526
Agent1_Critic_Loss : 0.28188690543174744
Agent1_Actor_Loss : -0.49395453929901123
Agent1_Alpha_Loss : 0.8067416548728943
Agent1_Temperature : 0.09469460971562342
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -16.15336799621582
Agent0_Eval_StdReturn : 28.634845733642578
Agent0_Eval_MaxReturn : 14.949225425720215
Agent0_Eval_MinReturn : -92.715087890625
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -16.384523391723633
Agent0_Train_StdReturn : 9.471922874450684
Agent0_Train_MaxReturn : -4.631152629852295
Agent0_Train_MinReturn : -36.152923583984375
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 288000
Agent0_TimeSinceStart : 1047.8491506576538
Agent0_Critic_Loss : 0.28974926471710205
Agent0_Actor_Loss : -0.3795873820781708
Agent0_Alpha_Loss : 0.7910690307617188
Agent0_Temperature : 0.09467973468052791
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -16.403064727783203
Agent1_Eval_StdReturn : 14.866743087768555
Agent1_Eval_MaxReturn : 8.48417854309082
Agent1_Eval_MinReturn : -37.41395568847656
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -20.522785186767578
Agent1_Train_StdReturn : 12.110053062438965
Agent1_Train_MaxReturn : 7.909696578979492
Agent1_Train_MinReturn : -37.705650329589844
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 288000
Agent1_TimeSinceStart : 1050.794742822647
Agent1_Critic_Loss : 0.2746175527572632
Agent1_Actor_Loss : -0.514237642288208
Agent1_Alpha_Loss : 0.7966504096984863
Agent1_Temperature : 0.09466914440466903
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -21.18321990966797
Agent0_Eval_StdReturn : 17.454971313476562
Agent0_Eval_MaxReturn : 9.581696510314941
Agent0_Eval_MinReturn : -58.98522186279297
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -13.889897346496582
Agent0_Train_StdReturn : 16.32234764099121
Agent0_Train_MaxReturn : 8.651387214660645
Agent0_Train_MinReturn : -40.4983024597168
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 289500
Agent0_TimeSinceStart : 1053.739129781723
Agent0_Critic_Loss : 0.29440754652023315
Agent0_Actor_Loss : -0.36422914266586304
Agent0_Alpha_Loss : 0.7973716259002686
Agent0_Temperature : 0.09465448568561925
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -15.620552062988281
Agent1_Eval_StdReturn : 11.366679191589355
Agent1_Eval_MaxReturn : -3.6148009300231934
Agent1_Eval_MinReturn : -39.678436279296875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -24.81437873840332
Agent1_Train_StdReturn : 11.454862594604492
Agent1_Train_MaxReturn : -7.086376190185547
Agent1_Train_MinReturn : -41.27470779418945
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 289500
Agent1_TimeSinceStart : 1056.6767492294312
Agent1_Critic_Loss : 0.2562897205352783
Agent1_Actor_Loss : -0.5700671672821045
Agent1_Alpha_Loss : 0.8178211450576782
Agent1_Temperature : 0.09464365099083231
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -12.358534812927246
Agent0_Eval_StdReturn : 13.452399253845215
Agent0_Eval_MaxReturn : 4.483063697814941
Agent0_Eval_MinReturn : -32.43844223022461
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -24.232707977294922
Agent0_Train_StdReturn : 25.661291122436523
Agent0_Train_MaxReturn : 14.60106086730957
Agent0_Train_MinReturn : -72.60223388671875
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 291000
Agent0_TimeSinceStart : 1059.6287069320679
Agent0_Critic_Loss : 0.23997515439987183
Agent0_Actor_Loss : -0.34758931398391724
Agent0_Alpha_Loss : 0.8030498027801514
Agent0_Temperature : 0.09462922289397001
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -17.563289642333984
Agent1_Eval_StdReturn : 17.94733428955078
Agent1_Eval_MaxReturn : 8.83665657043457
Agent1_Eval_MinReturn : -63.104591369628906
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -12.349102020263672
Agent1_Train_StdReturn : 15.213529586791992
Agent1_Train_MaxReturn : 25.34550666809082
Agent1_Train_MinReturn : -29.753677368164062
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 291000
Agent1_TimeSinceStart : 1062.5734083652496
Agent1_Critic_Loss : 0.2636369466781616
Agent1_Actor_Loss : -0.5112403631210327
Agent1_Alpha_Loss : 0.7981100082397461
Agent1_Temperature : 0.09461819087402794
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -19.190176010131836
Agent0_Eval_StdReturn : 19.75577163696289
Agent0_Eval_MaxReturn : 3.766908645629883
Agent0_Eval_MinReturn : -63.23993682861328
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -11.507198333740234
Agent0_Train_StdReturn : 12.656440734863281
Agent0_Train_MaxReturn : 4.169170379638672
Agent0_Train_MinReturn : -37.01472091674805
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 292500
Agent0_TimeSinceStart : 1065.2300446033478
Agent0_Critic_Loss : 0.3224337100982666
Agent0_Actor_Loss : -0.35510849952697754
Agent0_Alpha_Loss : 0.7912876605987549
Agent0_Temperature : 0.0946039824557464
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -21.781801223754883
Agent1_Eval_StdReturn : 10.269098281860352
Agent1_Eval_MaxReturn : -6.4136552810668945
Agent1_Eval_MinReturn : -36.603668212890625
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -25.019567489624023
Agent1_Train_StdReturn : 14.957709312438965
Agent1_Train_MaxReturn : -1.4478206634521484
Agent1_Train_MinReturn : -54.471343994140625
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 292500
Agent1_TimeSinceStart : 1067.561223268509
Agent1_Critic_Loss : 0.3203848600387573
Agent1_Actor_Loss : -0.4409598708152771
Agent1_Alpha_Loss : 0.8112874031066895
Agent1_Temperature : 0.09459272043197421
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -24.808305740356445
Agent0_Eval_StdReturn : 12.647464752197266
Agent0_Eval_MaxReturn : -8.042680740356445
Agent0_Eval_MinReturn : -56.325782775878906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.825664520263672
Agent0_Train_StdReturn : 14.762317657470703
Agent0_Train_MaxReturn : 5.084799289703369
Agent0_Train_MinReturn : -42.23396301269531
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 294000
Agent0_TimeSinceStart : 1069.8764176368713
Agent0_Critic_Loss : 0.2755749821662903
Agent0_Actor_Loss : -0.4086410403251648
Agent0_Alpha_Loss : 0.7954003214836121
Agent0_Temperature : 0.09457874894262115
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -24.786422729492188
Agent1_Eval_StdReturn : 10.784284591674805
Agent1_Eval_MaxReturn : -2.1212990283966064
Agent1_Eval_MinReturn : -42.84955596923828
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -20.635364532470703
Agent1_Train_StdReturn : 9.080026626586914
Agent1_Train_MaxReturn : -3.3393962383270264
Agent1_Train_MinReturn : -38.615760803222656
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 294000
Agent1_TimeSinceStart : 1072.1492822170258
Agent1_Critic_Loss : 0.28460535407066345
Agent1_Actor_Loss : -0.3847995102405548
Agent1_Alpha_Loss : 0.7972952723503113
Agent1_Temperature : 0.0945672821302783
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -19.26247787475586
Agent0_Eval_StdReturn : 24.96442222595215
Agent0_Eval_MaxReturn : 22.189071655273438
Agent0_Eval_MinReturn : -69.80530548095703
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -14.497662544250488
Agent0_Train_StdReturn : 12.99428653717041
Agent0_Train_MaxReturn : 5.157990455627441
Agent0_Train_MinReturn : -44.24360656738281
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 295500
Agent0_TimeSinceStart : 1074.409583568573
Agent0_Critic_Loss : 0.30382150411605835
Agent0_Actor_Loss : -0.46629220247268677
Agent0_Alpha_Loss : 0.7808669805526733
Agent0_Temperature : 0.09455356479006602
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -20.316112518310547
Agent1_Eval_StdReturn : 14.081576347351074
Agent1_Eval_MaxReturn : 6.420168399810791
Agent1_Eval_MinReturn : -43.482025146484375
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -8.076327323913574
Agent1_Train_StdReturn : 14.36064624786377
Agent1_Train_MaxReturn : 17.586441040039062
Agent1_Train_MinReturn : -33.7838249206543
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 295500
Agent1_TimeSinceStart : 1076.6598074436188
Agent1_Critic_Loss : 0.2230491191148758
Agent1_Actor_Loss : -0.457842618227005
Agent1_Alpha_Loss : 0.790692925453186
Agent1_Temperature : 0.09454189187092436
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -18.581737518310547
Agent0_Eval_StdReturn : 12.33694839477539
Agent0_Eval_MaxReturn : 1.7680168151855469
Agent0_Eval_MinReturn : -39.54655456542969
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -22.52381706237793
Agent0_Train_StdReturn : 12.584944725036621
Agent0_Train_MaxReturn : 0.6149856448173523
Agent0_Train_MinReturn : -41.08136749267578
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 297000
Agent0_TimeSinceStart : 1078.889306306839
Agent0_Critic_Loss : 0.3486052453517914
Agent0_Actor_Loss : -0.4146658778190613
Agent0_Alpha_Loss : 0.7777454257011414
Agent0_Temperature : 0.09452843361254987
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -22.138795852661133
Agent1_Eval_StdReturn : 16.75937843322754
Agent1_Eval_MaxReturn : -0.7100830078125
Agent1_Eval_MinReturn : -51.87519836425781
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -21.192399978637695
Agent1_Train_StdReturn : 21.591455459594727
Agent1_Train_MaxReturn : 25.02880859375
Agent1_Train_MinReturn : -53.897491455078125
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 297000
Agent1_TimeSinceStart : 1081.1068677902222
Agent1_Critic_Loss : 0.25362762808799744
Agent1_Actor_Loss : -0.42363542318344116
Agent1_Alpha_Loss : 0.8029833436012268
Agent1_Temperature : 0.09451650708932764
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -20.989200592041016
Agent0_Eval_StdReturn : 21.829662322998047
Agent0_Eval_MaxReturn : 6.994800567626953
Agent0_Eval_MinReturn : -71.57862854003906
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -18.138561248779297
Agent0_Train_StdReturn : 18.70830726623535
Agent0_Train_MaxReturn : 11.52265739440918
Agent0_Train_MinReturn : -49.182533264160156
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 298500
Agent0_TimeSinceStart : 1083.315081357956
Agent0_Critic_Loss : 0.2850511372089386
Agent0_Actor_Loss : -0.41142746806144714
Agent0_Alpha_Loss : 0.791693925857544
Agent0_Temperature : 0.09450330704624052
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -19.93498420715332
Agent1_Eval_StdReturn : 8.572479248046875
Agent1_Eval_MaxReturn : -4.9781293869018555
Agent1_Eval_MinReturn : -33.17725372314453
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -15.397012710571289
Agent1_Train_StdReturn : 18.742345809936523
Agent1_Train_MaxReturn : 30.028549194335938
Agent1_Train_MinReturn : -40.13487243652344
Agent1_Train_AverageEpLen : 150.0/home/harvey/anaconda3/envs/285final/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/285final/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/285final/lib/python3.7/site-packages/gym/core.py:257: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.[0m
  "Function `env.seed(seed)` is marked as deprecated and will be removed in the future. "

Agent1_Train_EnvstepsSoFar : 298500
Agent1_TimeSinceStart : 1085.500687122345
Agent1_Critic_Loss : 0.2704744338989258
Agent1_Actor_Loss : -0.5293686985969543
Agent1_Alpha_Loss : 0.811714768409729
Agent1_Temperature : 0.09449110033707671
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 0 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent0_Eval_AverageReturn : -20.520265579223633
Agent0_Eval_StdReturn : 17.30069351196289
Agent0_Eval_MaxReturn : 13.730127334594727
Agent0_Eval_MinReturn : -51.175437927246094
Agent0_Eval_AverageEpLen : 150.0
Agent0_Train_AverageReturn : -17.97773551940918
Agent0_Train_StdReturn : 13.020502090454102
Agent0_Train_MaxReturn : -3.9153239727020264
Agent0_Train_MinReturn : -42.4211540222168
Agent0_Train_AverageEpLen : 150.0
Agent0_Train_EnvstepsSoFar : 300000
Agent0_TimeSinceStart : 1087.6942262649536
Agent0_Critic_Loss : 0.24794819951057434
Agent0_Actor_Loss : -0.4485822319984436
Agent0_Alpha_Loss : 0.8228479027748108
Agent0_Temperature : 0.0944780898105094
Agent0_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...



Collecting data to be used for training...

Training agent 1 using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
Agent1_Eval_AverageReturn : -18.40567970275879
Agent1_Eval_StdReturn : 14.569023132324219
Agent1_Eval_MaxReturn : 10.247153282165527
Agent1_Eval_MinReturn : -37.630340576171875
Agent1_Eval_AverageEpLen : 150.0
Agent1_Train_AverageReturn : -17.002758026123047
Agent1_Train_StdReturn : 13.867268562316895
Agent1_Train_MaxReturn : 8.140453338623047
Agent1_Train_MinReturn : -48.282405853271484
Agent1_Train_AverageEpLen : 150.0
Agent1_Train_EnvstepsSoFar : 300000
Agent1_TimeSinceStart : 1089.874796628952
Agent1_Critic_Loss : 0.31620484590530396
Agent1_Actor_Loss : -0.5780975818634033
Agent1_Alpha_Loss : 0.8061892986297607
Agent1_Temperature : 0.09446568990161441
Agent1_Initial_DataCollection_AverageReturn : -41.37040710449219
Done logging...


./peersac.sh: 7: --seed: not found
